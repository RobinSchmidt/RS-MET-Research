Numerical schemes for solving partial differential equations by finite 
difference methods are straightforward to understand and use but they usually 
assume a regular grid. For geometrically more complicated domains, this 
restriction is often a showstopper, so in such cases, finite element methods 
are typically used. These, on the other hand, are conceptually much more 
difficult to understand and apply and can require considerable analytical 
preparatory work to convert the problem into an appropriate form - which may 
be not even always possible. So here, we want to consider an idea that allows 
us to use a finite difference method on an irregular grid (or mesh).

We need to find expressions for approximating spatial derivatives from known
function values on grid points. On a regular 2D grid with grid spacings h_x, 
h_y in the x- and y-directions, we could approximate the first spatial 
derivatives with a forward difference (time variable has been suppressed):

  u_x ~= (u(x+h_x,y) - u(x,y)) / h_x
  u_y ~= (u(x,y+h_y) - u(x,y)) / h_y
  
A central difference would be more accurate, but for the generalization to 
irregular grids, the forward difference is a more convenient starting point.
The key idea for the generalization to an irregular mesh is to approximate 
directional derivatives into the directions of neighbouring grid points 
instead of partial derivatives with respect to coordinate axes and later 
retrieve the partial derivatives from those directional derivatives by 
projecting them onto the unit vectors along the coordinate axes (which 
is just a fancy way of describing the extraction of x- and y-components).

Let P be a general grid point that is currently under consideration and let's
assume, the point P is connected to 3 other grid points Q,R,S. We want to
find a formula for approximating the first spatial derivatives u_x and u_y 
at P and we assume that (approximate) function values u(P),u(Q),u(R),u(S) are
known. We define the vectors a,b,c as:

  a := Q-P, b:= R-P, c := S-P
  
We approximate the directional derivatives at P into the a,b,c directions as:

  u_a ~= (u(P+a)-u(P)) / |a|
  u_b ~= (u(P+b)-u(P)) / |b|
  u_c ~= (u(P+c)-u(P)) / |c|
  
where, of course, u(P+a) = u(Q), etc. Finally, to obtain our desired partial 
derivatives with respect to x and y, we note that the x-components of 
u_a,u_b,u_c can all be seen as approximations to u_x and the y-components as 
approximations to u_y (right?). Let's denote them as u_a_x,u_a_y,u_b_x, etc. 
We now form a weighted sum of them:

  u_x ~= w_a*u_a_x + w_b*u_b_x + w_c*u_c_x
  u_y ~= w_a*u_a_y + w_b*u_b_y + w_c*u_c_y
  
where the weights are chosen to sum up to 1 and they should be inverserly 
proportional to the lengths of the vectors, i.e. as algorithm:

  w_a := 1 / |a|
  w_b := 1 / |b|
  w_c := 1 / |c|
  k   := 1 / (w_a + w_b + w_c)
  w_a *= k
  w_b *= k
  w_c *= k
  
The idea behind choosing the weights inversely proportional to the distance 
between the grid points is that closer grid points will give more accurate 
approximations to the derivative, so they should get a higher weight. Higher
order derivatives can be computed by just applying the same computations on 
the (scalar) functions u_x, u_y and so on (todo: maybe find a simplified 
scheme that almagamates the computations). Or maybe the weights should be 
chosen to be proportional to the x- and y- distances, i.e. for the estimation 
of u_x, we only look at a_x,b_x,c_x, etc.? If grid point Q is offset from P 
only in the x- but not in the y-direction, we cannot expect to gain any 
information about u_y from the directional derivative into the a=Q-P 
direction - this direction is just the direction of the x-axis in this case.
Or wait: the directional derivative u_a is actually just a scalar - to obtain
u_a_x, we may need to do: u_a_x := u_a * <a,e_x> where <a,e_x> is the scalar 
product between a and the unit vector in the x-direction e_x - so, it's 
basically just: u_a_x := u_a * a_x where a_x is the x-component of a. That 
actually gives a proportionality to the offset into the respective coordinate
direction. ...this seems to make sense...

oooh...i think, the formulas above to reconstruct the gradient from the 
directional derivative is wrong. We cannot just project the vectors a,b,c 
onto the coordinate unit vectors e_x,e_y to get an estimate for the respective
component of the gradient. Consider the case, where the direction vector a goes
up and we want to estimate the x-component of the gradient from the directional
derivative along a - the projection of a onto x would be zero and so would be 
the estimate for u_x given by a, i.e. u_x_a - that can't be right. I think, what 
needs to be done instead is to set up a system of equations, like so:

  u_a = u_x*a_x + u_y*a_y
  u_b = u_x*b_x + u_y*b_y
  u_c = u_x*c_x + u_y*c_y

and solve for the gradient components u_x,u_y. With 3 neighbors, the system is
overdetermined, so we may need a (weighted) least squares approach, where the 
weights should be given as before as inversely proportional to the lengths (we 
allow the solution error to be off more for points farther away from P). Maybe 
here, u_a,u_b,u_c should not be normalized to unit length. With 2 neighbors, we 
have 2 equations for 2 unknowns, so we should be able to compute a unique 
solution - but what if there's only 1 neighbor - we have a degree of freedom 
and may use it to compute a minimum length solution. However, the typical case 
will be the overdetermined one. Maybe as an optimization, we could use weights 
given inversely proportional to the sum of the absolute values - experimentation 
needed. Writing the equation system as matrix equation:

  |a_x a_y| * |u_x| = |u_a|
  |b_x b_y|   |u_y|   |u_b|
  |c_x c_y|           |u_c|

see: https://en.wikipedia.org/wiki/Ordinary_least_squares 
with p=2, n=3 - the least squares solution can be found by solving:

  |a_x b_x c_x| * |a_x a_y| * |u_x| = |a_x b_x c_x| * |u_a|
  |a_y b_y c_y|   |b_x b_y|   |u_y|   |a_y b_y c_y|   |u_b|
                  |c_x c_y|                           |u_c|

The weighted least squares solution:
https://en.wikipedia.org/wiki/Weighted_least_squares
can be found by solving:



todo: figure out what to do in the underdetermined case. also, what if n=p but 
the matrix is singular?

name: weighted least squares directional derivatives method


ToDo: see, if (and how) we recover the central difference rule on a regular 
grid...


-------------------------------------------------------------------------------

As an example, let's consider the diffusion equation in 2 spatial dimensions:

  u_t = D * Lap(u), where Lap(u) := u_xx + u_yy
  
Here, u stands for the function u(x,y,t) that we want to find, u_t for its
derivative with respect to time t, Lap(u) for the Laplacian of u which is 
defined as the sum of the second derivatives with respect to x and y which 
are denoted as u_xx and u_yy. The scalar constant D is the diffusion 
coefficient. This equation is also known as the heat equation, because heat
is one of the things that diffuse this way.

References:
https://en.wikipedia.org/wiki/Directional_derivative
