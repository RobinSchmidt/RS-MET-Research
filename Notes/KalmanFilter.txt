In this file, I try to bring together information about Kalman filters from various places in order
to synthesize it into an implementation ready algorithm. 

The purpose of a Kalman filter is to estimate the current state of some object based on two pieces
of noisy information: (1) An estimated previous state, (2) An observation of current measurements.
It is assumed that the system advances from one time step to the next according to some known 
deterministic rule which is encapsulated in a state transition matrix F. The state itself is 
represented as a state vector x. The observations are represented in an observation vector y. 

...TBC...



----------------------------------------------------------------------------------------------------
Algorithm from this video:
https://www.youtube.com/watch?v=EBjca6tPuO0

The video represnets the state space model as follows: 

x[k] = F x[k-1] + B u[k] + w[k]             where w[k] = N(0,Q)
z[k] = H x[k] + v[k]                        where v[k] = N(0,R)

I simplified the notation by assuming the matrices to be constant such that the time index [k] could
be dropped from the matrices. I also use [k] to denote the time-step instead of subscripts because 
they don't work well when the subscript is something like k-1. The notation w[k] = N(0,Q) means that
the random vector w[k] is normally distributed with mean 0 and covariance matrix Q. The "+ B u[k]"
represents a control input. If we just had  x[k] = F x[k-1], it would mean that the system advances
according to the rule without external nudges and without noise.

The estimation algorithm works like:

Prediction:
x[k] = F x[k-1] + B u[k]
P[k] = F P[k-1] F^T + Q


K[k] = P[k] H^T (H P[k] H^T + R)^(-1)        Kalman gain





----------------------------------------------------------------------------------------------------
Algorithm in "Adaptive Filter Theory" (4th Ed.) by Simon Haykin, page 484

Variables:

Name          Type       Meaning
M             Integer    Dimension of state vector
N             Integer    Dimension of observation vector
x(n)          M x 1      State vector at time n
y(n)          N x 1      Observation vector at time n
F(n+1,n)      M x M      Transition matrix from time n to time n+1
C(n)          N x M      Measurement matrix at time n
Q1(n)         M x M      Correlation matrix of process noise v1(n)
Q2(n)         N x N      Correlation matrix of measurement noise v2(n)
p(n|Y[n-1])   M x 1      Predicted estimate of state at time n given observations y(1),...,y(n-1)
q(n|Y[n])     M x 1      Filtered estimate of state at time n given observations y(1),...,y(n)
G(n)          M x N      Kalman gain at time n
a(n)          N x 1      Innovations vector at time n
R(n)          N x N      Correlation matrix of innovations vector a 
K(n,n-1)      M x M      Correlation matrix of error p(n|Y[n-1]) 
K(n)          M x M      Correlation matrix of error q(n|Y[n])

The book uses x with a hat for both p and q and uses alpha for a. For ASCII reasons, I renamed these
variables here. 


Algorithm:


Parameters:

Transition matrix: F(n, n+1)
Measurement matrix: C(n)
Correlation matrix of process noise: Q1(n)
Correlation matrix of measurement noise: Q2(n)


Inputs:

Observations y(1),y(2), ... , y(n)


Initial conditions:
n = 1, n-1 = 0
Expectation value of x(1): p(1|Y[0]) = E{x(1)} 
Correlation matrix:        K(1,0)    = E{ (x(1)-E{x(1)}) (x(1)-E{x(1)})^H } = Pi_0, 
                                       with: ^H: Hermitian transpose

Computation for n = 1,2,3:

G(n)        = F(n+1,n)  K(n,n-1)  C^H(n)  (C(n) K(n,n-1) C^H(n) + Q2)^(-1)
a(n)        = y(n) - C(n) p(n|Y[n-1])
p(n+1|Y[n]) = F(n+1,n) p(n|Y[n-1]) + G(n) a(n)
K(n)        = K(n,n-1) - F(n,n+1) G(n) C(n) K(n,n-1)
K(n+1,n)    = F(n+1,n) K(n) F^H(n+1,n) + Q1(n)




----------------------------------------------------------------------------------------------------

More resources:


https://en.wikipedia.org/wiki/Kalman_filter#Details
https://en.wikipedia.org/wiki/Alpha_beta_filter