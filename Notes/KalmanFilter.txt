In this file, I try to bring together information about Kalman filters from various places in order
to synthesize it into an implementation ready algorithm. 

The purpose of a Kalman filter is to estimate the current state of some system based on two pieces
of noisy information: (1) An estimated previous state, (2) An observation of current measurements.
It is assumed that the system advances from one time step to the next according to some known 
deterministic rule which is encapsulated in a state transition matrix F. The state itself is 
represented as a state vector x. The observations are represented in an observation vector y. 

...TBC...

I think, the Kalman filter is basically a noise reduction filter based on the idea that if you have
two estimates of a random variable, you can combine them into an estimate that is better than either
one of the two - at least, if the estimates are both unbiased and independent (maybe uncorrelated is 
good enough). Then, it can make sense to form a weighted average where you give higher weight to 
less noisy estimate, i.e. the one with lower variance in the error. But the Kalman filter solves 
this problem for the multidimensional case such that we not only have variances but covariance 
matrices. And it applies the idea to the special case where one of the estimates is computed 
recursively from a previous estimate via a known state transition matrix. ...that's my current 
understanding, at least.

----------------------------------------------------------------------------------------------------
Algorithm from this video:
https://www.youtube.com/watch?v=EBjca6tPuO0

The video represents the state space model as follows (at 11:45): 

x[k] = F x[k-1] + B u[k] + w[k]             where w[k] = N(0,Q)
z[k] = H x[k] + v[k]                        where v[k] = N(0,R)

I simplified the notation by assuming the matrices to be constant such that the time index [k] could
be dropped from the matrices. I also use [k] to denote the time-step instead of subscripts because 
they don't work well when the subscript is something like k-1. The notation w[k] = N(0,Q) means that
the random vector w[k] is normally distributed with mean 0 and covariance matrix Q. The "+ B u[k]"
represents a control input. If we just had  x[k] = F x[k-1], it would mean that the system advances
according to the state update/transition rule without external nudges (coming in from u) and without 
noise (coming from w). The first equation i be called process equation and the second is called 
measurement equation.

The state estimation algorithm works like (at 11:51):

Prediction:
x[k] = F x[k-1] + B u[k]
P[k] = F P[k-1] F^T + Q

Kalman gain computation:
K[k] = P[k] H^T (H P[k] H^T + R)^(-1)

Correction:
x[k] = x[k] + K[k](z[k] - H x[k])        z[k] is incoming measurement signal?
P[k] = (I - K[k] H) P[k]
     = P[k] - K[k] H P[k]

I think, the input to this filter is the current measurement vector z[k] and the control input u[k]. 
The output is the current state estimate x[k]. So - it looks like the measurement matrix H takes a 
state vector as input and produces a vector of observables/measurements? And the Kalman gain matrix 
K does the opposite: take a vector of innovations (which has the same dimension as measurements) and 
produces a vector of corrections for the state estimate?


----------------------------------------------------------------------------------------------------
Algorithm in "Adaptive Filter Theory" (4th Ed.) by Simon Haykin, page 484

Variables:

Name          Type       Meaning
M             Integer    Dimension of state vector
N             Integer    Dimension of observation vector
x(n)          M x 1      State vector at time n
y(n)          N x 1      Observation vector at time n
F(n+1,n)      M x M      Transition matrix from time n to time n+1
C(n)          N x M      Measurement matrix at time n
Q1(n)         M x M      Correlation matrix of process noise v1(n)
Q2(n)         N x N      Correlation matrix of measurement noise v2(n)
p(n|Y[n-1])   M x 1      Predicted estimate of state at time n given observations y(1),...,y(n-1)
q(n|Y[n])     M x 1      Filtered estimate of state at time n given observations y(1),...,y(n)
G(n)          M x N      Kalman gain at time n
a(n)          N x 1      Innovations vector at time n
R(n)          N x N      Correlation matrix of innovations vector a 
K(n,n-1)      M x M      Correlation matrix of error p(n|Y[n-1]) 
K(n)          M x M      Correlation matrix of error q(n|Y[n])

The book uses x with a hat for both p and q and uses alpha for a. For ASCII reasons, I renamed these
variables here. 


Algorithm:


Parameters:

Transition matrix: F(n, n+1)
Measurement matrix: C(n)
Correlation matrix of process noise: Q1(n)
Correlation matrix of measurement noise: Q2(n)


Inputs:

Observations y(1),y(2), ... , y(n)


Initial conditions:
n = 1, n-1 = 0
Expectation value of x(1): p(1|Y[0]) = E{x(1)} 
Correlation matrix:        K(1,0)    = E{ (x(1)-E{x(1)}) (x(1)-E{x(1)})^H }, 
                                       with: ^H: Hermitian transpose

Computation for n = 1,2,3,...:

G(n)        = F(n+1,n)  K(n,n-1)  C^H(n)  (C(n) K(n,n-1) C^H(n) + Q2)^(-1)
a(n)        = y(n) - C(n) p(n|Y[n-1])
p(n+1|Y[n]) = F(n+1,n) p(n|Y[n-1]) + G(n) a(n)
K(n)        = K(n,n-1) - F(n,n+1) G(n) C(n) K(n,n-1)
K(n+1,n)    = F(n+1,n) K(n) F^H(n+1,n) + Q1(n)




----------------------------------------------------------------------------------------------------

More resources:


https://en.wikipedia.org/wiki/Kalman_filter#Details
https://en.wikipedia.org/wiki/Alpha_beta_filter