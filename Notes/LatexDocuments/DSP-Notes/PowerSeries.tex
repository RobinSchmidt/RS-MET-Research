\title{Multiplication and Division for Power Series}
\author{Robin Schmidt (www.rs-met.com)}
\date{\today}
\maketitle

\section{Multiplication of Series}
Suppose, we are given 2 series of numbers:
\begin{equation}
 A = \sum_{n=0}^{\infty} a_n, \qquad
 B = \sum_{n=0}^{\infty} b_n
\end{equation}
and we set out to find the coefficients $c_n$ for a series which is the product of the two series:
\begin{equation}
 \label{Eq:CauchyProduct}
 C = \sum_{n=0}^{\infty} c_n = (\sum_{k=0}^{\infty} a_k) (\sum_{n=0}^{\infty} b_n)
\end{equation}
For this product series, we see that we must form the products of all pairs $a_k, b_n$ where the indices $k, n$ traverse the range from $0$ to $\infty$ independently - and then sum these products up. Hence $C$ can be written as the double sum:
\begin{equation}
 C = \sum_{k=0}^{\infty} \sum_{n=0}^{\infty} a_k b_n
\end{equation}
In this double sum, it is immaterial, which order of the terms we choose - all that counts is that each term eventually enters the sum once. We write the pairs of indices as a matrix:
\begin{equation}
 \begin{array}{ccccc}
  (0,0)  & (0,1)  & (0,2)  & (0,3)  & \ldots \\
  (1,0)  & (1,1)  & (1,2)  & (1,3)  & \ldots \\
  (2,0)  & (2,1)  & (2,2)  & (2,3)  & \ldots \\
  (3,0)  & (3,1)  & (3,2)  & (3,3)  & \ldots \\    
  \ldots & \ldots & \ldots & \ldots & \ldots \\    
 \end{array}
\end{equation}
and define the following sub-sums:
\begin{equation}
 \label{Eq:ProductCoeffsWrittenOut}
 \begin{array}{ccl}
   c_0  & = & a_0 b_0  \\ 
   c_1  & = & a_1 b_0 + a_0 b_1 \\ 
   c_2  & = & a_2 b_0 + a_1 b_1 + a_0 b_2 \\ 
   c_3  & = & a_3 b_0 + a_2 b_1 + a_1 b_2 + a_0 b_3 \\ 
   \ldots
 \end{array}
\end{equation}
That means that for each $c_n$, we pick the $n$th row of the matrix and take the diagonal upwards until we reach the top row. It should be clear, that we will pick each index pair once and only once. So set:
\begin{equation}
 \label{Eq:Convolution}
 c_n = \sum_{k=0}^{n} a_{n-k} b_k
\end{equation}
which is the desired result. Adding up all the sub-sums $c_n$ ought to give the total sum $C$. Such a product series of two other series goes by the name Cauchy-product. The formula for $c_n$ is also recognized as a convolution sum of portions of the sequences $a_n, b_n$. In signal processing terms, we would call $b_n$ the impulse response and $a_n$ the input signal.

\section{Division of Series}
This convolution process can be reversed. Consider equation \ref{Eq:CauchyProduct} again. We can write this as:
\begin{equation}
 \sum_{n=0}^{\infty} a_n = \frac{\sum_{n=0}^{\infty} c_n}{\sum_{n=0}^{\infty} b_n}
\end{equation}
Going back to $\ref{Eq:ProductCoeffsWrittenOut}$, and solving each equation for $a_n$ we see that:
\begin{equation}
 \begin{array}{ccl}
   a_0  & = &  c_0                                / b_0 \\ 
   a_1  & = & (c_1 - a_0 b_1)                     / b_0 \\ 
   a_2  & = & (c_2 - a_0 b_2 - a_1 b_1)           / b_0 \\ 
   a_3  & = & (c_3 - a_0 b_3 - a_1 b_2 - a_2 b_1) / b_0 \\ 
   \ldots
 \end{array}
\end{equation}
We see that the pattern is generally:
\begin{equation}
 \label{Eq:Deconvolution}
 a_n = \frac{1}{b_0} \left(c_n - \sum_{k=0}^{n-1} a_k b_{n-k} \right)
\end{equation}
This equation for computing the $a$-coefficients is recursive - for the computation each $a_n$, we need $a_{n-1}, a_{n-2}, \ldots$. The recursion is started with $a_0 = c_0/b_0$ and then proceeds for all other $a_n$. Having called \ref{Eq:Convolution} convolution, we may call \ref{Eq:Deconvolution} deconvolution. We need to take a little care when $b_0 = 0$ ....tbc.


\subsection{Power Series}
We have seen that all this stuff works for series of numbers. If each term $a_n, b_n$ is followed by an $x^n$, it turns out that the $x^n$ can be factored out in each equation for $c_n$ - so all what has been said applies directly to the sequences of the coefficients of power series.





%
%In this case, \ref{Eq:ProductCoeffsWrittenOut} reads like this:
%\begin{equation}
% \label{Eq:ProductCoeffsWrittenOut2}
% \begin{array}{ccl}
%   c_0 & = & 0  \\ 
%   c_1 & = & a_0 b_1 \\ 
%   c_2 & = & a_1 b_1 + a_0 b_2 \\ 
%   c_3 & = & a_2 b_1 + a_1 b_2 + a_0 b_3 \\ 
%   c_n & = & \sum_{k=1}^{n} a_{n-k} b_k
%   \ldots
% \end{array}
%\end{equation}
%For the general formula, the summation index now starts at $1$ as opposed to starting at $0$ in \ref{Eq:Convolution}. If we have more leading zeros leading zeros in the series $b$, let $m$ be the first index where $b_m$ has a nonzero value. The formulas turn into:
%\begin{equation}
% c_n = 
% \begin{cases}
%   0                           & n < m  \\ 
%   a_0 b_m                     & n = m  \\ 
%   \sum_{k=m}^{n} a_{n-k} b_k  & n > m   
% \end{cases}
%\end{equation}









%\begin{equation}
%\label{Eq:Integrals}
% \int_0^{\infty} e^{-\alpha t} dt = \frac{1}{\alpha}, \quad
% %\int_0^{\infty} e^{-\alpha t} \sin(\omega t + \phi) dt = \frac{\omega \cos \phi + \alpha \sin \phi}{\alpha^2 + \omega^2}, \quad 
% \int_0^{\infty} e^{-\alpha t} \cos(\omega t + \phi) dt = \frac{\alpha \cos \phi - \omega \sin \phi}{\alpha^2 + \omega^2}
%\end{equation}






%\begin{thebibliography}{9}  % 9 indicates that there are no more than 9 entries
% %\bibitem{Gum} Charles Constantine Gumas. A century old, the fast Hadamard transform proves useful in digital communications
%\end{thebibliography}

