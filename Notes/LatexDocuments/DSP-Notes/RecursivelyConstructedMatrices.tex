\title{Recursively Constructed Matrices}
\author{Robin Schmidt (www.rs-met.com)}
\date{\today}
\maketitle

We consider a recursive construction rule for $N \times N$ matrices where $N = 2^L, L \in \mathbb{N}$ from their respective predecessor matrices of size $N/2 \times N/2$. This construction will contain 4 free parameters. We will investigate some of the properties of such matrices and give an $\mathcal{O}(N \cdot L) = \mathcal{O}(N \cdot \log_2(N))$ algorithm to compute the product of any vector with such a matrix. By imposing some restrictions on the choice of the 4 parameters, we will obtain a 1-parametric family of unitary matrices.

\section{The Recursive Construction}
Consider the following recursive construction rule, containing 4 free parameters $a, b, c, d$:
\begin{equation}
 \mathbf{M}_0 = 1, \qquad
 \mathbf{M}_{L+1} =
 \begin{pmatrix}
 a \mathbf{M}_L &  b \mathbf{M}_L  \\
 c \mathbf{M}_L &  d \mathbf{M}_L
 \end{pmatrix}
\end{equation}
The first couple of such matrices are given by:
\begin{equation}
 \mathbf{M}_0 = 1, \qquad
 \mathbf{M}_1 =
 \begin{pmatrix}
 a & b \\
 c & d 
 \end{pmatrix}, \qquad
 \mathbf{M}_2 =
 \begin{pmatrix}
 aa & ab & ba & bb \\
 ac & ad & bc & bd \\
 ca & cb & da & db \\
 cc & cd & dc & dd
 \end{pmatrix} 
\end{equation}
\begin{equation}
 \mathbf{M}_3 =
 \begin{pmatrix}
 aaa & aab & aba & abb    & baa & bab & bba & bbb  \\
 aac & aad & abc & abd    & bac & bad & bbc & bbd  \\
 aca & acb & ada & adb    & bca & bcb & bda & bdb  \\
 acc & acd & adc & add    & bcc & bcd & bdc & bdd  \\
 caa & cab & cba & cbb    & daa & dab & dba & dbb  \\
 cac & cad & cbc & cbd    & dac & dad & dbc & dbd  \\
 cca & ccb & cda & cdb    & dca & dcb & dda & ddb  \\
 ccc & ccd & cdc & cdd    & dcc & dcd & ddc & ddd  \\ 
 \end{pmatrix} 
\end{equation}

\section{The Fast Algorithm}
The nice thing about a so constructed matrix is, that the matrix-vector multiplication $\mathbf{y} = \mathbf{M}_L \; \mathbf{x}$ can be carried out in $\mathcal{O}(N \cdot L) = \mathcal{O}(N \cdot \log_2(N))$ operations via the algorithm (in pseudo MatLab/Octave):
\begin{verbatim}
for i=1:L
 for j=1:N/2
  y(j)     = a*x(2*j-1) + b*x(2*j);
  y(j+N/2) = c*x(2*j-1) + d*x(2*j);  
 end
 x = y; % reused for intermediate result
end
\end{verbatim}
% consider "incomplete" transforms where the outer loop only runs to some number < L

\section{Some Properties}

\subsection{The Determinant}
Let's denote the determinant of the $L$-th order matrix $M_L$ as $D_L$. For the cases $L=0$ and $L=1$ it can be computed directly:
\begin{equation}
 D_0 = 1,             \qquad
 D_1 = a d - b c,     \qquad
\end{equation}
For $L > 1$, the following formula applies:
\begin{equation} 
 D_L = D_1^{p_L+q_L} 
 \qquad \text{where} \qquad
 p_L = 2^{L-1}, q_L = 2 (p_{L-1} + q_{L-1}) \; \text{with} \; q_0 = 0
\end{equation}
The definition of $q_L$ in this case is recursive such that it must be computed by starting with $q_0$ and iterating the recursive formula whereas for $p_L$ we have a closed form solution. However, in practice it is easy enough compute $p_L$ and $q_L$ jointly via the algorithm:
\begin{verbatim}
p = 1;
q = 0;
for i=2:L
 q = 2*(q+p);
 p = 2*p;
end
\end{verbatim}
The formula for $D_L$ can be derived by using:
\begin{equation} 
 \det
 \begin{pmatrix}
 \mathbf{A} & \mathbf{B} \\
 \mathbf{C} & \mathbf{D} 
 \end{pmatrix}
 = 
 \det(\mathbf{A D} - \mathbf{B C})
\end{equation}
which holds when all pairs of products of $\mathbf{A}, \mathbf{B}, \mathbf{C}, \mathbf{D}$ are commutative (see \cite{Sil}), which is the case here.

\subsection{The Trace}
The trace of the matrix $\mathbf{M_L}$ (the sum of the elements on the main diagonal) shall be denoted as $T_L$ and is given by:
\begin{equation}
 T_L = \sum_{i=1}^N (\mathbf{M}_L)_{i,i} = (a+d)^L
\end{equation}
Likewise, the sum of the elements on the subdiagonal is $(b+c)^L$. \textbf{[verify this]}

\subsection{Maximum Absolute Value}
The maximum absolute value of $\mathbf{M_L}$'s elements is given by the the maximum absolute value of $a, b, c, d$ raised to the power of $L$. \textbf{[verify this]}

\subsection{Number of Distinct Elements}
If we denote the number of distinct elements in the set $\{a, b, c, d\}$ as $n_d$ (such that $n_d \leq 4$), then the number of distinct elements in the matrix $\mathbf{M_L}$, denoted as $N_d$, is given by the binomial coefficient:
\begin{equation}
 N_d = 
 \begin{pmatrix}
  n_d+L-1 \\
  L
 \end{pmatrix}
\end{equation}
This can be found by considering that each element of the matrix $\mathbf{M_L}$ is given by a product of $L$ factors from the set $\{a, b, c, d\}$. The factors may repeat and the order doesn't count, so we are dealing with combinations with repetitions in the terminology of combinatorics. \textbf{[verify this]}

\section{The Inverse Transform}
For the $L = 1$ case, which transforms a $2$-dimensional vector, the matrix that produces the inverse transform can be calculated directly via:
\begin{equation}
 \mathbf{M}_1^{-1} =
 \frac{1}{D_1}
 \begin{pmatrix}
 d  & -b  \\
 -c &  a
 \end{pmatrix} 
\end{equation}
from which we conclude that $\mathbf{M}_1$ must be non-singular (i.e. the determinant $D_1$ must be non-zero). This is the general condition for the existence of an inverse matrix. Let us denote the elements of the inverse matrix as $a_i, b_i, c_i, d_i$, so we can write:
\begin{equation}
 \mathbf{M}_1^{-1} =
 \begin{pmatrix}
 a_i & b_i  \\
 c_i & d_i
 \end{pmatrix} 
 \qquad \text{where} \qquad
 a_i = \frac{d}{D_1}, b_i = -\frac{b}{D_1}, c_i = -\frac{c}{D_1}, d_i = \frac{a}{D_1}
\end{equation}
As it turns out, we may also construct the inverses of the higher-order inverse matrices $\mathbf{M}_L^{-1}$ via the very same recursive construction that we have used to construct the matrices for the forward transform:
\begin{equation}
 \mathbf{M}_0^{-1} = 1, \qquad
 \mathbf{M}_{L+1}^{-1} =
 \begin{pmatrix}
 a_i \mathbf{M}_L^{-1} &  b_i \mathbf{M}_L^{-1}  \\
 c_i \mathbf{M}_L^{-1} &  d_i \mathbf{M}_L^{-1}
 \end{pmatrix}
\end{equation}
which implies that we may use the same fast algorithm to compute the inverse transform, but with $a_i, b_i, c_i, d_i$ instead of $a, b ,c, d$. As an aside, the transposed matrix $\mathbf{M}_L^T$ may similarly be constructed recursively using $a_t = a, b_t = c, c_t = b, d_t = d$. 

% the same construction works also for recursively craeting the transposed transform from M^T_1 = [a c; b d]






\section{Unitary Matrices}
So far, we may choose $a, b, c, d$ arbitrarily. If we impose the restriction that the parameters either have the elationship $(1): c=b, d=-a$ or $(2): c=-b, d=a$, the matrix $\mathbf{M}_L$ will satisfy $\mathbf{M}_L^T \mathbf{M}_L = \diag(k)$ for some constant $k$ that depends on $a$ and $b$ via the formulas:
\begin{equation}
 r^2 = a^2+b^2, \; k = (r^2)^L
\end{equation}
To obtain an unitary transform, we would have to scale the whole matrix by $1 / \sqrt{k}$. In terms of the fast algorithm, this just means that we scale our resulting vector with that value. Alternatively, we could just preliminarily pick any two values $a', b'$ and obtain the final values $a, b$ for our unitary matrix by normalizing $a', b'$ by dividing by $r'^2 = a'^2 + b'^2$:
\begin{equation}
 r'^2 = a'^2 + b'^2, \;
 a    = \frac{a'}{r'^2}, \; b = \frac{b'}{r'^2}
\end{equation}
\textbf{[verify this]}
If we interpret the pair $(a, b)$ as a vector in the plane, we recognize $r$ as the Euclidean length of this vector. When we assume that we have done the normalization as explained above, we see that this vector has unit length, so it lies on the unit circle. Thus, when assuming such a normalization, the two parameters $a, b$ may be collapsed into a single parameter $\phi$ and letting $a = \cos(\phi), b = \sin(\phi)$.

% interpretation as sequence of 2-D rotations between 2 elements of the input (or intermediate) vector

\paragraph{}
Let's have a closer look at the 2nd case for the imposed restriction, namely $c=-b, d=a$. Then, $\mathbf{M}_1, \mathbf{M}_2$ would look like:
\begin{equation}
 \mathbf{M}_1 =
 \begin{pmatrix}
  a & b \\
 -b & a 
 \end{pmatrix}, \qquad
 \mathbf{M}_2 =
 \begin{pmatrix}
 a^2 & ab   & ab   & b^2  \\
 -ab & a^2  & -b^2 & ab   \\
 -ab & -b^2 & a^2  & ab   \\ 
 b^2 & -ab  & -ab  & a^2  \\ 
 \end{pmatrix} 
\end{equation}
from which we see that the main diagonal is solely populated with $a^2$, the subdiagonal is populated with $\pm b^2$ and the off-diagonal elements are populated by the cross-terms. By inspection, we may convince ourselves that for $\mathbf{M}_3$, we would see $a^3$ on the main diagonal, $\pm b^3$ on the other diagonal and it goes on that way for higher order matrices.



\section{Open Questions}
\begin{itemize}
	\item How do the basis vectors of the transform look like (maybe as functions of $a,b,c,d$)? 
	\item Can something be said about $M^T M$ for the general case of $a,b,c,d$?
	\item Can something interesting be obtained about the eigenvalues/-vectors? 
	\item Can something be said about various matrix-norms?
	\item What about matrices with complex elements?
	\item Can we find expressions for $\det(\mathbf{M}_L - \lambda \mathbf{I})$ and/or for the eigenvalues and -vectors?
	\item If not in general possible, can it be done at least for the unitary cases?	
\end{itemize}


% maybe note that a basis vector (i.e., a row of the matrix) of the transform can be obtained without constructing the matrix by applying the fast transfrom algorithm to a unit vector withh all zeros except one 1 at the respective position that corresponds to the desired matrix row

%\section{Applications}
























\begin{thebibliography}{9}  % 9 indicates that there are no more than 9 entries
 %\bibitem{Gum} Charles Constantine Gumas. A century old, the fast Hadamard transform proves useful in digital communications
 \bibitem{Sil} John R. Silvester. Determinants of Block Matrices 
\end{thebibliography}

