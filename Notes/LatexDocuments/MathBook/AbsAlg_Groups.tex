\section{Group Theory} 
One of the most fundamental algebraic structures is called a group. A group is defined by an underlying set and one binary operation on elements of this set. An example of a group would be the integer numbers with addition as the operation. One typically denotes a group as a pair (in the sense of 2-tuple) consisting of the set and the operation, so the group of integers with addition is denoted by $(\mathbb{Z},+)$. Let's abstract from this concrete example and consider an arbitrary set $G$ on which an arbitrary binary operation, denoted by $\circ$, is defined and consider the pair $(G,\circ)$. For such a pair to qualify as a group, the operation $\circ$ has to satisfy the following rules:

\medskip
\begin{tabular}{l l}
Closure: 
& $\forall a,b \in G: \; a \circ b \in G$  \\	
Associativity: 
& $\forall a,b,c \in G: \;  (a \circ b) \circ c = a \circ (b \circ c)$   \\
Existence of neutral element: 
& $\exists e \in G: \; (\forall a \in G: e \circ a = a)$ \\
Existence of inverse elements: 
& $\forall a \in G: \; (\exists a^{-1} \in G: a^{-1} \circ a = e )$ \\
\end{tabular}
\medskip

Let's translate that foreign mathspeak to plain English: Closure means that when we combine any two elements from our set with the operation, the result will be another element from the set. We also say that the set is \emph{closed} under the given operation. Sometimes, this first rule is not even explicitly stated as such but just somehow implicitly assumed by defining that all binary operations already have to satisfy this to be even called by the name binary operation. But if one allows a more general notion of what a binary operation is, allowing inputs and outputs to come from different sets: $op: A \times B \rightarrow C$, it makes sense to explicitly require this closure. Associativity requires that when we combine 3 elements $a,b,c$ via our operation by firstly combining $a$ and $b$ and then secondly combining the result of that with $c$ yields the same final result as combining $a$ with the result of the combination of $b$ and $c$. This is an abstraction of the idea that $(2+3)+4 = 2+(3+4)$ because $5+4 = 2+7$. The second rule says that there must exist some special element $e$ within the set which, when combined with any element $a$ from the set, just gives back $a$ itself. This special element $e$ is called the neutral element because it changes nothing when it's being combined with any element from the set. It's an abstraction of the idea that $0+x = x$ for any $x$ whatsoever. The number zero is the neutral element of addition in the integers. At the outset, it may seem that we should actually be more precise and call it the left-neutral element because we require it to behave neutrally only from the left. However, group theory gives us the theorem that any left-neutral element is automatically always also right-neutral, so it's justified to just call it the neutral element without any left or right qualifier. Group theory also gives us the theorem that such a neutral element is always unique so it's indeed justified to call it \emph{the} neutral element instead of \emph{a} neutral element. By the way, it is common to refer by the term "group" also to the underlying set rather than the pair if it's clear from the context about which operation we talk. The last rule requires that for any element $a$ from the group there must exist some element $a^{-1}$ also from the group, such that when we form the combination $ a^{-1} \circ a$ via our group operation, then the neutral element will come out as result. This element $a^{-1}$ is called the inverse element to $a$. So, the rules says that such inverse elements must exist for all group elements. In the integers with addition, the inverse element to $3$ is $-3$ because $-3 + 3 = 0$ and zero is the neutral element. Again, we need to require only the existence of left-inverses and group theory assures us that any left-inverse element is proven to always automatically also be right-inverse and is unique so we are justified to call it the inverse element. We may begin to see the value of group theory. If we want to verify that right-inverses exist in some brand new mathematical structure that we came up with, we actually just need to check these three simple conditions and if they hold true, we are good and can in fact assert a lot more properties of our structure without needing to verify them all separately.

\paragraph{Notation and Terminology}
So far, we have used the symbol $\circ$ in infix notation like in $c = a \circ b$ to denote the combination of elements $a,b$ by our group operation. This was done to indicate that we don't not necessarily mean a concrete well known operation such as addition or multiplication. Depending on the context, the group operation may indeed be addition. Or it may be multiplication. Or something else entirely. It may also mean the composition of certain \emph{actions} in the sense of performing one action after another. We may be used to the circle being used for function composition and indeed, if we consider only invertible (i.e. strictly monotonic) functions, they do indeed form a group under function composition (where "under" here means: "with the group operation being"). This usage of the circle is not universal. You will also often encounter the dot being used: $c = a \cdot b$ and sometimes the operation symbol is scrapped entirely: $c = a b$ like we often do with multiplication when dealing with numbers, matrices, functions and so on. From now on, we will also adopt these notations. We will also denote the combination of an element $a$ with itself as $a^2$, i.e. $a^2 = a a = a \cdot a$. In general we will denote by $a^n$ the idea to combine $a$ with itself $n-1$ times. That is, we have $n$ "factors" all of them being $a$ and we have $n-1$ "dots" in between them: $a^4 = a \cdot a \cdot a \cdot a$. The group operation is sometimes just called "multiplication" even though it is understood that a generalized, more abstract operation is meant. In the definition of a group, we did not require the operation to be commutative. However, if the operation does happen to be also commutative, i.e. if $a b = b a$ for all $a,b \in G$, then we call the group a \emph{commutative} group or some people also call it an \emph{Abelian} group. I prefer use descriptive names whenever possible - no offense to Niels Henrik Abel (hopefully). In such a commutative group, it makes no difference whether we pre- or post-multiply by an element. This holds of course also for inverses, so $c = b^{-1} a$ is equivalent to $c = a b^{-1}$ for any $a,b \in G$. That's why, for commutative groups, you'll sometimes find the division notation $c = \frac{a}{b}$ because the inherent ambiguity in this notation about whether pre- or post-multiplication of $a$ by $b^{-1}$ is meant, makes no difference because both ways yield the same result anyway. We won't use that notation here, though because we won't assume our groups to be commutative in general. If we are dealing with such a commutative group, we will have even more theorems at our disposal. When we are dealing with a group that is not generally commutative, it may still happen that for some specific choice of $a,b$ the equation $a b = b a$ holds. In such a case, we say that "$a$ and $b$ commute".
% Maybe use the dot right from the start

\subsection{Some Elementary Theorems}
Let's axiomatically assume that our rules stated above are true for some given pair of a set and an operation that is of interest to us. Let's say, we have painstakingly verified them by hand and have convinced ourselves that our pair does indeed form a group. What else can we deduce from the group axioms alone without knowing anything more about our pair other than it being a group? What other properties can we assert about our pair without actually verifying them by hand? Without proof, I'll hand you down the following useful facts for any group, some of which were already mentioned: 
\begin{enumerate}
\item Neutral and inverse elements are unique. 

\item Left neutral elements are also right-neutral and left-inverses are also right-inverse. If $a^{-1} a = e$ then $a a^{-1} = e$. Note that this does \emph{not} imply that pre- or post-multiplying any \emph{other} element by an inverse makes no difference! It \emph{does} make a difference, i.e. $a^{-1} b \neq b a^{-1}$ in general unless the group is commutative. It only says that $a$ and $a^{-1}$ commute but any other $b$ may not commute with $a^{-1}$.

\item We have general rules for exponents: $a^m a^n = a^{m+n}$ and $(a^{m})^{n} = (a^{n})^{m} = a^{n m}$. As a special case of this, we have the rule that inverting twice gives back the original: $(a^{-1})^{-1} = a$ because $(-1) \cdot (-1) = 1$.

\item In a power of a sandwich of the form $a^{-1} b a$, we can drag the exponent in to apply only to $b$: $(a^{-1} b a)^{-1} = a^{-1} b^n a$. This useful in the context of matrix exponentiation via diagonalization.

\item If $a$ and $b$ commute, i.e. if we have $a b = ba$ then we also have: $(a b)^n = a^n b^n = b^n a^n$ and $a b^n = b^n a$. In commutative groups, all $a,b$ commute so there, these laws hold for any $a,b$.

\item An inverse of a product is the product of the inverses in reverse order: $(a b c)^{-1} = c^{-1} b^{-1} a^{-1}$. I've written it down for three factors but it holds for any number of factors. In a commutative group, the order doesn't even matter so we can freely rearrange the factors however we like [VERIFY!]. 

\item The cancellation law says that if $a b = a c$ then $b = c$. We can cancel the $a$ from both sides. This works also for right-multiplication: if $b a = c a$ then $b = c$. 

\item If we have an equation like $a x = b$ with known $a, b$ and unknown $x$, we can solve for $x$ by pre-multiplying both sides with $a^{-1}$ to get $x = a^{-1}b$. Likewise, if the equation is $x a = b$, we can solve for $x$ by post-multiplying by $a^{-1}$ to get $x = b a^{-1}$. 
\end{enumerate}

\medskip
I hope you will agree that these are quite a lot of potentially useful facts. Especially the ability to solve equations for unknowns  may be a big boon in many situations. Some of these facts may not be immediately obvious. But they have been proven from the group axioms alone, so they \emph{will} hold for \emph{your} group, if it indeed is a group, i.e. if you were able to verify the 4 simple requirements. There are many more facts to come that your group will also automatically satisfy just by virtue of being a group. That is why we are doing all of this - so you don't have to. Abstract algebra is about avoiding duplication of work. Sorry, if I sometimes fall into the habit of sounding like one of those annoying motivational speakers. But being the lazy person that I am, I really appreciate the avoidance of duplication of work. ;-) It's all about automation and efficiency. About working smart instead of working hard. About doing things once and for all instead of again and again. The art and craft of this is to abstract and distill out those features that actually matter and are applicable to as many concrete problems as possible. It turns out that especially associativity is such an important property for an operation to have because it allows us to turn a binary operation in a well defined way into an operation that can operate on any number of inputs.

%fold a repeatedly applied binary

%https://www.cs.utoronto.ca/~szhao/group_theory_thms_defs.pdf
%Nice list of elementary facts

\medskip

%(8) The division axiom states that given any pair of elements $a,b$, there exist unique group elements $x,y$ such that $a x = b$ and $y a = b$, namely $x = a^{-1} b$ and $y = b a^{-1}$ [VERIFY!]. That means, we can solve equations for unknown variables. (see Riley/Hobson et al)

%\medskip
%\begin{tabular}{l l}
%Neutral elements are unique \\
%Inverse elements are unique \\	
%Left-neutral elements are also right-neutral \\
%Left-inverse elements are also right-inverse \\
%Inverses of inverses give back the original: & $(a^{-1})^{-1} = a$ \\
%Inverse of the square is square of inverse: & $(a^{2})^{-1} = (a^{-1})^{2} $ \\
%More generally - rule for exponents: & $(a^{m})^{n} = (a^{n})^{m} = a^{n m} $ \\
%\end{tabular}
%\medskip
% -uniqueness of inverses and neutral element
% -power rules
% In this context, it is natural to define $a^0 = e$.
% Q: 
% -could there be elements that are partially neutral, i.e. leave only some elements
%  unchanged but not all?

% https://math.stackexchange.com/questions/2930287/exponent-rules-under-a-group-g
% https://math.stackexchange.com/questions/2199421/do-the-laws-of-exponents-apply-to-a-group-as-for-real-numbers
% https://mathstats.uncg.edu/sites/pauli/112/HTML/secgroupexp.html


\subsection{Multiplication Tables}
Let's assume that we have a finite set $A$ with $N$ elements and let's assume that we have labeled all set elements by a unique index between $1$ and $N$, so our set is $A = \{ a_1, a_2, a_3, \ldots, a_N \}$. To define a binary operation on such a set, we may write down a table in which the headers of the rows and columns are the set elements and the cells tell us, what output the operation will produce, when we combine the elements that appear in the headers of our rows and columns. That means, when we view the table as a matrix $T$, the element $t_{ij}$ of the matrix will be given by the product $a_i a_j$. I call it a "matrix" here because that's the data structure one might naturally want to use for this in an (naive) implementation - but we do not intend to do any linear algebra with it. For our purposes here, it's just a 2D table of elements which we want to read out and I may use the terms "matrix" and "table" interchangably. For $N=5$, such a table would look like this:
\[
\setlength{\extrarowheight}{5pt}% local setting
\begin{array}{l|*{5}{c}}
	    & a_1   & a_2  & a_3 & a_4  & a_5 \\
	\hline
	a_1 & a_1 a_1 & a_1 a_2 & a_1 a_3 & a_1 a_4  & a_1 a_5 \\
	a_2 & a_2 a_1 & a_2 a_2 & a_2 a_3 & a_2 a_4  & a_2 a_5 \\
	a_3 & a_3 a_1 & a_3 a_2 & a_3 a_3 & a_3 a_4  & a_3 a_5 \\
	a_4 & a_4 a_1 & a_4 a_2 & a_4 a_3 & a_4 a_4  & a_4 a_5 \\
	a_5 & a_5 a_1 & a_5 a_2 & a_5 a_3 & a_5 a_4  & a_5 a_5 \\
\end{array} 
\]
This table is supposed to represent the results of our group operation. From the row header, we take the left factor $a_i$, from the column header, we take the right factor $a_j$ and into the cell $t_{ij}$, we write the product $a_i a_j$. Again, I say "factor" and "product" but actually mean it in a more general and abstract sense as "operand" and "result". Such a table of all possible products of group elements is called a multiplication table, also known as Cayley table. By using such tables, we have a completely general way to specify any binary operation on a finite set of elements. We just need to define an indexing scheme for our set elements and then write the values of all possible products into the cells of the table. It is customary to write the neutral element into the header of the first row and first column.

\medskip
Now, for the table to specify an operation that does satisfy our group requirements, it must satisfy some constraints. Firstly, it must be a so called Latin square which is a matrix in which each row and each column contains every element from the set exactly once. That this is necessary can be seen as follows: suppose you know a product $p = a_i a_j$ and you also know one of the factors, say $a_i$. Then, it should be possible to find the other factor $a_j = a_i^{-1} p$ by scanning the row of $a_i$ until you find $p$ and then read off the column header where you found it. That must be $a_j$. But for that to work for any $p \in G$, you can't have duplicate elements on the rows because then you wouldn't know, which column header you should read off - and hence, the product is not uniquely invertible. This can't be in a group. Another constraint is that there must be some row and column that just lists the elements in the same order as they appear in the header - that's the row/column of the neutral element. They must cross on the diagonal, i.e. row and column index must be the same. These criteria are rather easy to check. If they fail, the table is not a valid multiplication table. But if they hold, the table could still be invalid. These criteria are necessary but not sufficient. We also need to verify the associativity and that's the hard part. Of course, it can naively be verified algorithmically in $\mathcal{O}(N^3)$ by verifying for every possible triple $a_i, a_j, a_k$ that $(a_i a_j) a_k = a_i (a_j a_k)$. There are algorithms that can do better than that in certain cases (search for "Light's associativity test") but as of now, this seems indeed to be an $\mathcal{O}(N^3)$ problem in terms of worst case complexity.

\medskip
But be that as it may. Multiplication tables are an important tool in the analysis of finite groups because these tables completely specify (and hopefully reveal) the structure of a given group at hand. For example, we may easily find out whether or not a group is commutative. This is the case, if and only if the multiplication table is symmetric across the main diagonal. But as we will see when we come to the topic of subgroups, finite groups can have more structure than that. But after all that abstract stuff, let's first have a look at...

% Not every Latin square is a Cayley table:
% https://proofwiki.org/wiki/Latin_Square_is_not_necessarily_Cayley_Table_of_Group

% https://math.stackexchange.com/questions/1297564/verifying-if-a-multiplication-table-is-from-a-group

% https://math.stackexchange.com/questions/168663/is-there-an-easy-way-to-see-associativity-or-non-associativity-from-an-operation

% https://en.wikipedia.org/wiki/Light's_associativity_test

% Q: Does it actually make any sense to do linear algebra with Caley tables? If we have two Caley tables of two different groups (of the same size) - what is their product? Will it also be a Caley table of a group? ...maybe try it with some examples. I guess that would mean that Latin squares would need to form a subgroup of the group of all matrices under matrix multipication. Do they?
% https://en.wikipedia.org/wiki/Latin_square

\subsection{Some Examples}
Now that we have defined what a group is in an abstract sense and have a tool in hand to specify a group, let's look at some concrete examples of very different groups to get an appreciation for how surprisingly universally applicable these concepts are.

\subsubsection{Numbers}
We have already mentioned that the integer numbers form a group under addition. So do the rational numbers, real numbers and complex numbers. So, in math notation that means:  $(\mathbb{Z}, +), (\mathbb{Q}, +), (\mathbb{R}, +), (\mathbb{C}, +)$ all form groups. These latter three number systems also form groups under multiplication if we remove the number zero. If we denote by a superscript asterisk the set with the zero removed, for example $\mathbb{Q}^{\ast} = \mathbb{Q} \setminus \{0\}$, then we can state with math notation: The pairs $(\mathbb{Q}^{\ast}, \cdot), (\mathbb{R}^{\ast}, \cdot), (\mathbb{C}^{\ast}, \cdot)$ also form groups. A bit less familiar, but very important in number theory and some areas of computer science are the modular integers. I don't want to digress too much into explaining what they are here at this point, but if know what they are, then let me tell you that they also do form groups under modular addition. If the modulus is prime, they also form groups under modular multiplication. They actually even form fields, in this case - a more complex algebraic structure that we will encounter later. There are many more number systems that have the structure of a group and often even more complex structures. 

\subsubsection{Matrices}
Consider the set of all regular (i.e. invertible) $n \times n$ matrices for some $n$. This set forms a group under matrix multiplication. This group is clearly not commutative because we know that matrix multiplication isn't. You may find many different subsets of this group of \emph{all} invertible matrices which are closed under matrix multiplication and inversion and which contain the identity matrix. These subsets may be finite or infinite. Let's take $n=2$. A simple example for a finite subset of that kind is given by the set of matrices that rotate by multiples of 90 degrees:
\begin{equation}
r^0 = \begin{pmatrix}  1 &  0 \\  0  &  1  \end{pmatrix}, \;
r^1 = \begin{pmatrix}  0 & -1 \\  1  &  0  \end{pmatrix}, \;
r^2 = \begin{pmatrix} -1 &  0 \\  0  & -1  \end{pmatrix}, \;
r^3 = \begin{pmatrix}  0 &  1 \\ -1  &  0  \end{pmatrix}
\end{equation}	
I have suggestively written the index as superscript like an exponent because they are indeed all powers of $r^1$. We see a little bit of foreshadowing to generators here. The element $r^1$ generates the whole group via its powers. Note also that $r^2$ is the negative identity matrix. It's a sort of $-1$ within the realm of matrices. Note furthermore that $r^1$ squares to $r^2$. That means $r^1$ behaves like an imaginary unit. That's no coincidence. If we consider the vector space spanned by $r^0$ and $r^1$, we get something that is completely isomorphic to the complex numbers [VERIFY!]. But that's just an aside. The important takeaway for the context of group theory is that there exist interesting subsets of the big set of all $n \times n$ matrix that exhibit the structure of a group. An example of an infinite set of that kind would be the set of all 2D rotation matrices where the angle can be any real number. That set happens to be isomorphic to all complex numbers with unit magnitude, i.e. complex numbers of the form $e^{\i \theta}$ for some real number $\theta$ that is interpreted as angle. Rotations in higher dimensions also form groups. Also of interest are matrices over the complex numbers. We'll hear more about all of that in the section about Lie groups.

% Maybe introduce a bit of jargon like SO(2), SU(2), ...but maybe do that in a later section about Lie groups



% bring the set of rotations by multiples of 90° as example. point out that one of the matrices behaves like the imaginary unit. give the cayley table, call the matrices e, r, r^2, r^3...hmm..maybe using I for the identity is better than e

\subsubsection{Symmetries}
Imagine a regular polygon in the $xy$-plane centered at the origin. Maybe let's take the square. It is a very symmetrical shape in the sense that we can perform a lot of geometric transformations to it that leave it looking exactly the same. We could reflect it about a horizontal or a vertical axis or about one of the two diagonal axes. We could also rotate by the angles 90, 180 or 270 degrees and that would also leave our square unchanged. Of course, the "do nothing" operation also leaves it unchanged. If we want to make a group, we need a neutral element, so the do nothing operation aka identity has to be also in the set. Let's call that identity $e$, as always. Let's call the 90 degree rotation $r$ and note that the other two rotations can be obtained by applying $r$ twice or thrice respectively - so let's call them $r^2$ and $r^3$. Let's call the horizontal and vertical reflections $h$ and $v$ and the reflection about the downward and upward diagonals $d$ and $u$. I now claim that the set $\{ e,r,r^2,r^3,h,v,d,u \}$ forms a group under the operation of performing one of these actions after another. That group is called the symmetry group of the square. I could now give you the multiplication table for the eight possible products. But I won't - because in this case, we have something much better: an explicit formula for its entries that works not only for a square but for any regular polygon. Generally, every regular polygon with $n$ sides has such a symmetry group and it will always consist of exactly $n$ rotations and $n$ reflections. Let $r_0,\ldots,r_{n-1}$ denote all the rotations and $s_0, \ldots,s_{n-1}$ denote all the reflections. Then the products (i.e. compositions) of any of these can be computed by:
\begin{equation}
r_i r_j = r_{i+j}, \quad
r_i s_j = s_{i+j}, \quad
s_i r_j = s_{i-j}, \quad
s_i s_j = r_{i-j}
\end{equation}	
where the additions of the indices have to be performed modulo $n$. As we know from analytic geometry, we can represent general rotations and reflections by $2 \times 2$ matrices over $\mathbb{R}$ and that we can represent the combination of such transformations as multiplications of the respective matrices. The matrices for the $r_k, s_k$ are in this case given by:
\begin{equation}
\theta_k =\frac{2 \pi k}{n},  \quad
c_k = \cos(\theta_k), s_k = \sin(\theta_k), \quad	
r_k = \begin{pmatrix}  c_k & -s_k \\  s_k  &  c_k  \end{pmatrix}, \;
s_k = \begin{pmatrix}  c_k &  s_k \\  s_k  & -c_k  \end{pmatrix} \quad
\end{equation}	
The identity transformation is included as $r_0$. The fact that we can \emph{represent} the symmetries via matrices is a little foreshadowing into the field of representation theory which asserts that \emph{every} group can be represented as a group of matrices where the group operation translates to matrix multiplication. The symmetry groups of regular polygons are also called \emph{dihedral} groups. The etymology has something to do with polygons having two ("di") faces ("hedrons") but I don't really get what this has to do with it. The explanations I have seen so far are not really satisfying. [ToDo: try to figure it out]

% https://en.wikipedia.org/wiki/Dihedral_group
% https://en.wiktionary.org/wiki/dihedral_group
% https://de.wikipedia.org/wiki/Diedergruppe

% mention that we can represent these symmetries via matrices - a foreshadowing to representation theory.
% the r, r2, r^3 thing is a foreshadowing to generators


% https://proofwiki.org/wiki/Symmetry_Group_of_Square/Cayley_Table

\paragraph{Functions}
The set of all invertible (i.e. bijective) functions from any set $S$ to itself forms a group under the operation of function composition. It has even a special name: it's called the symmetric group over $S$. Let's verify the axioms: Closure: chaining a bijective function from $S$ to $S$ with another bijective function from $S$ to $S$ gives of course a combined function that is again a bijective function from $S$ to $S$. Associativity: For three functions $f,g,h$, if you apply ($f$-after-$g$) after $h$, it's the same as applying $f$ after ($g$-after-$h$). Saying it like this makes it obvious that function composition is actually always associative - no matter whether the functions are bijective or not. The neutral element is just the identity function and it is indeed invertible (it's its own inverse) and the existence of inverse functions is assured by requiring the functions in our set to be invertible (Duh! Thank you Captain Obvious!).

...TBC...

% 

%Invertible Functions with Composition

%https://en.wikipedia.org/wiki/Symmetric_group

\paragraph{Permutations}
Another important class of groups is that of permutation groups. In a certain sense, they can be seen as the prototypical examples of all groups. Permutation groups were studied before group theory was even a thing and in hindsight, it turned out that important results that were found for these permutation groups were in fact more general and apply to all groups. A permutation group consists of the set of all permutations, i.e. re-orderings, that we can apply to an ordered collection of $n$ objects. It is clear that when we permute (reorder) such a collection twice in a row, the total result is again a permutation (reordering). So, closure is verified. It's also clear that we can invert any reordering by an action that is again a permutation. So, existence of inverses is verified. The do-nothing action also counts as a permutation - that's our neutral element. Associativity checks also out because a permutation is just a special kind of function and function composition is always associative ...TBC...

%That's why an important theorem of group theory, namely Lagrange's theorem, is named after Joseph-Louis Lagrange

%https://en.wikipedia.org/wiki/Permutation
%https://en.wikipedia.org/wiki/Permutation_group
%https://en.wikipedia.org/wiki/Cayley%27s_theorem
%https://en.wikipedia.org/wiki/Symmetric_group

% https://math.libretexts.org/Bookshelves/Combinatorics_and_Discrete_Mathematics/Applied_Discrete_Structures_(Doerr_and_Levasseur)/15%3A_Group_Theory_and_Applications/15.03%3A_Permutation_Groups

% https://mathresearch.utsa.edu/wiki/index.php?title=Composition_of_Functions

% use the Klein Four group as example...or maybe the group of symmetries of a triangle

%\subsection{Some Examples}
%Now that we have defined what a group is in an abstract sense, let's look at some concrete examples of very different groups to get an appreciation for how surprisingly universally applicable these concepts are.
%
%
%\paragraph{Integers with Addition}
%
%\paragraph{Nonzero Rationals with Multiplication}
%
%\paragraph{Modular Integers with Modular Addition}
%
%\paragraph{Rotations in the Plane}
%
%\paragraph{Permutations}
%%bring this last

%%\paragraph{The Modular Integers with Prime Modulus with Modular Multiplication}
% Bool

%\paragraph{Invertible Functions with Composition}


\paragraph{}
OK - now we have seen a couple of examples of groups that seem to be very different. Yet, they all share a lot of commonalities which are distilled in group theory into theorems that abstract away from the concrete examples. Let's now take a deeper dive into what these commonalities are...


\subsection{Subgroups}
If a subset of the underlying set of a group forms itself a group under the same operation, then that subset together with the operation forms what is called a subgroup of the original group. For example, the even integers under addition form a subgroup of all the integers under addition. To see this, we just need to verify the group axioms for the even integers: Closure: Sums of even integers are again even - check. Associativity: Inherited from integer addition - check. Neutral element: Zero is among the evens - check. Inverse elements: Negating an even integer gives another even integer - check. Done. The even integers form a subgroup of the integers. The odd numbers, on the other hand, do not form a subgroup of the integers. They fail to contain the neutral element - zero is not odd. They are not even closed under addition: adding two odd numbers gives an even number. Associativity works out fine and inverses are also there - but that's not enough.

\medskip
The integers under addition are an infinite group. But we now want to focus on finite groups because many interesting theorems about subgroups are about those finite groups. In a certain way that is analogous to the prime factorization of natural numbers, finite groups can be factored into simpler constituents using the machinery of subgroups. 

\subsubsection{Cosets}



\subsection{Homomorphisms and Isomorphisms}
Assume we have a group $\mathcal{G} = (G, \circ)$ and another group $\mathcal{H} = (H, \diamond)$. Assume furthermore that we have a function $f: G \rightarrow H$ that maps elements from $G$ to elements from $H$. That function is called a group homomorphism from $G$ to $H$ if it has the following property for all $a,b \in G$:
\begin{equation}
f(a \circ b) = f(a) \diamond f(b)
\end{equation}
This equation says that it doesn't matter whether we firstly combine $a$ and $b$ in $G$ via $\circ$ and then secondly map the result over to $H$ via $f$ or if we firstly map $a$ and $b$ separately over to $H$ via $f$ and then secondly combine them over there via $\diamond$. The end result should be the same in both cases. Such a function $f$ is constrained to preserve the structure of group $G$ while mapping over to group $H$ and it may or may not exist. If such a homomorphism does exist then we can find the structure of group $G$ within group $H$ in the sense that the image of $G$ is a subgroup of $H$. Note that, as promised, I'm sloppy with using $G$ and $H$ as stand-in in some places where it would actually be more correct to write $\mathcal{G}$ and $\mathcal{H}$. Formally, it makes no sense to talk about a subgroup of $H$ because $H$ is just a set and the group is actually the pair $\mathcal{H}$. But to state that stuff formally correctly would be very awkward and confusing. Formally correctly, one would have to say that $f$ maps $G$ to a subset of $H$ and that subset happens to be the underlying set of a subgroup of $\mathcal{H}$. Nobody wants to speak like that so this slight sloppiness is quite common and acceptable in group theory [VERIFY!]. Anyway - the essence of a homomorphism is that the function $f$ maps from $G$ to $H$ and thereby preserves the structure of $\mathcal{G}$ within $\mathcal{H}$ - and such a thing may or may not be possible for two given groups at hand. If it is possible, then we may say that $\mathcal{H}$ is homomorphic to $\mathcal{G}$ which means that we find the structure of $\mathcal{G}$ within $\mathcal{H}$ [VERIFY!].

\paragraph{Example}
Let  $\mathcal{G} = (\mathbb{R}, +), \; \mathcal{H} = (\mathbb{R}^+, \cdot)$ and $f: \mathbb{R} \rightarrow \mathbb{R}^+$ with $f(x) = e^x$. That is, we consider the exponential function as a map from the real numbers with addition to the positive real numbers with multiplication. It is indeed a homomorphism between these two groups because $e^{a+b} = e^a \cdot e^b$ for all $a,b \in \mathbb{R}$. On the left hand side, we first add $a$ and $b$ (adding is the group operation of $\mathcal{G}$) and then map the sum over via $f$. On the right hand side, we first map $a$ and $b$ over seperately via $f$ and then multiply (multiplication is the group operation of $\mathcal{H}$). As we know from the laws of exponential functions, the result is the same, so $f$ is indeed a homomorphism.

% maybe say something about the logarithm as inverse homomorphism...but maybe later
% verify that neutral and inverse elements get mapped to their counterparts

\paragraph{} Homomorphisms have some noteworthy properties: ...TBC...


%https://mathworld.wolfram.com/GroupHomomorphism.html



\begin{comment}

https://en.wikipedia.org/wiki/Group_(mathematics)
https://en.wikipedia.org/wiki/Cayley_table
https://en.wikipedia.org/wiki/Dihedral_group

https://en.wikipedia.org/wiki/Category:Theorems_in_group_theory

-what about Cayley graphs?

\end{comment}