\section{Polynomial Algebra}
Linear algebra is, to a large part, about finding solution vectors $\mathbf{x}$ to equations of the general form $\mathbf{A x} = \mathbf{b}$ which are called linear equations. More generally, algebra is about the solution of algebraic equations which are defined to be equations that can be formed by using the basic arithmetic operations and roots. The roots are included because they are thought of as being inversions of the operation of repeatedly multiplying an object by itself so they are in some sense induced by the multiplication operation. Polynomial equations are an important subset of algebraic equations. In one dimension, these are equations of the general form:
\begin{equation}
 \sum_{n=0}^N a_n x^n = 0
\end{equation}
where the $a_n$ are known quantities, called the coefficients and $x$ is an unknown \emph{variable} or \emph{indeterminate} that we want to find. If on the right hand side, we have some other constant than $0$, we can just move it over to the left hand side and absorb it in the $a_0$ coefficient. The left hand side is called a polynomial in $x$. We have already met polynomials as a certain type of functions: we can plug in a value $x$ and it spits out a value $f(x)$. When polynomials are viewed as functions, it makes sense to ask: for which values of $x$ does the function produce zero as output. That question is what solving the equation $f(x) = 0$ encodes. Besides the view of polynomials as functions that produce an output for a given input and as equations that we want to solve, we may also view polynomials as formal expressions. Viewed as such an expression, a polynomial is determined by its sequence of coefficients and we forget about the functional aspect - we just think of it as a finite sequence of numbers. These sequences of polynomial coefficients admit an algebra among themselves: we can add, subtract, multiply and (with caveats) divide such sequences of polynomial coefficients. We may even consider infinite sequences which leads us to the idea of formal power series. With such power series, the formal point of view really yields a generalized view because power series can be interpreted as functions only when the series is convergent - but when seen just as formal expressions, we may work with them even when the series does not converge. With polynomials, we don't have that convergence problem, so we can always view a polynomial as a function - but in some contexts, we may voluntarily de-emphasize that point of view. It should also be noted that there are settings in which two polynomials with different coefficients encode the same function. That happens when we do not work over our usual real or complex numbers but over a finite ring. Over finite sets, there is only a finite number of possible functions from the set to itself but there is still an infinite number of different coefficient sequences so some of them must encode the same function. From this point of view, the formal and functional view of polynomials differ in what it means for two polynomials to be equal to one another. The formal sense of equality of polynomials is more restrictive than the functional sense. In the formal view, the sequences of coefficients must match whereas in the functional view, only the input/output relation must match which is, in some contexts, a weaker requirement as we have seen. Over infinite rings though, the two requirements are the same [VERIFY!].

...TBC...

%===================================================================================================
\subsection{Operations on Polynomials}

%---------------------------------------------------------------------------------------------------
\subsubsection{Arithmetic Operations}

\paragraph{Addition and Subtraction}

\paragraph{Multiplication}

\paragraph{Division}

\paragraph{Greatest Common Divisor}

% -Euclidean algorithm for polynomials
% -Extended euclidean algorithm
% -least common multiple

\paragraph{Composition}

%---------------------------------------------------------------------------------------------------
\subsubsection{Calculus Operations}

\paragraph{Derivative}

\paragraph{Antiderivative}

\paragraph{Definite Integrals}


%---------------------------------------------------------------------------------------------------
\subsubsection{Miscellaneous Operations}

\paragraph{Evaluation}

\paragraph{Base Change}



%===================================================================================================
\subsection{Roots and Factorization}

% -Factorization



%===================================================================================================
\subsection{Multivariate Polynomials}



%===================================================================================================
\subsection{Rational Functions}




\begin{comment}

-maybe put the section between Calculus and Multilinear Algebra

ToDo:
-Move some of the content from the section about functions over to here
-Base change (maybe under some sort of transformatiuons) where we can also list scaling or shifting
 the argument, raising the argument to a power
-Root finding, Viete's theorem, formulas for quadratic, cubic and quartic case, unsolvability of
 quintic case, companion matrix
-Construction of (sets of) polynomials with certain properties
 -Interpolation polynomials
 -Approximation polynomials
 -Orthogonal polynomials
-Multivariate polynomials
 -elementary symmetric polynomials 

% Interpretation of polynomials as:
% -functions
% -equations to be solved
% -formal expressions
%  -this view generalizes to power series even when they do not converge

-Two polynomials seen as formal expressions are equal iff all their coeffs are equal. 
 When viewed as function, it may happen that two polynomials with different coefficients
 give rise to the same function. That happens when the domain and range of the polynomial
 is a finite ring. For such finite rings, there is only a finite number of functions from
 the ring to itself but there is an infinite number of different coefficient sequences, so
 some of these sequences must encode the same function.
 
-As formal expressions, we can define an algebra on the set of polynomials itself: 
 polynomials can be added and multiplied by one another. There is even a notion of 
 polynomial division with remainder
 
ACRS, pg 17 ff 
 
 ACRS, pg 34: field of fractions ..done in the fields section
 
-Make subsections for:
 -Unsolvability of the Quintic (theorem of Abel-Ruffini)
 -Sylvester matrix, resultant, elementary symmetric polynomials
 -discriminant, which is essentially the resultant of a polynomial and its derivative, see
  https://en.wikipedia.org/wiki/Resultant


ABoAA
-pg 261: if s/t is a root of a(x) in Z[x] the s|a_0 and t|a_n, i.e. a_0 divides s and a_n divides t

https://en.wikipedia.org/wiki/Resultant
https://en.wikipedia.org/wiki/Elimination_theory
https://en.wikipedia.org/wiki/Gr%C3%B6bner_basis
https://en.wikipedia.org/wiki/System_of_polynomial_equations
https://en.wikipedia.org/wiki/B%C3%A9zout%27s_theorem
https://en.wikipedia.org/wiki/Multi-homogeneous_B%C3%A9zout_theorem

https://en.wikipedia.org/wiki/Reciprocal_polynomial

https://en.wikipedia.org/wiki/Polynomial_greatest_common_divisor#Euclid's_algorithm
https://en.wikipedia.org/wiki/Polynomial_greatest_common_divisor#Subresultant_pseudo-remainder_sequence
 
https://en.wikipedia.org/wiki/Sturm%27s_theorem 
https://en.wikipedia.org/wiki/Sturm%27s_theorem#Generalized_Sturm_chains




Der Fundamentalsatz über symmetrische Polynome:
https://www.youtube.com/watch?v=_qnj3Iz6vL4

Every symmetric polynomial can be expressed as algebraic combination of the elementary symmetric polynomials in a unique way. An "algebraic combination" is basically a polynomial. So we can construct *every* symmetric polynomial as polynomial of polynomials, namely the *elementary* symmetric polynomials. In Mathematica, the function SymmetricReduction can do this - takes as input a symmetric polynomial and expresses it as polynomial of elementary symmetric polynomials.

Viete's theorem: the coefficient a_k of any polynomial p(x) in x of degree n with n roots x_1,...,x_n is given by the elementary symmetric polynomial e_{n,k} (or is it e_{n,n-k} ?) in the roots x_1, ..., x_n. But there's also a sign alternation, so the polynomial must be prepended by (-1)^k or (-1)^(n-k) or something. So, the theorem tells us how to express the coeffs of p(x) in terms of elementary symmetric polynomials in x_1...x_n - the coeffs are themselves expressed as multivariate polynomials in the roots.

...could this theorem help to figure out what happens to the roots when we add two polynomials? The sum polynomial will be obtained by summing the coeffs - therefore, we sum these polynomials in the roots. Maybe try it with a sum of two quadratic polynomials for a simple example.


Resultante und Sylvester-Matrix: Die algebraischen Zahlen bilden einen Körper.
https://www.youtube.com/watch?v=dC6dxFhzKoc

https://www.youtube.com/watch?v=6MgN4B_PpUo&list=PLb0zKSynM2PCrgebQsfrzEsUIuA0I_wdG&index=35
"Klausurlemma". If a polynomial with integer coefficients has a rational root p/q, then p divides the constant coefficient and q divides the leading coefficient. It follows that if a monic polynomial has a rational root, then that root must be an integer. Or: Roots of monic polynomials are either integer or irrational. Or: If a polynomial is irreducible over Z, then it is also irreducible over Q. If f = g*h and all coeffs of f are divisible by some prime p, then all coeffs of g are divisible by p or all coeffs of h are divisible by p.


https://mathworld.wolfram.com/PolynomialDiscriminant.html
https://math.stackexchange.com/questions/2303465/why-discriminant-of-a-polynomial-is-so-special

 
-make a similar section on 
 -Differential Algebra 
 -Lie Algebras
 ...but maybe the algebra of polynomials should not be a section but rather a chapter in its own
 right - maybe after the Linear Algebra chapter. The topic might be too big for a section and
 Abstract Algebra might be too late to cover that material. 
 
Quintic equations:
https://www.youtube.com/watch?v=t4u_lwONAdc    I can solve any quintic equation!!  https://de.wikipedia.org/wiki/Bringsches_Radikal
https://en.wikipedia.org/wiki/Bring_radical
 
How to Get to Galois Theory Naturally
https://www.youtube.com/watch?v=lkvkUT3Qdw4 
-Every polynomial has a symmetry group
-The structure of this group determines, if the polynomial is solvable by radicals

For Galois theory we need groups, rings and fields 
Notation:

F              field of coefficients (ex: real numbers R)
F[x]           ring of polynomials over F
p(x)           element of F[x], polynomial in x with coeffs from F, ex: p(x) = x^2 + 1
r1,r2,...      roots of a polynomial p
F(c1,c2,..)    F with the roots of p adjoined: root-field/splitting field of p(x) over F
               ex: complex numbers C. (ABoAA pg 313)
F[x] / p(x)    Quotient ring of F[x] wrt p(x) ??? see below      
Gal(K:F)       Galois group of K over F where: F: field of coeffs, K: root-field of p(x), p(x):
               a given polynomial (suppressed in the notation - must be clear from context). It's 
               the group of all automorphisms of K which fix F. (ABoAA pg 325)

ex 1:
F = Q, p(x) = x^2 - 2, r1 = sqrt(2), r2 = -sqrt(2), K = Q(sqrt(2))
Gal(K:F) = ...

ex 2:
F = R, p(x) = x^2 + 1, r1 = i, r2 = -1, K = R(sqrt(-1)) = C
Gal(K:F) = identity + complex cojugation? 


Note to F[x] / p(x): Normally, we build quotient sets wrt to an equivalence relation - how is a
polynomial an quivalence relation? Maybe it defines one by saying: all polynomials that have the 
same roots as p are equivalent to p? Nah - that just filters out a subset of R[x]. I think, it's
about the remainders from polynomail division: all polynomials if F[x] that have the same remainder
after dividing out p(x) go into one equivalence class - or something


https://math.stackexchange.com/questions/395028/how-to-deal-with-polynomial-quotient-rings
https://sites.millersville.edu/bikenaga/abstract-algebra-1/quotient-rings-of-polynomial-rings/quotient-rings-of-polynomial-rings.html

\end{comment}