\section{Vector Spaces}
For a \emph{vector space}, we need two ingredients: \emph{scalars} and \emph{vectors}. The scalars are elements of a field $\mathbb{F}$, called the \emph{base field}, and the vectors can, for intuition building, be thought of as elements of $\mathbb{F}^n$ for some positive natural number $n$. It is possible to generalize this to things like $\mathbb{F}^\mathbb{F}$, the space of all functions from $\mathbb{F}$ to $\mathbb{F}$ but thinking about $\mathbb{F}^n$ is fine. To be even more concrete, you may imagine $\mathbb{R}^3$. But we are in the abstract algebra chapter here, so let's keep it general. Actually, even $\mathbb{F}^n$ is too concrete. We will instead just use $V$ to denote the set of vectors. ...TBC...


%===================================================================================================
\subsection{Subspaces} 
We have seen subgroups, subrings and subfields. It may not be surprising that we define a subspace of a given vector space in an entirely analoguous way. ...TBC...


%===================================================================================================
\subsection{Operations}


% Intersection
% Union - does not give a vector space, I think

\subsubsection{Direct Sum}

%https://en.wikipedia.org/wiki/Direct_sum


\subsubsection{Orthogonal Complement}

% https://en.wikipedia.org/wiki/Orthogonal_complement




\subsubsection{Quotient Space}
Let $V$ be a vector space and $U$ be a subspace of $V$. We partition the vector space $V$ into equivalence classes by saying that two vectors belong to the same equivalence class, when their difference is an element of a certain subspace $U$ of our original vectors space $V$. ...TBC...

% https://www.youtube.com/watch?v=d6ixmHDlrNo&list=PLdTL21qNWp2Z2iZOktAgucHyWMnJPA6eu&index=23&t=4s



\subsubsection{Implementation}
The operations on or between vector spaces, like taking an orthogonal complement or forming a direct sum, are very abstract ideas indeed. To get a more hands-on feeling for what is going on, I'll suggest a possible concrete (maybe naive) implementation of these operations. 

\paragraph{The Data Structure}
Firstly, we need a way to represent a vector space as a data structure. Every vector space has a basis, so it seems to make sense to represent a vector space by its basis. A basis is just a set of vectors. We will assume here that we represent a basis of a vector space as an $m \times n$ matrix $\mathbf{B}$ whose columns are considered to be our basis vectors. The matrix has $n$ columns, so we represent an $n$-dimensional space. The number $m$ - the number of rows of the matrix - can be any number $m \geq n$ but for the operations to be nontrivial, we'll want $m > n$. We may imagine $m$ to be the dimensionality of some embedding space. That is: we want to view our vector space of interest as a subspace of some possibly higher dimensional embedding space. This is because we want to implement operations like the orthogonal complement which is defined on a subspace with respect to a higher dimensional embedding space. 

%over the field $\mathbb{R}^n$ by an $n \times m$ matrix.

\paragraph{The Algorithms}
Given a vector space represented by its basis vectors stored in the rows of an $m \times n$ matrix $\mathbf{B}$, we now want compute a matrix $\mathbf{C}$ that represents a basis for the orthogonal complement of $\mathbf{B}$ with respect to the embedding space. That is, we want to compute a basis for $\mathbf{C} = \mathbf{B}^\perp$. The shape of $\mathbf{C}$ has to be $m \times k$ where $k = m - n$ because $\mathbf{B}$ represents a basis for an $n$-dimensional subspace of the $m$-dimensional embedding space, so the number of remaining dimensions is $m-n$. ...TBC...

% Give an algorithm that checks if some subspace V is a subspace of another subspace W. For example
% the embedding space may be U = R^3 (U for "universal space" or "universe"), W may be a plane 
% through the origina and V be a line thorugh the origin. It may or may not lie in the plane. The
% algorithm should figure that out.



%===================================================================================================
\subsection{Additional Structure from Products} 
There are vector spaces that have additional operations defined on the vectors. These algebras are classified based on the properties of these additional operations. Typically, there is at least one additional binary operation between vectors, i.e. an operation that takes two vectors as input and produces another vector as output. This operation is often interpreted as "product" between the vectors although it may be any operation - remember that we are doing abstract algebra here. It could be a commutator between matrices (that happens in Lie algebras), a concatenation of loops (that happens in algebraic topology), etc.

% Maybe make the subsection "Extra Structure" with subsubs
% Products, Gradation, Derivation

%an additional operation defined that serves the purpose of a product between vectors that yields another vector. Depending on the properties of that product operation, they get different names.

\subsubsection{Associative Algebras} 

\subsubsection{Alternative Algebras} 

\subsubsection{Lie Algebras} 
% https://en.wikipedia.org/wiki/Lie_algebra
% https://en.wikipedia.org/wiki/Lie_bracket_of_vector_fields

%\subsubsection{Vertex Algebras} 


\subsubsection{Poisson Algebras} 
% https://en.wikipedia.org/wiki/Poisson_algebra
% https://en.wikipedia.org/wiki/Product_rule
% https://en.wikipedia.org/wiki/Derivation_(differential_algebra)


\subsubsection{Differential Algebras} 
Differential algebras have a product and an additional univariate operator that we call a \emph{derivation}. That is: a derivation takes one vector as input and produces another one. Let's denote the operation as $D$ and use functional notation, i.e. $D(a)$ means: apply the operator $D$ to the vector $a$. The operator must satisfy the product rule:
\begin{equation}
 D(a \cdot b) = a \cdot D(b) + D(a) \cdot b
\end{equation}
Compare this to the product rule that you know from calculus and you will notice that it is indeed the same rule. The vector space of differentiable functions with the pointwise multiplication between the functions as product and the derivative as derivation does indeed form a differential algebra. ...TBC...what other interesting examples are there (subalgebras don't count)?

% Sequences of numbers with convolution as multiplication. the derivation is: left-shifting by one and dividing by the old index, i.e. b_k = (a_{k+1})/(k+1). That's how polynomial multiplication and taking derivatives work. But we do not need to interpret the sequence as sequnece of polynomial coeffs

% There is a univariate operation that satisfies the product rule
% https://en.wikipedia.org/wiki/Derivation_(differential_algebra)

% I think  computer algebra systems rely on such an "algebraization" of calculus to do their computations

% https://en.wikipedia.org/wiki/Gerstenhaber_algebra
% https://en.wikipedia.org/wiki/Superalgebra

% https://en.wikipedia.org/wiki/Graded_ring#Graded_algebra

% https://en.wikipedia.org/wiki/Graded_ring
% https://en.wikipedia.org/wiki/Graded_vector_space

\begin{comment}

-discuss operations like orthogonal complement, direct sum of vector spaces etc.
-maybe let Orthogonal complement and Direct sum be subsections, i.e. scrap the level "Operations"
-Implementation could also be a subsection
-Moduln: like a vector space but made from an underlying ring rather than a field

-Matrix Lie groups are (?):
 -Vector space under matrix addition (just like more general sets of matrices)
 -Group under matrix multiplication (that's the special property?)
 
-NxN-matrices together with scalars form a vector space under matrix-addition (as the
 vector-addition operation). Can we use other matrix operations to form such a vector
 space? Multiplication? Commutator? -> figure out!
 I think, for the distributive law to hold: a * (B * C) = a*B * a*C for scalar a and matrices B,C, 
 we must have a^2 = a so the set of scalars can contain only elements that square to themselves? In
 Z_6, we have 0,1,3 as such elements - but Z_6 is not a field...hmm...but perhaps Z_2 could work.
	
A gentle description of a vertex algebra.	
https://www.youtube.com/watch?v=7j4YVIFmAXw	
-Associative algebra: 
 -A vector space with a (bilinear, associative) product
 -Examples: 
  -NxN matrices (with matrix multiplication), 
  -polynomials (with polynomial multiplication)
  -multivariate polynomials with infinitely many variables
 -It's kinda like a ring with some extra structure.
-Lie algebra:
 -The product is usually written as a "Lie bracket": [x,y]
 -The product not associative but it follows the Jacobi identity 
 -The product is anticommutative aka antisymmetric (verify!)
 -Examples:
  -Take any associative algebra and define [x,y] = xy - yx, i.e. use the commutator
...ToDo: continue watching at 9:55 - there's more stuff



alternative algebra -- featuring the octonions!
https://www.youtube.com/watch?v=ZC7YofZp-cw
-An alternative algebra also starts with a vector space with a product
-The product satisfies (aa)b = a(ab)  and  (ab)b = a(bb). Note that these rules would follow
 immediately from associativity (ab)c = a(bc), if we would have associativity (which we don't). So, the "alternative" rule is a weaker rule than associativity.
-The "associator" is an operation defined as [a,b,c] = a(bc) - (ab)c. Thisn operation is an alternating multilinear form if we have an alternting(alternative?) algebra
...ToDo: continue watching at 5:00


The Multiplication Multiverse | Infinite Series
https://www.youtube.com/watch?v=H4I2C3Ts7_w
-Lie-Algebra: Product is anti-commutative and satisfies the Jacobi identity
-Other possible identities: Poisson identity, Super-commutativity
 ...Poisson Algebra, Gerstenhabe algebra
-Topology gives a way to "multiply" loops - the multiplication is just concatenation

Associahedra: The Shapes of Multiplication | Infinite Series
https://www.youtube.com/watch?v=N7wNWQ4aTLQ&list=PLa6IE8XPP_glwNKmFfl2tEL0b7E9D0WRr&index=19

	
\end{comment}