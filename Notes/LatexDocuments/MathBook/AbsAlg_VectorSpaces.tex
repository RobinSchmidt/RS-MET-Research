\section{Vector Spaces}
For a \emph{vector space}, we need two ingredients: \emph{scalars} and \emph{vectors}. The scalars are elements of a field $\mathbb{F}$, called the \emph{base field}, and the vectors can, for intuition building, be thought of as elements of $\mathbb{F}^n$ for some positive natural number $n$. It is possible to generalize this to things like $\mathbb{F}^\mathbb{F}$, the space of all functions from $\mathbb{F}$ to $\mathbb{F}$ but thinking about $\mathbb{F}^n$ is fine. To be even more concrete, you may imagine $\mathbb{R}^3$. But we are in the abstract algebra chapter here, so let's keep it general. Actually, even $\mathbb{F}^n$ is too concrete. We will instead just use $V$ to denote the set of vectors. ...TBC...


%===================================================================================================
\subsection{Subspaces} 
We have seen subgroups, subrings and subfields. It may not be surprising that we define a subspace of a given vector space in an entirely analogous way. A subset $U$ of a given vector space $V$ is called a \emph{subspace} of $V$ when it is itself a vector space. ...TBC...

% https://en.wikipedia.org/wiki/Linear_subspace

% https://math.stackexchange.com/questions/594887/notation-for-subspaces

\subsubsection{Lattices}

% A lattice is a set of discrete points within a vector space that is itself a vector space.

% Counting points on the E8 lattice with modular forms (theta functions) | #SoME2
% https://www.youtube.com/watch?v=AxPwhJTHxSg	

%===================================================================================================
\subsection{Operations}


% Intersection
% Union - does not give a vector space, I think

\subsubsection{Direct Sum}

%https://en.wikipedia.org/wiki/Direct_sum


\subsubsection{Orthogonal Complement}

% https://en.wikipedia.org/wiki/Orthogonal_complement




\subsubsection{Quotient Space}
Let $V$ be a vector space and $U$ be a subspace of $V$. We partition the vector space $V$ into equivalence classes by saying that two vectors belong to the same equivalence class, when their difference is an element of a certain subspace $U$ of our original vectors space $V$. ...TBC...

% https://www.youtube.com/watch?v=d6ixmHDlrNo&list=PLdTL21qNWp2Z2iZOktAgucHyWMnJPA6eu&index=23&t=4s



\subsubsection{Implementation}
The operations on or between vector spaces, like taking an orthogonal complement or forming a direct sum, are very abstract ideas indeed. To get a more hands-on feeling for what is going on, I'll suggest a possible concrete (maybe naive) implementation of these operations. 

\paragraph{The Data Structure}
Firstly, we need a way to represent a vector space as a data structure. Every vector space has a basis, so it seems to make sense to represent a vector space by its basis. A basis is just a set of vectors. We will assume here that we represent a basis of a vector space as an $m \times n$ matrix $\mathbf{B}$ whose columns are considered to be our basis vectors. The matrix has $n$ columns, so we represent an $n$-dimensional space. The number $m$ - the number of rows of the matrix - can be any number $m \geq n$ but for the operations to be nontrivial, we'll want $m > n$. We may imagine $m$ to be the dimensionality of some embedding space. That is: we want to view our vector space of interest as a subspace of some possibly higher dimensional embedding space. This is because we want to implement operations like the orthogonal complement which is defined on a subspace with respect to a higher dimensional embedding space. It's a bit like with the set complement. It only makes sense with respect to an (perhaps implicitly understood) universal set.

%over the field $\mathbb{R}^n$ by an $n \times m$ matrix.

\paragraph{The Algorithms}
Given a vector space represented by its basis vectors stored in the rows of an $m \times n$ matrix $\mathbf{B}$, we now want compute a matrix $\mathbf{C}$ that represents a basis for the orthogonal complement of $\mathbf{B}$ with respect to the embedding space. That is, we want to compute a basis for $\mathbf{C} = \mathbf{B}^\perp$. The shape of $\mathbf{C}$ has to be $m \times k$ where $k = m - n$ because $\mathbf{B}$ represents a basis for an $n$-dimensional subspace of the $m$-dimensional embedding space, so the number of remaining dimensions is $m-n$. ...TBC...

% Give an algorithm that checks if some subspace V is a subspace of another subspace W. For example
% the embedding space may be U = R^3 (U for "universal space" or "universe"), W may be a plane 
% through the origina and V be a line thorugh the origin. It may or may not lie in the plane. The
% algorithm should figure that out.

%---------------------------------------------------------------------------------------------------
\subsubsection{Homomorphisms}
Just like groups and rings, vector spaces do also have a notion of homomorphisms. Such \emph{vector space homomorphisms} are also known as \emph{linear transformations} or \emph{linear maps}. If we view the transformation as a function $f: V \rightarrow W$ from some vector space $V$ so some (possibly) other vector space $W$, then the function $f$ must satisfy:
\begin{equation}
  f(\mathbf{v_1} + \mathbf{v_2}) = f(\mathbf{v_1}) + f(\mathbf{v_2}), \qquad
  f(\alpha \mathbf{v_1}) = \alpha f(\mathbf{v_1})
\end{equation}
for all $\mathbf{v_1, v_2} \in V$ and all $\alpha \in \mathbb{F}$, i.e. the $\mathbf{v_1, v_2}$ are vectors from $V$ and $\alpha$ is any scalar from our field of scalars. These two conditions are called \emph{additivity} and \emph{homogeneity} and taken together, they constitute the important notion of \emph{linearity}. Note again that on the left hand side, we are dealing with the addition and scalar multiplication of $V$ whereas on the right hand side, we are dealing with the operations of $W$. A simple example would be: $f: \mathbb{R}^2 \rightarrow \mathbb{R}^3$ with $f((x,y)) = (2 x + y, 3 x - 2 y, x - y)$. A slightly more advanced example dealing with infinite-dimensional vector spaces would be the derivative of a function. We could denote this as an operator $T: C^1(\mathbb{R}) \rightarrow C^0(\mathbb{R}) $ with $T[f] = f'$.

\medskip
Further important features of linear maps that follow from this definition are: (1) The zero vector of $V$ is mapped to the zero vector of $W$: $f(\mathbf{0}_V) = \mathbf{0}_W$. (2) The image of $V$ under $f$ is a subspace of $W$. (3) The kernel of $f$, i.e. the set of all vectors $\mathbf{v} \in V$ that get mapped to the zero vector, is a subspace of $V$. (4) When $f$ is injective such that we can invert it, then the inverse map is also linear.

%Furthermore $f[V] \subseteq W$

% Lineare Abbildungen und die Kern-Bild-Formel
% https://www.youtube.com/watch?v=blroCEbv1X8  by Weitz

% linear maps mus map the zero vector of V to the zero vector of W

% Kern-Bild-Formel aka Rangsatz aka Dimensionssatz
% dim V = dim ran(f) + dim ker (f)
% https://en.wikipedia.org/wiki/Range_of_a_function
% https://en.wikipedia.org/wiki/Kernel_(algebra)


%===================================================================================================
\subsection{Additional Structure from Products} 
There are vector spaces that have additional operations defined on the vectors. These spaces are classified based on the properties of these additional operations. Typically, there is at least one additional binary operation between vectors, i.e. an operation that takes two vectors as input and produces another vector as output. This operation is often interpreted as "product" between the vectors although it may be any operation - remember that we are doing abstract algebra here. It could be a commutator between matrices (that happens in Lie algebras), a concatenation of loops (that happens in algebraic topology), etc. The product operation is typically bilinear. Vector spaces with such a bilinear product are called \emph{algebras over a field} or just \emph{algebras}\footnote{The term "algebra" is really overloaded a lot in mathematics. If you find that confusing then welcome to the club!}.

% These vector spaces with additional structure are called \emph{algebras over a field} or just \emph{algebras}\footnote{The term "algebra" is really overloaded a lot in mathematics. If you find that confusing then welcome to the club!} when the additional structure arises from a bilinear product operation.

% Maybe make the subsection "Extra Structure" with subsubs
% Products, Gradation, Derivation

%an additional operation defined that serves the purpose of a product between vectors that yields another vector. Depending on the properties of that product operation, they get different names.

% https://en.wikipedia.org/wiki/Algebra_over_a_field

\subsubsection{Associative Algebras} 

\subsubsection{Alternative Algebras} 

\subsubsection{Lie Algebras} 
% https://en.wikipedia.org/wiki/Lie_algebra
% https://en.wikipedia.org/wiki/Lie_bracket_of_vector_fields

%\subsubsection{Vertex Algebras} 


\subsubsection{Poisson Algebras} 
% https://en.wikipedia.org/wiki/Poisson_algebra
% https://en.wikipedia.org/wiki/Product_rule
% https://en.wikipedia.org/wiki/Derivation_(differential_algebra)


\subsubsection{Differential Algebras} 
Differential algebras have a product and an additional univariate operator that we call a \emph{derivation}. That is: a derivation takes one vector as input and produces another one. Let's denote the operation as $D$ and use functional notation, i.e. $D(a)$ means: apply the operator $D$ to the vector $a$. The operator must satisfy the product rule:
\begin{equation}
 D(a \cdot b) = a \cdot D(b) + D(a) \cdot b
\end{equation}
Compare this to the product rule that you know from calculus and you will notice that it is indeed the same rule. The vector space of differentiable functions with the pointwise multiplication between the functions as product and the derivative as derivation does indeed form a differential algebra. A subspace of that is the space of all polynomials. If we represent the polynomials as sequences of coefficients, we could use convolution as our product operation. The derivation operation would be to apply the rule how to differentiate polynomials: left-shifting the sequence $a_n$ by one and multiplying by the old index, i.e. informally $D(a_n) = n a_{n+1}$. If we now forget that the sequences originally were meant to represent polynomial coefficients, we have now a differential algebra on (finite) sequences of numbers without necessarily having to think about functions. It would straightforwardly generalize to infinite sequences, too - even if they don't converge as a power series, i.e. don't define a function [VERIFY!]. ...TBC...what other interesting examples are there (subalgebras don't count)? TODO: use boldface for $a,b$

% Sequences of numbers with convolution as multiplication. the derivation is: left-shifting by one and dividing by the old index, i.e. b_k = (a_{k+1})/(k+1). That's how polynomial multiplication and taking derivatives work. But we do not need to interpret the sequence as sequnece of polynomial coeffs

% My C++ code says that the commutator in geometric algebra obeys the product rule - see enum class ProductType in Tools.cpp, line 5661

% There is a univariate operation that satisfies the product rule
% https://en.wikipedia.org/wiki/Derivation_(differential_algebra)

% What about the arithmetic derivative?

% I think  computer algebra systems rely on such an "algebraization" of calculus to do their computations

% https://en.wikipedia.org/wiki/Gerstenhaber_algebra
% https://en.wikipedia.org/wiki/Superalgebra


\subsubsection{Graded Algebras} [VERIFY!]
In a graded algebra, we are dealing with objects of different kinds that cannot be mixed up with one another in addition but can be in multiplication. As an example, consider the algebra of tensors over a vector space $V^n$. The product is, of course, the tensor product. We cannot add tensors of different ranks $m$ and $n$ but we can always multiply such tensors to get a new tensor of rank $m+n$. The rank of the tensor may also be called the \emph{grade}. Another example is the exterior algebra with the exterior product. We have scalars, vectors, bivectors, trivectors, etc. We cannot add a vector to a bivector - but we can multiply them to get a trivector ...TBC...


%For example, it doesn't make sense to add a scalar to a vector but multiplication between a scalar and a vector does. 

%In a graded algebr

% Formally, the space we are dealignwith is expressed a a direct sum of the vector spaces for each grade

% https://en.wikipedia.org/wiki/Graded_ring#Graded_algebra

% https://en.wikipedia.org/wiki/Graded_ring
% https://en.wikipedia.org/wiki/Graded_vector_space

\begin{comment}

-discuss operations like orthogonal complement, direct sum of vector spaces etc.
-maybe let Orthogonal complement and Direct sum be subsections, i.e. scrap the level "Operations"
-Implementation could also be a subsection
-Moduln: like a vector space but made from an underlying ring rather than a field

-Matrix Lie groups are (?):
 -Vector space under matrix addition (just like more general sets of matrices)
 -Group under matrix multiplication (that's the special property?)
 
-NxN-matrices together with scalars form a vector space under matrix-addition (as the
 vector-addition operation). Can we use other matrix operations to form such a vector
 space? Multiplication? Commutator? -> figure out!
 I think, for the distributive law to hold: a * (B * C) = a*B * a*C for scalar a and matrices B,C, 
 we must have a^2 = a so the set of scalars can contain only elements that square to themselves? In
 Z_6, we have 0,1,3 as such elements - but Z_6 is not a field...hmm...but perhaps Z_2 could work.
	
A gentle description of a vertex algebra.	
https://www.youtube.com/watch?v=7j4YVIFmAXw	
-Associative algebra: 
 -A vector space with a (bilinear, associative) product
 -Examples: 
  -NxN matrices (with matrix multiplication), 
  -polynomials (with polynomial multiplication)
  -multivariate polynomials with infinitely many variables
 -It's kinda like a ring with some extra structure.
-Lie algebra:
 -The product is usually written as a "Lie bracket": [x,y]
 -The product not associative but it follows the Jacobi identity 
 -The product is anticommutative aka antisymmetric (verify!)
 -Examples:
  -Take any associative algebra and define [x,y] = xy - yx, i.e. use the commutator
...ToDo: continue watching at 9:55 - there's more stuff



alternative algebra -- featuring the octonions!
https://www.youtube.com/watch?v=ZC7YofZp-cw
-An alternative algebra also starts with a vector space with a product
-The product satisfies (aa)b = a(ab)  and  (ab)b = a(bb). Note that these rules would follow
 immediately from associativity (ab)c = a(bc), if we would have associativity (which we don't). So, the "alternative" rule is a weaker rule than associativity.
-The "associator" is an operation defined as [a,b,c] = a(bc) - (ab)c. Thisn operation is an alternating multilinear form if we have an alternting(alternative?) algebra
...ToDo: continue watching at 5:00


The Multiplication Multiverse | Infinite Series
https://www.youtube.com/watch?v=H4I2C3Ts7_w
-Lie-Algebra: Product is anti-commutative and satisfies the Jacobi identity
-Other possible identities: Poisson identity, Super-commutativity
 ...Poisson Algebra, Gerstenhabe algebra
-Topology gives a way to "multiply" loops - the multiplication is just concatenation

Associahedra: The Shapes of Multiplication | Infinite Series
https://www.youtube.com/watch?v=N7wNWQ4aTLQ&list=PLa6IE8XPP_glwNKmFfl2tEL0b7E9D0WRr&index=19

Was sind Vektorr√§ume?
https://www.youtube.com/watch?v=DhW-Qef7mhQ
-at around 20:00 - some rules that follow from the axioms

	
\end{comment}