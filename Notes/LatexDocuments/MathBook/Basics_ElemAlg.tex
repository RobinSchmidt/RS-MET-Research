\section{Elementary Algebra}
Now that we have numbers, we want to do something with them. One thing we can do is to perform elementary operations such as addition and multiplication on given numbers to generate new numbers. For example $3 \cdot 5 + 7 = 22$. Hey - cool - we have produced a $22$ from $3,5$ and $7$! Not yet impressed? Then consider this: assume that the number $5$ is unknown and we only know the relation $3 \cdot x + 7 = 22$. Can we recover the unknown $x$ from that equation? It turns out that the answer is yes, we can: by subtracting $7$ from both sides of the equation and then dividing both sides by $3$, we recover $x = 5$. Elementary algebra is the science of solving such equations for unknown variables where it is understood that the unknowns are supposed to be numbers. The setting is usually as follows: we have given one or more equations that contain one or more unknown variables, typically denoted by letters from the end of the alphabet such as $x,y,z$. The question is if there are any solutions to the given equations, i.e. any assignments for the variables that make the equations hold true - simultaneously, if there's more than one equation. If there are solutions, we may ask how many there are. Just one solution, i.e. a unique solution? Or some other number of solutions? Or maybe even infinitely many? And, of course, we may also want to know, what the solution is or what they are. How can we produce them in a systematic way? Elementary algebra is about answering questions like these. Surpassing the "elementary" qualifier, the higher levels of algebra investigate mathematical systems which are then called algebraic structures and it turns out that our familiar natural, integer, rational, real and complex numbers are just specific examples of such systems which are studied there in an abstract way. In this realm of \emph{abstract algebra}, our familiar number systems serve as prototypes which are subsequently generalized. But this stuff is for later chapters. 

% Field theory gives a systematic answer to the question of solving polynomial equations

%We need to learn to walk before we can run.

%It is this realm abstract algebra that will

%An equation may or may not have a solution within the number system that we are working in. 

%Sometimes, different sequences of operations will produces the same result. When different computations produce the same result

% Consider 3 * 5 + 7 = 22. Can we reverse the process? Let's assume, the 5 is not given and we only
% know the relation 3 * x + 7 = 22
%
%   4 * 5 + 2 = 22
% 
% We could also have 4*x + 2 = 3*x + 7 given  -> sub 3*x and 2 on both sides -> x=5

%That something is computation ...tbc...

% Complex conjugation is an automorphism on C that fixes R. That means it can be pulled out or 
% shoved in in all the operation: conj(z * w) = conj(z) * conj(w), etc. - it works also for +,-,/

%===================================================================================================

%\subsection{Expressions}

%\subsection{Equations} Notationally, an equation is something with an left hand side (LHS), a right hand side (RHS) and an equals sign $=$ in between. Conceptually, it is a logical statement that the LHS and the RHS are equal to each other. This logical statement can be either true or false. For example, the equation $2 + 3 = 5$ is true whereas the equation $2 + 3 = 4$ is false. Equations may contain variables such as $x,y,z, \ldots$. If they do, it may be the case that they are true for certain assignments for the variables and false for other assignments. In such cases, we usually want to find all those assignments that make the equation a true statement. We call this "finding the solutions of" or shorter "solving" the equation. ...TBC...


% You probably don't know what an equation is
% https://www.youtube.com/watch?v=WR-_Ugo_4dc


%===================================================================================================
\subsection{Arithmetic}
Arithmetic is about performing the elementary operations of addition, subtraction, multiplication and division and on a higher level also exponentiation, extraction of roots and taking logarithms. 


%I would argue that when taking roots is included, taking logarithms should be included as well for consistency (both operations are ways of inverting the exponentiation), but that's usually not listed among the arithmetic operations. Be that as it may, we will look at logarithms in this section, too. We will also learn sum and product notation, the factorial and binomial coefficients...tbc...

% Maybe mention also, how the operatioons succesion, addition, multiplication, and exponentiation
% build up on each other and how this could continue with tetration etc.
% https://www.youtube.com/watch?v=u1x_FJZX6Vw
%
% Explaing infinte sums, products and continued fractions
%
% In a section about mathematical constants, explain how formulas for the they can be generated in
% a systematic way using continued fractions and the conservative matrix field. this should be a
% an optional section with asterisk

% https://en.wikipedia.org/wiki/Arithmetic

%---------------------------------------------------------------------------------------------------
\subsubsection{Notation for Sums and Products}
\label{Sec:SumsAndProducts}
If we have $n$ numbers $a_1, a_2, \ldots, a_n$, there is a special notation to form their sum and their product. It uses the uppercase greek letters $\Sigma$ (sigma) and $\Pi$ (pi) which correspond to the latin letters S and P respectively which should remind us of taking \emph{s}ums and \emph{p}roducts. The notation looks like this:
\begin{equation}
 \sum_{k=1}^{n}  a_k = a_1   +   a_2   +   \ldots   +   a_n, \qquad
 \prod_{k=1}^{n} a_k = a_1 \cdot a_2 \cdot \ldots \cdot a_n 
\end{equation}
At the bottom, we introduce a dummy variable for the index (here $k$) and assign the starting value for the index. At the top, we assign the final value for $k$ - which, in this case, is another variable, namely $n$. If $k$ starts at $1$, $n$ is the total number of values that we add or multiply. You may also see things like:
\begin{equation}
 \qquad  \sum_{k} a_k,       \qquad
 \sum_{0 \leq k \leq n} a_k, \qquad
 \sum_{k \neq 5} a_k
\end{equation}
and so on. In the first case, the range of the index $k$ must be inferred from the context. This may be used when specifying index range is too complicated to integrate into the summation sign notation. Then, the surrounding text may clarify what values the index $k$ will take on. The second sum writes the conditions for the index $k$ all at the bottom. It's  uncommon to see this for simple and common conditions like $0 \leq k \leq n$ but this notation may be used for more general conditions that the index needs to satisfy. In the 3rd case, the index $k$ would run over all possible values of $k$ except $5$ - again, which values are possible or make sense must be inferred from the context. You can also have double sums like:
\begin{equation}
 \sum_{i=1}^{m} \sum_{j=i}^n  a_i b_j
\end{equation}
which would mean to sum over all products $a_i b_j$ of values from two given sequences of numbers $a_1,\ldots,a_m$ and $b_1,\ldots,b_n$. Note that the lower limit of inner summation index $j$ starts at $i$ in this example. This is something you can do: use the current index of the outer sum for computing limits of the index of the inner sum. One could also use a more general condition like $j \neq i$ - in this case, the sum would leave out the terms where $j$ would be equal to $i$. In cases where the upper summation index is less than the lower index, e.g. something like $\sum_{k=8}^{3} a_k$, then there is nothing to sum over - the sum is considered to be an \emph{empty sum} and gets the value zero by definition. Likewise, an \emph{empty product} gets the value of one by definition. These definitions make sense because they assign the neutral elements of the respective operation to the empty case which tends to play nicely with edge cases. If, in a double sum (or general multiple sum), the different indices have the same range, then one can also use a single summation sign to save space. For example:
\begin{equation}
 \sum_{i=1}^{n} \sum_{j=1}^n  a_i b_j = \sum_{i,j=1}^{n} a_i b_j
\end{equation}
It is also possible to write something like:
\begin{equation}
 \sum_{i,j \geq 0, i+j \leq n} a_i b_j
\end{equation}
which means that we have a double sum (i.e. one with two indices $i,j$) and they both start at $0$ and their ranges are such that their sum is at most $n$. That means $i,j$ range over all combinations whose sum is less than or equal to $n$. You can also use set notation for the indices as in:
\begin{equation}
 \sum_{p \in \mathbb{P}, p \leq n} p
\end{equation}
That would mean to sum over all primes less than or equal to $n$. If the index range is too complicated to write down below (and possibly above) the summation sign, authors may just write the index variable $k$ under the sum and explain the range in the surrounding text or in separate formulas. You may also see things like $k \in S$ where $S$ is some set of integers that is further specified elsewhere in the text. But sometimes authors just don't care and write complicated expressions into the summation footer anyway. That tends to look ugly and scary. To see an example, look up the wikipedia page for the multinomial theorem. But don't let it scare you off. It's just a (multiple) sum, so you know, what you have to do in order to compute it. You just need to first decipher the algorithm for computing the ranges of the indices from the expression in the summation footer. 

...TBC...TODO: give formulas for sums (splitting, combining, index-shifting, ...), give the scary multinomial coeffs formual from wikipedia here as example

% Maybe include continued fractions, too

% Endliche Summen und Produkte / gau√üsche und geometrische Summenformel
% https://www.youtube.com/watch?v=O_MI8V8MEkU
% Ein paar einfache Regeln f√ºr Summen - Indexverschiebung, Aufsplitten, ...

\paragraph{$\star$ Iterated Binary Operations}
The sum and product notation are the most common examples for an idea in math known as \emph{iterated binary operations}. The general idea is to take a binary operation and turn it into an operation that can take any number of arguments. Other examples include the set theoretical intersections and unions of arbitrary many sets. ...TBC...ToDo: explain requirements on the binary operation (I think, it needs to be associative and commutative), give more examples, give the notations, explain relations to fold aka reduce in functional programming

% If it's associative, it doesn't make a difference if we left-fold or right-fold (aka reduce). If it's also commutative, we can reorder the arguments in any way we like. If the operation has a neutral element, we can define the empty iterated operation to be that element (an iteration over just one element is always that element itself). We may even be abel to define countably infinite numbers of iterations if a suitable idea of convergence is available.

% https://en.wikipedia.org/wiki/Fold_(higher-order_function)

% Sums and products are examples of iterated binary operations:
% https://en.wikipedia.org/wiki/Iterated_binary_operation
% - Other examples are: set theoretic union and intersection, logical conjunction and disjunction,
%   tensor products, direct sums of vector spaces, etc.
% - I think, the binary operator needs to be associative and maybe also commutative to arrive at
%   a unique definition. Otherwise, one may have to introduce conventions about the evaluation
%   order.
% - In this discussion, some profesor seems to be using a curved arrow to indicate a chaining
%   order:  https://math.stackexchange.com/questions/1391874/matrix-product-notation

%---------------------------------------------------------------------------------------------------
\subsubsection{Factorials and Binomial Coefficients}
\label{Sec:FactorialsAndBinomCoeffs}
Another thing that is used often enough to deserve its own notation is the so called \emph{factorial} of a natural number $n$. It's defined as the product of all positive natural numbers up to $n$ and denoted with an exclamation mark:
\begin{equation}
 n! = 1 \cdot 2 \cdot 3 \cdot 4 \cdot \ldots \cdot n = \prod_{k=1}^{n} k
\end{equation}
The factorial of $0$ is defined to be $1$. This is in line with the definition of the empty product: $\prod_{k=1}^{0} k = 1$. With the factorial, we can define the so called \emph{binomial coefficients} as follows:
\begin{equation}
\label{Eq:BinomialCoeffs}
 \binom{n}{k} = \frac{n!}{k! (n-k)!}
\end{equation}
They get their name from the fact that they give the coefficients that arise from expanding a so called \emph{binomial} $(a+b)^n$. Expanding it yields:
\begin{equation}
\label{Eq:BinomialTheorem}
(a+b)^n = \sum_{k=0}^{n} \binom{n}{k} a^k b^{n-k}
\end{equation}
They also arise in combinatorics. There, they give the number of different ways that one may pick $k$ objects from a set of $n$ objects. That's why they are pronounced as "n-choose-k". The formula above is important enough to have a name: It's the \emph{binomial theorem}. One important formula for binomial coefficients is that when we take the sum over all $k$ for a given $n$, the result is always $2^n$:
\begin{equation}
\label{Eq:BinomialCoeffsSum}
\sum_{k=0}^n \binom{n}{k} = 2^n
\end{equation}
ToDo: Give more formulas. Especially important are the various recursion formulas, I think.

% See:
% https://en.wikipedia.org/wiki/Binomial_coefficient#Identities_involving_binomial_coefficients

\paragraph{$\star$ Multinomial Coefficients}
A generalization of the binomial coefficients are the \emph{multinomial coefficients} defined as:
\begin{equation}
\label{Eq:MultinomialCoeffs}
 \binom{n}{k_1,k_2,\ldots,k_m} 
= \frac{n!}{k_1! k_2! \ldots k_m!}
= \frac{n!}{ \prod_{i=1}^{m} k_i ! }
\end{equation}
They arise from multiplying out a multinomial:
\begin{equation}
\label{Eq:MultinomialTheorem}
(a_1 + a_2 + \ldots + a_m)^n 
= \sum_{k_1+\ldots+k_m = n} \binom{n}{k_1, \ldots, k_m} a_1^{k_1} a_2^{k_2} \ldots a_m^{k_m}
\end{equation}
This sum is to be interpreted as follows. The indices $k_i$ run over all possible combinations that give a total sum of $n$. It is understood that the $k_i$ cannot become negative, i.e. $k_i \geq 0 \; \forall i$. The formula is unsurprisingly called the \emph{multinomial theorem}. To really show off our new notational skills, we could also write this as:
\begin{equation}
\Bigl( \sum_{i=1}^{m} a_i \Bigr)^n
%= \sum_{k_1+\ldots+k_m = n} n! \frac{\prod_{i=1}^{m} a_i^{k_i}} {\prod_{i=1}^{m} k_i!}
= \sum_{\sum k_i = n} \Bigl( n! \frac{\prod_{i=1}^{m} a_i^{k_i}} {\prod_{i=1}^{m} k_i!} \Bigr)
\end{equation}
although that may look a bit obscure and it is rarely written down like this. There is a generalization for formula (\ref{Eq:BinomialCoeffsSum}). First, let's rewrite it as:
\begin{equation}
\sum_{k=0}^n \binom{n}{k} 
= \sum_{k_1 + k_2 = n} \frac{n!}{k_1! k_2!}
= 2^n
\end{equation}
In this form, it can be generalized to the multinomial case as follows:
\begin{equation}
\label{Eq:MultinomialCoeffsSum}
  \sum_{k_1+\ldots+k_m = n} \binom{n}{k_1,k_2,\ldots,k_m} 
= \sum_{k_1+\ldots+k_m = n} \frac{n!}{k_1! k_2! \ldots k_m!}
= m^n
\end{equation}
This can easily be derived by taking equation (\ref{Eq:MultinomialTheorem}) and considering the special case for $a_1 = a_2 = \ldots = a_m = 1$. The LHS then just becomes $m^n$ and the product over the $a_i^{k_i}$ in the RHS becomes $1$.

% https://en.wikipedia.org/wiki/Multinomial_theorem#Sum_of_all_multinomial_coefficients


% There's also a formula that expresses the multinomial coeffs in terms of binomial coeffs.
% I think, it may be implemented in the codebase...not sure, though.

%...TBC...VERIFY...TODO: explain Pascal's triangle 
% Maybe use substack for the k_i >= 0 condition. Maybe use product notation in the rhs and sum notation in the lhs

% Pascal's triangle is actually already prsented in the combinatorics chapter - maybe move that to here and then there, just out a reference. Or maybe include a forward reference here.

% How does Pascal's triangle generalize to the multinomial case? Will we get a (hyper)tetraeder?

% What Lies Above Pascal's Triangle?
% https://www.youtube.com/watch?v=q2daqMR3l24

% https://en.wikipedia.org/wiki/Multinomial_theorem
% https://de.wikipedia.org/wiki/Multinomialtheorem
% Arens, pg 891

% Pascal's Triangle But The World Isn't Flat #SoME3
% https://www.youtube.com/watch?v=ETA-4P9hA88
% -has also trinomial expansion - Pascal's pyramid
% -trinomial distribution

% How to Extend the Sum of Any* Function
% https://www.youtube.com/watch?v=hkn9zeRuzHs
% -At 33 min, it has a nice explanation for generalizing binomial coeffs n-choose-k for non-integer
%  n. The formula  n-choose-k = n * (n-1) * (n-2) * ... * (n-(k-1)) / k!  still applies

% How to Take the Factorial of Any Number
% https://www.youtube.com/watch?v=v_HeaeUUOnc

% A Fun Fact About the Sum of n¬∑n!
% https://www.youtube.com/watch?v=_9I3wTtyfQs
% sum_{k=1}^n k k! = (n+1)! - 1

%---------------------------------------------------------------------------------------------------
\subsubsection{$\star$ Multi-Index Notation}
Certain formulas in mathematics require juggling with multiple indices an there are some operations on these sets of indices that are common enough to justify the introduction of a condensed notation for them. The formula for the multinomial coefficients is an example for that. ...TBC...



% https://en.wikipedia.org/wiki/Multi-index_notation



%---------------------------------------------------------------------------------------------------
\subsubsection{Some Important Formulas}

\paragraph{Power Sums}
Consider the task of summing powers of all natural numbers up to some given upper limit $n$. That means, we want to compute the sum $S = \sum_{k=1}^{n} k^p$. Doing it the naive way would require us to perform $n-1$ additions. That's a lot of work when $n$ is large. It turns out that there are formulas for that. For the first powers up to 3, they are:
\begin{equation}
\sum_{k=1}^{n} k^0 = n, \quad
\sum_{k=1}^{n} k^1 = \frac{n (n+1)}{2}, \quad
\sum_{k=1}^{n} k^2 = \frac{n (n+1) (2 n  + 1) }{6}, \quad
\sum_{k=1}^{n} k^3 = \frac{n^2 (n+1)^2}{4}
\end{equation}
The formula for $k^0 = 1$ is trivial but given for the sake of completeness. The formula for $k^1 = k$ of these formulas is also known as Gauss's summation formula\footnote{As the story goes, Gauss discovered the formula as child in school when his math teacher wanted to keep the children busy by letting them add up all numbers from $0$ to $100$. To his teachers surprise, Gauss gave the correct answer of 5050 very quickly because he discovered the formula during this task.}. The numbers resulting from the formula for $k^2$ are known as the square pyramidal numbers. They give the number of balls that make up a pyramid with a square base where the side length of the square is $n$. The tip of the pyramid is made from $1$ ball, the layer directly below is made from $2^2 = 4$ balls, etc. If you compare the formula for $k^1$ and $k^3$, you'll notice that you get the latter from the former by just squaring everything. That means that the sum over all the cubes up to $n$ equals the square of sum of all the numbers up to $n$.

...TBC...TODO: Give general formula (Faulhaber's Formula), explain how these formulas can be used to compute sums that don't start at 1 and/or have an increment other than 1 (general arithmetic progressions), give other sum formulas - for a geometric progression

% https://en.wikipedia.org/wiki/Faulhaber%27s_formula
% https://de.wikipedia.org/wiki/Faulhabersche_Formel
% https://mathworld.wolfram.com/FaulhabersFormula.html
% https://mathworld.wolfram.com/PowerSum.html



% https://de.wikipedia.org/wiki/Gau%C3%9Fsche_Summenformel
% https://didaktik.mathematik.hu-berlin.de/user/Euler.pdf  
% https://www.wolframalpha.com/input?i=sum+k%3D1+to+n+k%5E2

% Binomial theorem, Faulhaber's formula (small special cases)
% Multinomial coefficinet (see Arens, pg 891)

% https://en.wikipedia.org/wiki/Square_pyramidal_number
% https://de.wikipedia.org/wiki/Quadratische_Pyramidalzahl

% (x+1)(x-1) = x^2 - 1
% (x+y)(x-y) = x^2 - y^2
% generally:
% (x+y)(x-y) = x^2 - xy + yx - y^2 = x^2 - y^2 + [y,x] = x^2 - y^2 - [x,y] where [.,.] is
% the commutator

%\paragraph{Arithmetic Progressions}
%An arithmetic progression is a finite sum that starts at some given value $m$

% A New Year 2025 Math Fact
% https://www.youtube.com/watch?v=ZWLkIW4NsQ0
% \sum_{i=1}^n i^3 = (\sum_{i=1}^n i)^2

\paragraph{Geometric Progressions}
A geometric progression is a finite sum of powers of a given number $r$, typically starting at $r^0 = 1$. There's a formula for that, too:
\begin{equation}
 r^0 + r^1 + r^2 + \ldots + r^n = \sum_{k=0}^{n} r^k = \frac{1-r^{n+1}}{1 - r}
\end{equation}
The number $r$ is the ratio of two successive terms in the sum. You may also find it as a sum of the form $a r^0 + a r^1 + a r^2 + \ldots + a r^n$ but the overall scaling factor $a$ can be factored out anyway, so we don't really need that kind of generality in our formula.

% https://en.wikipedia.org/wiki/Arithmetic_progression
% https://en.wikipedia.org/wiki/Geometric_progression
% https://en.wikipedia.org/wiki/Geometric_series#Derivation_of_sum_formulas

% https://mathworld.wolfram.com/HarmonicNumber.html

%===================================================================================================
\subsection{Linear Equations}


%---------------------------------------------------------------------------------------------------
\subsubsection{Simultaneous Linear Equations}






%===================================================================================================
\subsection{Polynomial Equations}

%---------------------------------------------------------------------------------------------------
\subsubsection{The Quadratic Equation}

%---------------------------------------------------------------------------------------------------
\subsubsection{The Cubic Equation}
% special cases - "depressed cubic"

%---------------------------------------------------------------------------------------------------
\subsubsection{The Quartic Equation}

% Shows a managable formula for th 4th degree equation at 0:20:
% https://www.youtube.com/watch?v=OkaEPi_hYdk

% How to Solve ANY Cubic or Quartic Equation!
% https://www.youtube.com/watch?v=o8UNhs2OaG8
% -Very good (I think best) video on solving quadratics, cubics and quartics

% Why Is There No Quintic Formula?
% https://www.youtube.com/watch?v=mMMRghKLdYE
% -Video on why there is no quitic formula from the same channel


%---------------------------------------------------------------------------------------------------
\subsubsection{Equations of Higher Degree}

\paragraph{The Fundamental Theorem of Algebra}

% The Fundamental Theorem of Algebra
% https://www.youtube.com/watch?v=RBRVL6nP2Dk



\begin{comment}

ToDo:

-introduce positional number systems, in particular decimal and maybe binary as alternative
-give algorithms for long addition, subtraction, multiplication, division
-maybe also for numbers in scientific notation, i.e. floating point numbers
-introduce sum and product notation
 -maybe with a spoiler to infinite sums, use $\sum_{k=1}^{\infty} (1/10)^k = 0.1111... = 1/9$ as example
-introduce sums, products, factorials and binomial coefficients
-but maybe that stuff should go into the "Elementary Algebra" section because it involves 
 variables

References:

https://en.wikipedia.org/wiki/Arithmetic
https://www.britannica.com/science/arithmetic
https://en.wikipedia.org/wiki/Positional_notation
https://en.wikipedia.org/wiki/Mixed_radix


\end{comment}


%\subsection{Elementary Algebra}
%Elementary algebra is about solving an equation for an unknown variable, typically denoted as $x$. This is done by isolating $x$ on one side of the equation and moving all known quantities to the other side. On this other side, no unknown quantity should occur anymore such that it can be directly evaluated. ...tbc...
% example 2 x + 3 = 9 -> 2 x = 6 -> x = 3
% Really ...is it? Or is elementary algebra more generally "computation with letters"?
% ...yep...wikipedia says so


\begin{comment}

ToDo:
-make a (sub)section "Algebraic Reasoning" analogous to the "Geometric Reasoning" section in the Geometry chapter, so we can refer from there to here for analogies

References:

https://en.wikipedia.org/wiki/Elementary_algebra
https://en.wikipedia.org/wiki/Equation

Factoring a sum (or difference) of two cubic terms into a linear and quadratic factor: 
  a^3 + b^3 = (a + b) (a^2 - a b + b^2)
  a^3 - b^3 = (a - b) (a^2 + a b + b^2)
Can be put near the binomial formulas

I think, there's some formula to factor x^n - 1  into  (x-1) * ... ...or something similar. 
Not sure. could also be x^n + 1 = ...  Figure out! I think:

  (x^n - 1) = (x - 1) (1 + x + x^2 + x^3 + ... + x^{n-1})

It can be found by considering the equation x^n - 1 = 0 with root r = 1 and then factoring out
the linear factor (x-r) i.e. (x-1) by polynomial division. I think it can be generalized to

  (x^n - a) =  (x - r) (r^n + r^{n-1} x^1 + ... + r^2 x^{n-3} + r^1 x^{n-2} + r^0 x^{n-1})
  
where r = \sqrt[n]{a}...but this needs to be verified! I'm not sure. Example for the special case 
with a = 1, n = 4:

 (x - 1) (x^3 + x^2 + x + 1) = x^4 + x^3 + x^2 + x - x^3 - x^2 - x - 1
                             = x^4 - 1


From middle school to masters 10 essential algebraic formulas
https://www.youtube.com/watch?v=_l-wjeuPojY
(a+b)^2   = a^2 + 2ab + b^2
(a-b)^2   = a^2 - 2ab + b^2
a^2 + b^2 = (a+b)^2 - 2ab 
          = (a-b)^2 + 2ab
a^2 - b^2 = (a+b)(a-b)
(a+b)^3   = a^3 + b^3 + 3ab(a+b)
(a-b)^3   = a^3 - b^3 - 3ab(a-b)
a^3 + b^3 = (a+b)^3 - 3ab(a+b)
a^3 - b^3 = (a-b)^3 + 3ab(a-b) 
          = (a-b)(a^2 + ab + b^2)

https://www.youtube.com/watch?v=k2RRQtkjtCY  at 36:08:
(a+b)^2 + (a-b)^2 = 2(a^2+b^2)


https://www.youtube.com/watch?v=XfWgfZ5V2qI  A New Definition of the Derivative? #SoME3
t^2 - 1 = (t - 1) * (1 + t)
t^3 - 1 = (t - 1) * (1 + t + t^2)
t^4 - 1 = (t - 1) * (1 + t + t^2 + t^3)
       ...
t^n - 1 = (t - 1) * (1 + t + t^2 + t^3 + ... + t^(n-1))


Maybe this whole section should be called Elementary Algebra.
This video says (at 5:25):
https://www.youtube.com/watch?v=FQYOpD7tv30
(elementary) algebra is the study of abstraction applied to numbers. 

One could perhaps also say: algebra is computing with letters ("Rechnen mit Buchstaben"), 
deriving general formulas with placeholders into which we can plug in actual values (numbers).
It's also about algorithms to compute an unknown quantity from known quantities (think 
polynomial division, Gaussian elimination, partial fraction expansion). In this context, a 
formula can be seen as a very simple algorithm. A formula usually translates to a single 
assignment operation in a computer program. ..well, some formulas are a bit more complicated.
The quadratic formula spits out two values due to the +- sqrt..., so it would translate to
two assignments (or more when we use intermediate vars). But something like Pythagoras' 
theorem just translates to the assignment: c = sqrt(a*a + b*b) when we assume that c is the
unknown. A formula like lhs = rhs can often be directly translated into an assignment 
operation which, in the context of algorithms, can be seen as a simple mini "algorithm" made
from just a single assignment. If a is unknown, we would translate Pythagoras to 
a = sqrt(c*c - b*b), so one formula may potentially translate to multiple different of such
mini-algorithms. More complicated algorithms may contain branches and loops. In math notation
branches can be expressed in the form of piecewise defined right hand sides and loops in the
form of sequences. These sequences may actually be infinite in which case we step into
calculus territory. An example would be the Babylonian algorithm to compute a square-root. 
It's defined via an infinite sequence. In this specific case, the sequence is defined
recursively. In general sequences may also be defined in other ways as well (for example,
explicitly...i.e...by a formula for the elements).


What other types of "Computation" could we have? Maybe I should explain the basic algorithms 
for hand-calculations with pen and paper?


About squaring equations:

https://math.libretexts.org/Courses/Monroe_Community_College/MTH_165_College_Algebra_MTH_175_Precalculus/01%3A_Equations_and_Inequalities/1.04%3A_Radical_Equations

https://math.stackexchange.com/questions/568780/why-cant-you-square-both-sides-of-an-equation

It introduces extraneous solutions. Explain that. From x = y follows x^2 = y^2 but not the other
way around. For example, take x = 2, y = -2. x^2 = y^2 holds but x != y. Generally, squaring 
both sides may introduce an extra solution. I think, more generally, applying a function to both
sides, maybe introduce as many extra solutions as there are x-values that have a y-value as solution
that solves the new equation (that results form applying f). I think, using a bijective function is
unproblematic but using a non-injective one may introdcue extra solutions and using a 
non-surjective one may throw away solutions? Or vice versa? Figure this out!


All of ALGEBRA explained in just 10 minutes!
https://www.youtube.com/watch?v=5Bdr4oD2SXg


Caution:

Algebra is also about manipulating expressions where the goal is mostly to *simplify* them as much
as possible. However, in a derivation, it is not always advisable to simplify all expressions as 
much as possible as soon as possible. In situations where we want to derive an expression that 
depends on some general variable, say a natural number n, when we first try a couple of small 
values of n like n = 0,1,2,3 in order to try to see a pattern for general n emerge, it may be better
to not eagerly simplify everything as soon as possible. The reason is that a general pattern may be 
easier to spot in unsimplified expressions. As a simple exampe, consider the case where we are given
a sequence of rational numbers like: 

5/5, 8/7, 11/9, 14/11, 17/13, 20/15, 23/17, 26/19, 29/21, 32/23, 35/25, 38/27, 41/29, 44/31, ...

These numbers follow the general rule a_n = (3n+2)/(2n+3). This rule is much easier to spot in the
sequence in the given unsimplified form where unsimplified in this case means that the fractions
are unreduced. If we would reduce 5/5 to 1/1 and 20/15 to 4/3 and 35/25 to 7/5 and so on, the rule
may be harder to guess. The numbers can be produced by ths following Python snippet:

for n in range(1,30):
    print(3*n+2, 2*n+3)

The lesson to be learned is that a too eager simplification may obfuscate patterns that may be more
apparent in the more complicated form. The example involved just rational numbers but the same 
considerations apply to more complicated situations where we need to deal to expressions involving
variables. When working with computer algebra systems, it is tempting to always request a fully
simplified expression. Sometimes it may be better to resist that temptation. But it depends.


\end{comment}


