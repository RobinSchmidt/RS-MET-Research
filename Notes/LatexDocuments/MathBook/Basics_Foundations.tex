\section{Foundations} 

This book is about applied math, so we will not go deeply into the foundations of math. However, a brief and very superficial look into this topic is a good idea to set the stage for the material that follows. It also establishes the basics of the language which we will need to talk about mathematical concepts.

\subsubsection{Logic}
Math is the pursuit of finding truths, so it makes sense to have a framework, within which we can say that something is true or false. In mathematics, that framework is mathematical logic, more specifically propositional logic (sometimes called zeroth order logic) and predicate logic (a.k.a. first order logic). 

\paragraph{Propositional logic} deals with propositions which are statements that can be either true or false. You also have ways of combining given propositions $A,B$ to make new, more complex propositions. For example, you can combine two propositions with a logical "and" (usually denoted as $\wedge$). The resulting new proposition $A \wedge B$ is true, if and only if both of the input propositions $A,B$ are true, otherwise it's false. You also have a logical "or" (denoted as $\vee$), which in this context is taken to be an inclusive or: the combined proposition $A \vee B$ is true, if any one of the input propositions or both are true. You also have a logical "not" (denoted as $\neg$) which takes a single proposition as input and the result $\neg A$ is true, if the the input $A$ is false and vice versa. These operators that take in one or two propositions to produce a new proposition are called logical \emph{connectives}. In this context, the basic propositions like $A,B$ are sometimes called \emph{atomic}. Logic also provides the tools that are required to figure out whether a given proposition is true, given that some other propositions are true. That process of drawing conclusions from given (true) propositions is called deduction. In this process of deduction, yet another logical connective, called the \emph{implication} and denoted by $\Rightarrow$, plays a central role. A proposition like $A \Rightarrow B$ means: "$A$ implies $B$" which you may also interpret and read as "$B$ follows from $A$" or as "if $A$, then $B$". The combined proposition $A \Rightarrow B$ evaluates to "true" if, whenever $A$ is true, then $B$ is also true. It makes no statement about situations when $A$ is false, i.e. when $A$ is false, it doesn't matter, if $B$ is true or false - $A \Rightarrow B$ still evaluates to true. The only way that $A \Rightarrow B$ can evaluate to false is when $A$ is true and $B$ is false. Logical equivalence of two propositions $A,B$ is also sometimes important and denoted as $A \Leftrightarrow B$ and it can be decomposed into two implications that are "and"ed together: $(A \Rightarrow B) \wedge (B \Rightarrow A)$, i.e. into a mutual implication. The parentheses are actually optional due to suitably defined operator precedence rules: $\Rightarrow$ precedes $\wedge$. You can read the statement $A \Leftrightarrow B$ as "$B$ if and only if $A$", "$A$ if and only if $B$" or "$A$ is logically equivalent to $B$". To build up mathematics, propositional logic is not quite expressive enough, so...
% https://en.wikipedia.org/wiki/Propositional_calculus
%https://en.wikipedia.org/wiki/Logical_connective
% explain redundancy of the set of connectors - pick the topic up at where we see that the equivalence can be expressed as "and"ing two implications
% explain logical implications and the equivalent ways to express them: (A -> B)  <->  (~B -> ~A)  <->  (~A or B)  <->  (~(~B and A))  ...this is used later in the section about category theory
% read it as: "A implies B", "B follows from A", "A is logically equivalent to B"

% explain logical equivalences (A <-> B)  <-> (A -> B and B -> A)
% read it as "B iff A", "A iff B" where "iff" is short for "if and only if"

%introduce truth tables


\paragraph{Predicate Logic} builds up on propositional logic and a bit of set theory to let you talk about objects and relations between them. There, you have so called quantors like the symbol for "there exists an object such that..." (denoted as $\exists$) or a symbol for "for all objects it is true that..." (denoted as $\forall$). There are yet other levels and kinds of logic, but these two are enough for the moment. ...TBC...
%introduce the $\exists ! x$ quantor meaning "there is exactly one x, such that"


\subsubsection{Axioms}
One has to start somewhere. That starting point is typically a set of "axioms" together with the rules of logic. An axiom is a proposition that is just assumed to be true without further justification. Axioms should state things that are "obviously true". An example are the Peano axioms, some of which are: zero is a natural number, each natural number has a successor, any number is equal to itself, etc. If you really want to build up the whole tower of mathematics axiomatically, you have to \emph{choose} a set of axioms and from there, using only the rules of logic, i.e. deduction, find new propositions that are also true.
% Mention Peano Axioms, Zermelo-Frenkel(-Choice) ZFC, etc.

\subsubsection{Theorems and Proofs}
If you want to prove a proposition, the tools that you have in hand are all the propositions that are already known to be true together with the rules of logic. A proposition is known to be true if it is either an axiom or it has been previously proven by the same technique. A proof for a proposition is a sequence of true propositions in which each one follows from known or previous ones by applying the rules of logic and the last of which is the one you actually wanted to prove in the first place. If a proposition has been proven to be true, it becomes a "theorem". The idea of a theorem is a fundamentally important concept in math - math is all about finding theorems. You may have observed a pattern by looking at a bunch of examples and you may conjecture that the pattern is generally true. What you then have to do is to find a proof for your conjecture. If you have succeeded in this highly creative endeavour (i.e. your proof is determined to be correct by the mathematical community), your conjecture is elevated to the venerable status of a theorem. And if the theorem is important enough and you were the first to prove it, you will typically achieve immortality by having your name attached to the theorem for the rest of eternity. Thousands of years later, still everbody knows the name of Pythagoras today - although, it wasn't actually him who proved "Pythagoras' Theorem" - sometimes the world is a little unfair, too :'-(. Along the way of finding a proof, you may generate a whole bunch of proven propositions, some of which are only instrumental to your final goal, some of which are spinoffs, etc. There are some other terms for such "lesser theorems" such as "lemma", "corollary", etc. A theorem is usually a result with a certain level of importance, generality and usefulness. You wouldn't call something like $3+5=8$ a theorem, for example - although it manifestly is a true proposition (and can actually be proven).

\subsubsection{Definitions}
OK - this is kind of awkward. We now have to \emph{define} what \emph{a definition} is. A definition is actually just an agreement about certain conventions to be used in the following material, in particular about what a given term or symbol is supposed to mean. Definitions often stand at the beginning of the development of a new subject. Definitions cannot be right or wrong. They can just be more or less useful. For a definition to be useful, it should clearly encapsulate a concept that is important in the development of all the things further down the line. The so defined term or symbol shall be used a lot in the material to be developed and will be referred to often. It makes sense to pick definitions in such a way that theorems can be stated succinctly. What the most useful definitions for a particual (new) mathematical subject are is often not clear from the get go but instead crystallizes out over time as experience with the new subject grows and when a bit of hindsight is available. As users of math, we may take definitions for granted because smart mathematicians have already figured (and fought) them out for us (and for themselves, of course). But we should keep in mind that they are fundamentally just conventions to make it possible (and ideally convenient and easy) to talk about a given subject. They are not fundamental truths. They just establish the language that we will use. That's why sometimes different authors use different definitions. Sometimes there is just no universal consenus (yet or ever) about which definitions are the most useful ones. Which ones are more or less useful may also differ from field to field. So, care has to be taken when reading mathematical material from different sources - the definitions in use may not always agree.

\subsubsection{Set Theory}
Set theory is often said to be the foundation of all mathematics - even much more fundamental than the natural numbers. 
%In fact, it is possible to "construct" the natural numbers from sets. We will not go down this road though, since this is not really relevant in applied math. 
The idea of a set was initially introduced by Georg Cantor in an intuitive way. His way of establishing set theory later turned out to have some flaws which is why it was later rebuilt more formally. The result of this rebuild is called "axiomatic set theory" and is very abstract and formal. Fortunately, Cantor's view, which is today sometimes called "naive set theory", is good enough for us. 

\paragraph{Sets}
In Cantor's definition "A set is a gathering together into a whole of definite, distinct objects of our perception or of our thought which are called elements of the set.". So, in essence, a set is just a bunch of things. A very general concept indeed. Sets are usually denoted in curly braces. For example, the set of the 3 letters a,b,c would be denoted as $\{a,b,c\}$. Two sets are considered equal, if and only if they contain the same elements. It does not matter in which order the elements are written down or if an element appears multiple times. So that means, for example, the sets $\{c,a,b\}$ or $\{a,c,a,a,b,c\}$ are in fact equal to the set $\{a,b,c\}$. By the way, the phrase "if and only if" appears sufficiently often in math texts that some authors use the abbreviation "iff" for that - yes, that's an "if" with a double-f. I may sometimes use that, too. Sets can be given names, for example, we may call our set above $S$ and we may write this as $S = \{a,b,c\}$. Element membership is denoted by an $\in$ symbol, so to express the fact that $b$ is an element of the set $S$, we would write $b \in S$. 

\medskip 
Sets can have other sets as elements and that nesting capability can be used recursively to build arbitrarily complex structures purely from sets. These structures also include the number systems that are used in math. For example, the number zero can be represented by the empty set: $0 = \{\}$, which is also denoted by $\emptyset$, the number one by the set that contains the empty set (i.e. zero): $1 = \{ 0 \} =  \{ \emptyset \}$, the number two by the set that contains zero and one: $2 = \{ 0, 1 \} = \{ \emptyset, \{ \emptyset \} \}$ and so on. Of course, that's super tedious and nobody actually thinks about numbers this way - but in principle, it can be done. Note that in this context $\emptyset$ and $\{ \emptyset \}$ are different things. The first is the empty set and the second is the set that contains the empty set. The nesting matters. One is an empty box, the other one is a box that contains something: namely, an empty box. If you really go down to the very lowest levels of math, then sets are actually the \emph{only} things that can occur as elements of (other) sets because sets are really the only thing that exists in this world. There is nothing else but sets. [VERIFY!]

\medskip 
In math, the sets we are dealing with are often sets of numbers and they may have many or even infinitely many elements. To denote very large or infinite sets compactly there are notations based on predicate logic. For example to denote the set of all numbers larger than 100 but less than 1000, we may write $\{x : x > 100 \wedge x < 1000\}$, but now we are getting ahead of ourselves. To understand that notation, we actually first have to understand what $>$ and $<$ means. I'm pretty sure, you already do know what they mean, but in the context of set theory, these symbols first need to be defined, too. To do so, we need some more tools...
% it's called "set builder notation", I think

\paragraph{Tuples}
Sometimes, we may want to model situations in which the order of elements actually does matter. Sets are per definition not suitable for this (at least not directly), so we need something else. That other thing is the tuple. A tuple is typically denoted by listing the elements in parentheses. The 3-tuple $(a,b,c)$ is not equal to $(c,a,b)$. Tuples with two elements are also called ordered pairs, 3-tuples triples, 4-tuples quadruples and 5-tuples quintuples.

\medskip 
Sidenote: If you really want to be puristic and build \emph{everything} from sets, you can use sets to model tuples, too: you could just pack the tuple members into another set with another object that serves as index, so $(a,b,c)$ would become $\{\{a,1\},\{b,2\},\{c,3\}\}$ and $(c,a,b)$ would become  $\{\{c,1\},\{a,2\},\{c,3\}\}$. These sets of sets would indeed be uniquely indentified purely by their elements regardless of order because the correct order could be reconstructed due to the 2nd element in the inner sets wich serve as tags. There are other ways to model tuples purely via sets, too.

% https://en.wikipedia.org/wiki/Ordered_pair#Defining_the_ordered_pair_using_set_theory

\paragraph{Relations}
A relation can formally be defined to be a set of tuples. Of particular importance are binary relations, i.e. sets of 2-tuples, aka ordered pairs. As an example, consider the two sets $A = \{1,2,5\}, B = \{0,2,4\}$. We can define a relation $R$ between the sets $A$ and $B$ as a set of ordered pairs where the first entry comes from $A$ and the second from $B$, for example: $R = \{(1,2),(1,4),(2,4)\}$. Such a relation can be visualized pictorially by drawing the two sets side by side and drawing an arrow between each pair of elements that is in the relation. This is shown for our relation $R$ in figure .... ...TBC...
% explain left/right totality, left/right uniqueness - see Bill Shillito's course on youtube
% use as example A = {1,2,3,4}, B = {0,2,4} and the < relation. R = {(1,2),(1,4),(2,4),(3,4)}
% not left-total due to (4,X) missing
% not right-total due to (X,0) missing
% not left-unique due to (1,4),(2,4),(3,4)
% not right-unique due to (1,2),(1,4)
% draw diagrams

\begin{figure}[h]
\centering
	
\begin{tikzpicture}
[mydot/.style={circle, fill=lightgray, inner sep=5pt}, >=latex, shorten >= 1pt, shorten <= 1pt]

% Left set:
\node[mydot,                    label={center:1}] (a1) {}; % 1
\node[mydot, below=0.3cm of a1, label={center:2}] (a2) {}; % 2
\node[mydot, below=0.3cm of a2, label={center:5}] (a3) {}; % 5
	
% Right set:	
\node[mydot, right=1.5cm of a1, label={center:0}] (b1) {}; % 0
\node[mydot, below=0.3cm of b1, label={center:2}] (b2) {}; % 2
\node[mydot, below=0.3cm of b2, label={center:4}] (b3) {}; % 4

% Arrows for the relations:
\path[->] (a1) edge (b2);                                  % 1 -> 2
\path[->] (a1) edge (b3);                                  % 1 -> 4
\path[->] (a2) edge (b3);                                  % 2 -> 4

% Ellipses around the sets:
\draw (0.0,-0.75) ellipse (0.5cm and 1.5cm);
\draw (2.0,-0.75) ellipse (0.5cm and 1.5cm);

\end{tikzpicture}
\end{figure}
There are a couple of important features that such a relation may or may not have. In a \emph{left-total} relation, every element in the left set has an outgoing arrow emanating from it. In a \emph{right-total} relation, every element in the right set has an incoming arrow. In a \emph{right-unique} relation, every element in the left set has at most one outgoing arrow. The naming convention reflects the idea that when you pick one element from the left set, the related  element of the right set, if any, is uniquely determined. We always know, where we have to go. Likewise, in a \emph{left-unique} relation, every element of the right set has at most one incoming arrow. For every element from the right set, the related element from the left set, if any, is uniquely dtermined. We always know, where we came from. As we can see, our example relation $R$ actually has none of these features.

% https://www.youtube.com/watch?v=Tk5_B7w5fiY&list=PLZzHxk_TPOStgPtqRZ6KzmkUQBQ8TSWVX&index=9

\paragraph{Functions} are a special kind of relation, namely those relations which are left-total and right-unique. You can think of functions as a definition of a map from a set $A$ to another set $B$. Each element of $A$ gets mapped to some unique element of $B$. You can throw any $a \in A$ at a function $f$ and it will unambiguously tell you, which $b \in B$ your given $a$ is mapped to. If a function is additionally left-unique, we can actually traverse the arrow back to figure out unambiguously, where we came from. We can undo the application of the functional mapping for those $b \in B$ which have an incoming arrow. Such left-unique functions are also called \emph{injective}. If, on the other hand, the function is right-total, i.e. every element of $B$ has an incoming arrow, we call the function \emph{surjective}. A function that is both injective and surjective is called \emph{bijective}. Such bijective functions can be inverted, i.e. undone, for any $b \in B$ and are therefore also called invertible.

\paragraph{Set Algebra} Just like we could combine logical propositions via the connectives $\wedge, \vee, \neg, \ldots$ to yield new propositions, there are operations that we can perform on sets to yield new sets. The \emph{intersection} of two sets $A, B$ is denoted as $A \cap B$ and defined to be the set with the elements that are present in $A$ and in $B$, i.e. the set of elements that $A$ and $B$ have in common. The visual resemblence of $\cap$ and $\wedge$ is, of course, no coincidence.

\begin{eqnarray}
 A \cap B      &=& \{x: \; x \in A \wedge x \in    B \} \qquad \text{intersection} \\
 A \cup B      &=& \{x: \; x \in A \vee   x \in    B \} \qquad \text{union} \\
 A \setminus B &=& \{x: \; x \in A \wedge x \notin B \} \qquad \text{difference} \\
\end{eqnarray}

...TBC...

% https://tikz.dev/tikz-transformations

% see:
% https://tex.stackexchange.com/questions/157450/producing-a-diagram-showing-relations-between-sets
% https://latex.org/forum/viewtopic.php?t=21987

% explains special kinds of relations - especially equivalence relations and partial and total orders


% set algebra
%   cartesian product, intersection, union, difference, subset, etc.

% what about relational algebra?

% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/

\subsubsection{Category Theory}
Category theory is an area of math that attempts to structure and organize mathematics itself. This section here is meant to only give a very superficial birds eye overview. Category theory has been called the "mathematics of mathematics" and - less charitably - as "abstract nonsense". Its relation to set theory has been compared to that of higher level programming languages to assembly language. Category theory describes a given field of mathematics in terms of a so called \emph{category}. Such a category consists of a class of \emph{objects} and relationships between those objects, called \emph{morphisms}. You can picture the objects as nodes (as in a multigraph, see [REF needed to Graph Theory section]) and the morphisms as directed edges, depicted as arrows. Morphisms are also sometimes called arrows. The category must also have a notion of composing morphisms. When there's an arrow from node $A$ to node $B$ and another arrow from node $B$ to node $C$ then this induces an arrow from node $A$ to node $C$. This sort of \emph{composition} of morphisms into other morphisms can be pictured as traversing a path in the multigraph. The composition of morphisms must be associative. As final ingredient for a category, we need for each node a special kind of morphism, called an \emph{identity}. An identity is a morphism/arrow that goes from a node $A$ to itself and is meant to convey an abstraction of the idea of an identity function - a sort of neutral element in the realm of morphisms.
%ToDo: composition (-> paths), identites (-> loopy edges)

% https://en.wikipedia.org/wiki/Multigraph

\paragraph{The category "Set"} is perhaps the most immediate and prototypical example of a category. In Set, the objects are sets and the morphisms are functions. Composition of morphisms is the usual compositon of functions and the identites are of course the respective identity functions for each set. Don't make the mistake of thinking about set elements as objects. The sets themselves are the objects and their internal structure is considered opaque. From a categorical point of view, we can't look into the sets. We can't talk about individual elements at all in categorical terms - the reason being that, in general, the objects can be of a completely different nature and don't need to have a concept of "elements". One object in Set would be $\mathbb{N}$, another one would be $\mathbb{R}$, another one would be $\{0,1\}$, etc. The category Set has a node for every possible set and for every possible ordered pair $(A,B)$ of nodes (i.e. sets), it has bunch of directed edges (arrows) where each such arrow stands for a possible function from set $A$ to set $B$. For example, there would be an arrow called "$\sqrt{\; \;}$" from $\mathbb{N}$ to $\mathbb{R}$. There would also be a "$\sqrt{\; \;}$" from $\mathbb{R}$ to $\mathbb{R}$ and one from $\mathbb{N}$ to $\mathbb{N}$. As morphisms, they would all be considered different entities even though the are supposed to stand for the same mathematical operation of taking the square root. Each morphism carries along with it its source (or domain) and target (or codomain). Functions can have certain properties the most important ones are injectivity, surjectivity and bijectivity. Within the framework of set theory, these properties are defined in a way that references set elements. For example, a function from $A$ to $B$ is defined to be bijective, iff every element $a \in A$ maps to a unique element $b \in B$ and vice versa.

\paragraph{Iso-, Mono- and Epimorphisms}
Abstracting the idea of bijective functions to a more general setting in which the objects are not necessarily sets and the morphisms are not necessarily functions leads to the categorical idea of an \emph{isomorphism} which is a particular kind of morphism with certain additional properties. The key here is that in the definition of what properties such an isomorphism must satisfy, we are not allowed to talk about "elements". The only things that we are allowed to talk about are categorical terms like objects, morphisms, compositions and identities. We must capture the idea of bijctivity in terms of these and only these ideas. It goes like this: A morphism $f: A \rightarrow B$ is an isomorphism, if there exists a morphism $g: B \rightarrow A$ such that $g \circ f = id_A$ and  $f \circ g = id_B$. In this context, $g$ is the inverse morphism of $f$ and itself an isomorphism as well. In terms of elements when we are in Set: we take an element $a \in A$, apply $f$ to map it to an element $b \in B$, then apply $g$ to map $b$ back to an element $a' \in A$, then $a' = a$, i.e. we're back to where we started so the whole roundtrip from set $A$ to set $B$ and back to $A$ reduces to an identity operation in $A$. Stated without mentioning elements and therefore generalizable: Applying $f$ first, then $g$ yields a composed morphism that equals the identity in $A$ and applying $g$ first, then $f$ yields the identity in $B$. Note how at no point are we talking about the internal structure of the objects like we did when referencing set elements in the definition of bijective functions. The categorical definition of an isomorphism is not even focusing on the objects at all but it's all about the morphisms. The objects are just the backdrop and our main attention goes to the morphisms. We are indeed looking at things from a higher level than in set theoretical descriptions where we looked \emph{inside} the details of the objects. This is typical of category theoretical definitions and theorems. Likewise, a monomorphism $f: A \rightarrow B$ is characterized by the property that for all morphisms $g,h: C \rightarrow A$, we have that $f \circ g = f \circ h$ implies $g = h$. Monomorphisms generalize the idea of an injective (i.e. left-unique) function. An epimorphism $f: A \rightarrow B$ is defined by the property that for all morphisms $g,h: B \rightarrow C$, we have that $g \circ f = h \circ f$ implies $g = h$. Epimorphisms generalize the idea of a surjective (i.e. right-total) function. [VERIFY] If you don't immediately see why these definitions indeed capture and generalize the ideas of injectivity and surjectivity (I certainly didn't), try reversing the implications. For injectivity/monomorphisms, rewrite $(f \circ g = f \circ h) \Rightarrow (g = h)$ as the equivalent statement $(g \neq h) \Rightarrow (f \circ g \neq f \circ h)$ and draw some picture where $g \neq h$. Then do something similar for surjectivity/epimorphisms. Then pat yourself on the back for having grasped a rather abstract and obscure categorical definition. [TODO: maybe add a figure that shows this].
%pg 482 in Ehrig et al


%ToDo: 
%-explain the categorical analogs of injective and surjective functions (mono- and epimorphisms)
% -explain why the definitions of mono- and epimorphisms work, i.e. capture the desired idea
% -reverse the implications: g != h  ->  g°f != h°f etc. and draw pictures where g != h and show how that implies the RHS
%-give other examples of categories: FinSet, Grp, Vect, deductive systems, Graph
%-explain subcategories in this context - many other examples actually are subcategories of Set
%-explain functors, natural transformations

% https://en.wikipedia.org/wiki/Category_theory
% https://plato.stanford.edu/entries/category-theory/#Exam

% https://www.youtube.com/watch?v=SmXB2K_5lcA  Category Theory for Programmers: Chapter 1 - Category

% https://texample.net/tikz/examples/tag/diagrams/
% https://texample.net/tikz/examples/labeled-chain/

\medskip 
Of course, there's much more to say about category theory but this very brief overview shall suffice for a book that attempts to focus on applied math.