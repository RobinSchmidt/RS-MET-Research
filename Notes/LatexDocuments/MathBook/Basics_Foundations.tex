

\subsection{Set Theory}
Set theory is often said to be the foundation of all mathematics - even much more fundamental than the natural numbers. 
%In fact, it is possible to "construct" the natural numbers from sets. We will not go down this road though, since this is not really relevant in applied math. 
The idea of a set was initially introduced by Georg Cantor in an intuitive way. His way of establishing set theory later turned out to have some flaws which is why it was later rebuilt more formally. The result of this rebuild is called "axiomatic set theory" and is very abstract and formal. Fortunately, Cantor's view, which is today sometimes called "naive set theory", is good enough for us. 

\subsubsection{Sets}
In Cantor's definition "A set is a gathering together into a whole of definite, distinct objects of our perception or of our thought which are called elements of the set.". So, in essence, a set is just a bunch of things. A very general concept indeed. Sets are usually denoted in curly braces. For example, the set of the 3 letters a,b,c would be denoted as $\{a,b,c\}$. Two sets are considered equal, if and only if they contain the same elements. It does not matter in which order the elements are written down or if an element appears multiple times. So that means, for example, the sets $\{c,a,b\}$ or $\{a,c,a,a,b,c\}$ are in fact equal to the set $\{a,b,c\}$. By the way, the phrase "if and only if" appears sufficiently often in math texts that some authors use the abbreviation "iff" for that - yes, that's an "if" with a double-f. I may sometimes use that, too. Sets can be given names, for example, we may call our set above $S$ and we may write this as $S = \{a,b,c\}$. Element membership is denoted by an $\in$ symbol, so to express the fact that $b$ is an element of the set $S$, we would write $b \in S$. If we want to express that a certain object, say $d$, is not an element of a set $S$, we write this as $d \notin S$.

%\medskip 
\paragraph{Sets of Sets}
Sets can have other sets as elements and that nesting capability can be used recursively to build arbitrarily complex structures purely from sets. These structures also include the number systems that are used in math. For example, the number zero can be represented by the empty set: $0 = \{\}$, which is also denoted by $\emptyset$, the number one by the set that contains the empty set (i.e. zero): $1 = \{ 0 \} =  \{ \emptyset \}$, the number two by the set that contains zero and one: $2 = \{ 0, 1 \} = \{ \emptyset, \{ \emptyset \} \}$ and so on. Of course, that's super tedious and nobody actually thinks about numbers this way - but in principle, it can be done. Note that in this context $\emptyset$ and $\{ \emptyset \}$ are different things. The first is the empty set and the second is the set that contains the empty set. The nesting matters. One is an empty box, the other one is a box that contains something: namely, an empty box. If you really go down to the very lowest levels of math, then sets are actually the \emph{only} things that can occur as elements of (other) sets because sets are really the only thing that exists in this world. There is nothing else but sets and all the rich and complex mathematical structures that exist can, in principle, be built recursively from these sets. 
%[VERIFY!]

%\medskip 
\paragraph{Set Builder Notation}
In math, the sets we are dealing with are often sets of numbers and they may have many or even infinitely many elements. To denote very large or infinite sets compactly there are notations based on predicate logic. For example to denote the set of all numbers larger than 100 but less than 1000, we may write $\{x : x > 100 \wedge x < 1000\}$. but now we are getting ahead of ourselves. To understand that notation, we actually first have to understand what $>$ and $<$ means. I'm pretty sure, you already do know what they mean, namely, \emph{less than} and \emph{greater than} but in the context of set theory, these symbols first need to be defined, too. To do so, we need to first define what an order relation is. We'll look these soon. Generally, this kind of notation to define a set is called set builder notation and specifies a set by listing some properties that elements of the set must satisfy. In this notation, we use a placeholder or variable, here $x$, which stands in for some generic element of the set, then use a colon (or, alternatively a vertical bar) and then list the properties that this element should satisfy. So, a set defined by set builder notation generally look like: $\{x : P(x)\}$ or  $\{x \; | \; P(x)\}$ where $P(x)$ is some property or predicate that can be expressed using the machinery of predicate logic.

% Soo - which of the two notations should we use in this book? The vertical bar looks more aesthetic but I have used the colon already in a few places and its also easier to typeset because we don't nee the spaces \; ..hmmmm

%https://en.wikipedia.org/wiki/Set-builder_notation

\subsubsection{Tuples}
Sometimes, we may want to model situations in which the order of entries actually does matter. Sets are per definition not suitable for this (at least not directly), so we need something else. That other thing is the tuple. A tuple is typically denoted by listing the entries in parentheses. The 3-tuple $(a,b,c)$ is not equal to $(c,a,b)$. Tuples with two elements are also called ordered pairs, 3-tuples triples, 4-tuples quadruples and 5-tuples quintuples. For brevity, in the following, I will just say "pair" when I mean "ordered pair", i.e. pairs are implicitly always assumed to be ordered. When we talk about a tuple, the entries are no longer called "elements" but instead \emph{coordinates} or \emph{components}. If you really want to be puristic and build \emph{everything} from sets, you need to use sets to model tuples, too. You could just pack the tuple members into another set with another object that serves as index, so $(a,b,c)$ would become $\{\{a,1\},\{b,2\},\{c,3\}\}$ and $(c,a,b)$ would become  $\{\{c,1\},\{a,2\},\{c,3\}\}$. These sets of sets would indeed be uniquely indentified purely by their elements regardless of order because the correct order could be reconstructed due to the second element in the inner sets which serve as tags. While this definition, proposed by Hausdorff, is intuitive, it has a problem: We would need to require that our set of indices is disjoint from all the sets that make up the tuple's coordinates. Otherwise, we could not distiguish between $(1,2)$ and $(2,1)$, for example. There are other ways to model tuples purely via sets that avoid this problem but are less intuitive. The currently accepted definition is due to Kuratowski and defines the ordered pair $(a,b) = \{ \{a\}, \{a,b\} \}$. From ordered pairs, bigger tuples can be defined recursively. For example, a triple can be defined as $(a,b,c) = ((a,b),c)$. [VERIFY!] I won't expand this any further because it gets messy really quick. Nobody really thinks about tuples this way anyway. For set-theorists who work at the very foundations of mathematics, the point of this is to convince themselves once and for all that it is possible to express tuples via sets and from then on, use the higher level tuple notation - just like scientists and engineers do. 

% = (\{\{a\}, \{a,b\} \}, \; c)= \{ \{\{a\}, \{a,b\} \}, \; \{ \{\{a\}, \{a,b\},c \} \}\}$.

% https://en.wikipedia.org/wiki/Ordered_pair#Defining_the_ordered_pair_using_set_theory

%https://en.wikipedia.org/wiki/Tuple

% What about sequences? are they yet another way to specify a collection of objects with order? I think, the difference is that sequences are, by default, infinitely long and if we want a finite sequence, we just extend it with zeros. A tuple has always a fixed number of elements.

\subsubsection{Multisets}
In a set, neither the order of the elements nor their multiplicity matters where by "multiplicity" we mean the number of times, an element occurs. For example: $\{ 1,2,2 \} = \{ 2,1,2 \} = \{ 1,2 \}$ when we assume the collections to be sets. In a tuple, both of these things matter: $(1,2,2) \neq (2,1,2) \neq (1,2)$. There is an intermediate situation where we may want to distinguish collections based on multiplicity but not based on the exact position of occurrence of an item. A structure suited for that purpose is the so called multiset. Like in a set, it is immaterial at which position we list an item - but it does matter whether an item occurs once or twice or whatever other number of times. For multisets, we would have: $\{ 1,2,2 \} = \{ 2,1,2 \} \neq \{ 1,2 \}$. Multisets are mostly denoted just like sets with curly braces and it must be inferred from the context that multisets are meant. I have also seen a notation using square-brackets to distinguish multisets from sets but that notation doesn't seem to be standard and not even widespread.

% maybe move this section before the tuples and edit the text accordingly - should not reference tuples - and the tuples-text may reference multisets

% https://en.wikipedia.org/wiki/Multiset
% https://brilliant.org/wiki/multiset/#_=_
% https://www.statisticshowto.com/multiset/
% https://mathworld.wolfram.com/Multiset.html

% What operations do we have on multisets? There is the multiset sum explained in ACRS, pg 29. we just add the multiplicities where it is understood that objects which aren't in a multiset have a multplicity of zero. This is kinda like the set union for multisets. What about intersection? Maybe taking the minimum of the respective multiplicities of the elements could make sense?

% Applicaitons: Prime factorization, roots of polynomials, multiple traversals of a curve in algebraic geometry, e.g. the polynomial euqation  (x^2 + y^2 - 1)^n = 0  contains the unit circle n times (although there is no notion of "traversal" - I think, this idea comes in only when parameterizing the curve)



\subsubsection{Relations}
A relation can formally be defined to be a set of tuples. Of particular importance are binary relations, i.e. sets of 2-tuples, aka pairs. As an example, consider the two sets $A = \{1,2,5\}, B = \{0,2,4\}$. We can define a relation $R$ between the sets $A$ and $B$ as a set of pairs where the first entry comes from $A$ and the second from $B$, for example: $R = \{(1,2),(1,4),(2,4)\}$. Such a relation can be visualized pictorially by drawing the two sets side by side and drawing an arrow between each pair of elements that is in the relation. This is shown for our relation $R$ in figure .... ...TBC...
% explain left/right totality, left/right uniqueness - see Bill Shillito's course on youtube
% use as example A = {1,2,3,4}, B = {0,2,4} and the < relation. R = {(1,2),(1,4),(2,4),(3,4)}
% not left-total due to (4,X) missing
% not right-total due to (X,0) missing
% not left-unique due to (1,4),(2,4),(3,4)
% not right-unique due to (1,2),(1,4)
% explain inverse relations
% reflexive/irreflexive, symmetric/asymmetric/antisymmetric
%   https://www.youtube.com/watch?v=GvNGf9Gki7o 
% transitive
%   https://www.youtube.com/watch?v=O19RpfoxQpA
%   make table like at the end of this video - but verify the definitions - the asymmetry definition
%%  looks fishy. Maybe antimsymmtery is also "wrongly" defined there?
% draw diagrams

% https://byjus.com/maths/relations-and-its-types/
% https://www.toppr.com/guides/maths/relations-and-functions/types-of-relations/

\begin{figure}[h]
\centering
	
\begin{tikzpicture}
[mydot/.style={circle, fill=lightgray, inner sep=5pt}, >=latex, shorten >= 1pt, shorten <= 1pt]

% Left set:
\node[mydot,                    label={center:1}] (a1) {}; % 1
\node[mydot, below=0.3cm of a1, label={center:2}] (a2) {}; % 2
\node[mydot, below=0.3cm of a2, label={center:5}] (a3) {}; % 5
	
% Right set:	
\node[mydot, right=1.5cm of a1, label={center:0}] (b1) {}; % 0
\node[mydot, below=0.3cm of b1, label={center:2}] (b2) {}; % 2
\node[mydot, below=0.3cm of b2, label={center:4}] (b3) {}; % 4

% Arrows for the relations:
\path[->] (a1) edge (b2);                                  % 1 -> 2
\path[->] (a1) edge (b3);                                  % 1 -> 4
\path[->] (a2) edge (b3);                                  % 2 -> 4

% Ellipses around the sets:
\draw (0.0,-0.75) ellipse (0.5cm and 1.5cm);
\draw (2.0,-0.75) ellipse (0.5cm and 1.5cm);

\end{tikzpicture}
\end{figure}
% https://tikz.dev/tikz-transformations
% see:
% https://tex.stackexchange.com/questions/157450/producing-a-diagram-showing-relations-between-sets
% https://latex.org/forum/viewtopic.php?t=21987
There are a couple of important features that such a relation may or may not have. In a \emph{left-total} relation, every element in the left set has an outgoing arrow emanating from it. In a \emph{right-total} relation, every element in the right set has an incoming arrow. In a \emph{right-unique} relation, every element in the left set has at most one outgoing arrow. The naming convention reflects the idea that when you pick one element from the left set, the related  element of the right set, if any, is uniquely determined. We always know, where we have to go. Likewise, in a \emph{left-unique} relation, every element of the right set has at most one incoming arrow. For every element from the right set, the related element from the left set, if any, is uniquely determined. We always know, where we came from. As we can see, our example relation $R$ actually has none of these features.
% It's actually the "<"  relation - maybe mention it in the section about orders...but that doesn't really fit well because in orders domain and codomain are the same

% https://www.youtube.com/watch?v=Tk5_B7w5fiY&list=PLZzHxk_TPOStgPtqRZ6KzmkUQBQ8TSWVX&index=9


% Was sind angeordnete Körper? (Ordnungstheorie)
% https://www.youtube.com/watch?v=RyhUIoif_B8


\paragraph{Functions} are a special kind of relations, namely those relations which are left-total and right-unique. You can think of functions as a definition of a map from a set $A$ to another set $B$. Each element of $A$ gets mapped to some unique element of $B$. You can throw any $a \in A$ at a function $f$ and it will unambiguously tell you, which $b \in B$ your given $a$ is mapped to. If a function is additionally left-unique, we can actually traverse the arrow back to figure out unambiguously, where we came from. We can undo the application of the functional mapping for those $b \in B$ which have an incoming arrow. Such left-unique functions are also called \emph{injective}. If, on the other hand, the function is right-total, i.e. every element of $B$ has an incoming arrow, we call the function \emph{surjective}. A function that is both injective and surjective is called \emph{bijective}. Such bijective functions can be inverted, i.e. undone, for any $b \in B$ and are therefore also called \emph{invertible}.

% explain domain, range, image, pre-image (of elements and whole sets)
% give the condctiosn for functions in math notation

\paragraph{Equivalences} are another special kind of relations. In equivalence relations, the left and right set are actually the same set and a couple of additional properties must be satisfied. The relation must be \emph{reflexive} meaning that every element $a$ must be in relation with itself. They must also be \emph{symmetric} meaning that if the tuple $(a,b)$ is in the relation, then the tuple $(b,a)$ must also be in it. Finally, they must be \emph{transitive} meaning that if the tuples $(a,b)$ and $(b,c)$ are in the relation, then the tuple $(a,c)$ must also be in it. The prototype of an equivalence relation is the usual equality denoted by $=$ but the concept of an equivalence is more general. It is meant to capture the idea that two objects are interchangable within a given context. They do not necessarily have to be the exact same object but certainly can be - this is ensured by the reflexivity. For some equivalence relations, we may actually use the $=$ symbol even though another relation may be meant. Sometimes alternatives like $\sim, \equiv, \simeq, \cong,$ etc. are used. The notation for various equivalence relations may vary form field to field and from author to author. The symmetry condition captures the idea that if $a$ is equivalent to $b$ then $b$ is also equivalent to $a$. Transitivity captures the idea that if $a$ is equivalent to $b$ and $b$ is equivalent to $c$, then $a$ is also equivalent to $c$.

% ToDo: write down the conditions in math notation - maybe in a tabular

% https://www.youtube.com/watch?v=Ogm711KWwaw
% at around 14:00 The identity is the smallest equivalence relation and AxA (the "all-relation"? "universal relation" is the largest)
% https://www.ask-math.com/universal-relation.html

% https://en.wikipedia.org/wiki/Equivalence_relation
% https://mathworld.wolfram.com/EquivalenceRelation.html

\paragraph{Orders} are yet another special kind of relations. They are also a type of relation defined on one set, i.e. the left and right sets are the same. If a set $A$ is equipped with such an \emph{order} relation, the set is said to be \emph{ordered}. An order relation is typically denoted by the symbol $<$ which we read as "less than", i.e. $a < b$ reads as: "$a$ is less than $b$".

...TBC...
% -trichotomy
% -transitivity
% -it can be inverted into >
% -we have also "less than or equal to", "greater than or equal to"
% -partial and total orders 
% -well ordering - well ordered sets have a least element
% -compatibility with operations: if $a < b$ then $a + c < b + c$ for any $c \in A$

% https://en.wikipedia.org/wiki/Order_theory
% https://en.wikipedia.org/wiki/Total_order
% https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_order
% https://en.wikipedia.org/wiki/Ordered_field

% https://math.libretexts.org/Bookshelves/Applied_Mathematics/Seven_Sketches_in_Compositionality%3A_An_Invitation_to_Applied_Category_Theory_(Fong_and_Spivak)/01%3A_Generative_Effects_-_Orders_and_Adjunctions/1.01%3A_What_is_Order

\medskip
As we see, relations are quite flexible and can be used to model several important concepts in mathematics by imposing some additional requirements on the broad and general concept of a relation.
% ToDo: mention some other kind special kinds/classes of relation

%---------------------------------------------------------------------------------------------------
\subsubsection{Set Operations and Relationships}

\paragraph{Set Algebra} Just like we could combine logical propositions via the connectives $\wedge, \vee, \neg, \ldots$ to yield new propositions, there are operations that we can perform on sets to yield new sets. The \emph{intersection} of two sets $A, B$ is denoted as $A \cap B$ and defined to be the set with the elements that are present in $A$ and in $B$, i.e. the set of elements that $A$ and $B$ have in common. If the intersection between $A$ and $B$ is empty, i.e. the two sets have nothing in common, formally denoted as $A \cap B = \emptyset$, we say that $A$ and $B$ are \emph{disjoint}. The visual resemblence of $\cap$ and $\wedge$ is, of course, no coincidence. The \emph{union} of two sets $A,B$ is the set of all elements that are in $A$ or in $B$ where the "or" is again to be understood as an inclusive or. Imagine throwing all contents of sets $A$ and $B$ together into a bucket. You may get doublings for the common elements (i.e. the intersection) but that doesn't matter anyway because set membership does not care about potential multiplicities - but if you want, you can imagine to remove the doublings after throwing the sets together. The notation for the union of $A$ and $B$ is: $A \cup B$. The symbol looks a bit like a cup and resembles the "or" symbol $\vee$ from logic - again no coincidence. There is also a notion of a set \emph{difference} $A \setminus B$ which is the set $A$ minus those elements of $A$ which are also in $B$. You may read this as "$A$ without $B$". [VERIFY!] If we assume that we have some sort of universal set, i.e. the set of all things that we could possibly consider in the current context, we may also define the \emph{complement} of a set $A$, denoted as $\overline{A}$, which is the set of "everything" except the elements of $A$. Here, "everything" refers to our universal set which may depend on the context. If we call this universal set $U$, we could define $\overline{A} = U \setminus A$. Finally, the so called \emph{cartesian product} or \emph{set product} or just \emph{product} of two sets $A$ and $B$, denoted as $A \times B$, is the set of all possible pairs $(a,b)$ where $a$ is an element of $A$ and $b$ is an element of $B$. Here is a summary of these basic set operations:
\begin{eqnarray}
 \overline{A}  =& \{x: \; x \notin A \}                    \qquad &\text{complement} \\	
 A \cap B      =& \{x: \; x \in A \wedge x \in    B \}     \qquad &\text{intersection} \\
 A \cup B      =& \{x: \; x \in A \vee   x \in    B \}     \qquad &\text{union} \\
 A \setminus B =& \{x: \; x \in A \wedge x \notin B \}     \qquad &\text{difference} \\
 A \times B    =& \{(x,y): \; x \in A \wedge y \in    B \} \qquad &\text{product}
\end{eqnarray}
We may iterate the product operation in the following way: Form the product $A \times B$ and then take the result of that and form the product with a third set $C$ to get: $(A \times B) \times C$. What we formally get would be a set of pairs where the first element is itself a pair of elements from $A$ and $B$ and the second element is an element from $C$. Elements of $(A \times B) \times C$ would look like $((a,b),c)$ where $a \in A, b \in B, c \in C$. On the other hand, elements of $A \times (B \times C)$ would be of the form $(a, (b,c))$. Formally, this is a different set, so our set product is formally not associative. However, the set $(A \times B) \times C$ is isomorphic (i.e. of the "same form") to $A \times (B \times C)$ and in many practical applications, what we actually want to form is not a set of nested pairs but rather a set of triples $(a,b,c)$ with no further inner structuring. We will adopt the convention that when we write a set product with multiple factors without any parentheses like: $A \times B \times C$, we mean the set of triples $(a,b,c)$ where $a \in A, b \in B, c \in C$. And this, of course, generalizes to quadruples, quintuples, etc. We will also use the notation $A^n$ to mean a set of $n$-tuples in which each element is from $A$.



\medskip
There are a lot of algebraic equations involving these operators.... [TODO: give a rather comprehensive list of useful set-algebraic equations]

\medskip
In addition to these basic and most common set operations, there are a couple of more, less common ones. [TODO: explain symmetric difference, disjoint union, $A^B$, $2^A$, quotient set i.e. set of equivalence classes, ... then maybe give more equations involving those less common operations]

% https://en.wikipedia.org/wiki/Disjoint_union
% https://en.wikipedia.org/wiki/Symmetric_difference

% https://math.stackexchange.com/questions/1631396/what-is-the-difference-between-disjoint-union-and-union

% Weitz uses a union symbol with a dot above to denote a union of disjoint sets. see:
% https://www.youtube.com/watch?v=RcDjuXLK-Jg&list=PLb0zKSynM2PDUcEEkjv48Y_4N9CBFyzsz&index=7
% at 9:00.  ...don't confuse this with the "disjoint union" operation which artificially makes sets disjoint before froming the union. Maybe explain both notations in the section of set theory 







\paragraph{Subsets} The set algebra can be seen as providing some operations that we can perform on sets. Sometimes, we also want to express certain relationships between sets. Of particular interest is, when one set $B$ contains all the elements that another set $A$ contains (and possibly more). In such a case, we call $A$ a \emph{subset} of $B$ and we call $B$ a \emph{superset} of $A$. The set $B$ may or may not contain other elements, i.e. elements that are not in $A$. If the superset $B$ does indeed contain additional elements, then we call $A$ a \emph{strict subset} of $B$ and we call $B$ a \emph{strict superset} of $A$. These relations are expressed with the notation $\subseteq, \subset, \supset, \supseteq$ as follows:
\begin{eqnarray}
A \subseteq B \;\;  \Leftrightarrow \;\;   x \in A \Rightarrow x \in B&           \qquad & \text{$A$ is subset of $B$} \\
A \supseteq B \;\;  \Leftrightarrow \;\;   x \in B \Rightarrow x \in A&           \qquad & \text{$A$ is superset of $B$} \\
A \subset   B \;\;  \Leftrightarrow \;\;   x \in A \Rightarrow x \in B,& A \neq B \qquad & \text{$A$ is strict subset of $B$} \\
A \supset   B \;\;  \Leftrightarrow \;\;   x \in B \Rightarrow x \in A,& A \neq B \qquad & \text{$A$ is strict superset of $B$}
\end{eqnarray}
% Alignment is ugly! Maybe use a tabular environment
With the subset relation defined, we can also define yet another operation on sets: taking the set of all subsets of a given set $A$. This operation is called taking the \emph{power set} and the power set of a set $A$ is denoted by $\mathcal{P}(A)$ or by $2^A$ where the latter notation is motivated by the observation that, if the set $A$ has $n$ elements, then the power set will have $2^n$ elements. This also explains the name "power set". Formally, the power set can be written as:
\begin{equation}
  \mathcal{P}(A) = 2^A = \{x : \; x \subseteq A \}
\end{equation}
Here, the elements $x$ of the power set $2^A$ are themselves sets - namely, subsets of $A$. The empty set does also count as a subset of any nonempty set. If a set has $n$ elements, then the number of subsets with $k$ elements is given by the binomial coefficient "$n$-choose-$k$". [VERIFY, REF needed]. The fact that the power set of a finite set $A$ with $n$ elements has $2^n$ elements can also be understood as follows: For each subset, we may define a function $f$ from $A$ to the set $\{0,1\}$ which maps an element $a$ of $A$ to $1$, if the element $a$ is to be included into the subset and to $0$ if the element $a$ is not to be included. For such a function $f$, there are $2^n$ different possibilities because for each element $a \in A$ (of which there are $n$), we can choose either $0$ or $1$ as the mapped value. Including an element into a subset or not is a binary decision. We can establish a bijection between the subsets of $A$ and the set of possible functions $f: A \rightarrow \{0,1\}$, so these sets must have the same number of elements - namely, both have $2^n$ elements.

\paragraph{Cardinality} While we are speaking of the number of elements of a set, i.e. the "size" of a set, it should be noted that this size has been given a special name: \emph{cardinality}. We do not simply call it "size" because there are different notions of set size in mathematics and cardinality is just one of them (another one would be the so called \emph{measure}, for example). In the case of finite sets, the cardinality is just the number of elements, i.e. a natural number (to be defined in the next chapter). For infinite sets, however, set sizes are given by the so called \emph{cardinal numbers} the investigation of which is a whole branch of math on its own. The natural numbers are the finite cardinal numbers - but there are infinite cardinal numbers, too. And yes, that means there are different sizes of infinity in math. In a sense that can be made rigorous, there are more real numbers than natural numbers, for example. The cardinal number of the set of real numbers is bigger than that of the set of natural numbers. The notion of cardinality for infinite sets is defined in terms of existence of bijections: If for two infinite sets, you can find a bijective function between those sets, then these sets have, by definition, the same cardinality. This has a couple of counterintuitive consequences, one of which is that a strict subset of an infinite set can have the same cardinality as the whole set. The part is \emph{not} smaller than the whole. For example, the set of natural numbers and the set of even numbers have the same cardinality. The required bijection and its inverse are simply the functions $y = f(x) = 2 x$ and $x = f^{-1}(y) = y / 2$. But this stuff is beyond the scope of this book. [TODO: give equations for cardinalities]

\paragraph{Relations vs Operations}
We defined a function to be a special kind of relation (left-total, right-unique) and we also intuitively characterized a function as a sort of operation with an input and an output. The operational viewpoint is the way, we usually think of functions: we put some object in and get some object out. It is important to realize that such an "operational" point of view is not necessary. From a set-theoretic point of view, the "relational" point of view is all there is: a function is just a special kind of relation and a relation is just a special kind of set consisting of ordered pairs and ordered pairs can also be thought of a special kinds of sets. The definition really says nothing at all about a function being an input/output "operation". This is merely our interpretation. Or maybe it's more appropriate to say, that an input/output device is something that we want to \emph{model} with the concept of a function. At the core, it all just boils down to specific sorts of sets, because in set theory, sets are really the only thing that we have to work with anyway.

\medskip

A similar consideration can be applied to equivalences. By definition, they are also just a special kind of relation. When we use equivalences, for example to simplify equations, we sometimes interpret an equivalence as a possible replacement rule that can be applied to (parts of) a formula, i.e. a sort of operation. That's not what an equivalence is, at its core, though. It's a common misconception to think of the equals sign $=$ as a prescription to do something like an assignment or replacement when in reality, it just expresses the fact that two things are equal. An equation like $E = m c^2$ can indeed be used to compute $E$ when $m$ and $c$ are known but $E$ is yet unknown and that's indeed how we often use equations: to compute an unknown value from known ones. But the equation in and of itself, at its core and by its nature, is not an algorithmic prescription for a computation. It's just a relation.

%there's not really an intrinsic input/output interpretation
%-explain the relational and operational aspects of 
% -functions (is-related vs input-output) 
% -equivalences (is-equivalent-comparisons vs assigments in programming and simplifications in math)
% -subset relations (is-a-subset vs form/extract-a-subset)
% -explain bi- and multivariate functions (using a set-product as input), 
% -functions of a scalar that yield vector outputs (parametric curves) using a set-product as output
% -explain multifunctions such as the n-th root of a complex number
% -explain the notation f: A -> B
% -explain how the interpretations of two-input/one-output functions: A x B -> C and one-input/two-output function A -> B x C can be though of as ternary relations, i.e. subsets of A x B x C where formally A -> B x C is *interpreted* as A x (B x C) and A x B -> C as (A x B) x C but the interpretation can be "flattened out" and we can do partial application, currying, etc.



% what about relational algebra? useful for databases, I think

% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
%Cardinality

\subsection{Category Theory}
Category theory is an area of math that attempts to structure and organize mathematics itself. This section here is meant to only give a very superficial birds eye overview. Category theory has been called the "mathematics of mathematics" and - less charitably - as "abstract nonsense". Its relation to set theory has been compared to that of higher level programming languages to machine code. Category theory describes a given field of mathematics in terms of a so called \emph{category}. Such a category consists of a class of \emph{objects} and relationships between those objects, called \emph{morphisms}. You can picture the objects as nodes (as in a multigraph, see [REF needed to Graph Theory section]) and the morphisms as directed edges, depicted as arrows. Morphisms are also sometimes called arrows. The category must also have a notion of composing morphisms. When there's an arrow from node $A$ to node $B$ and another arrow from node $B$ to node $C$ then this induces an arrow from node $A$ to node $C$. This sort of \emph{composition} of morphisms into other morphisms can be pictured as traversing a path in the multigraph. The composition of morphisms must be associative. As final ingredient for a category, we need for each node a special kind of morphism, called an \emph{identity}. An identity is a morphism/arrow that goes from a node $A$ to itself and is meant to convey an abstraction of the idea of an identity function - a sort of neutral element in the realm of morphisms.
%ToDo: composition (-> paths), identites (-> loopy edges)

% https://en.wikipedia.org/wiki/Multigraph

\paragraph{The category "Set"} is perhaps the most immediate and prototypical example of a category. In Set, the objects are sets and the morphisms are functions. Composition of morphisms is the usual compositon of functions and the identites are of course the respective identity functions for each set. Don't make the mistake of thinking about set elements as objects. The sets themselves are the objects and their internal structure is considered opaque. From a categorical point of view, we can't look into the sets. We can't talk about individual elements at all in categorical terms - the reason being that, in general, the objects can be of a completely different nature and don't need to have a concept of "elements". One object in Set would be $\mathbb{N}$, another one would be $\mathbb{R}$, another one would be $\{0,1\}$, etc. The category Set has a node for every possible set and for every possible ordered pair $(A,B)$ of nodes (i.e. sets), it has bunch of directed edges (arrows) where each such arrow stands for a possible function from set $A$ to set $B$. For example, there would be an arrow called "$\sqrt{\; \;}$" from $\mathbb{N}$ to $\mathbb{R}$. There would also be a "$\sqrt{\; \;}$" from $\mathbb{R}$ to $\mathbb{R}$ and one from $\mathbb{N}$ to $\mathbb{N}$. As morphisms, they would all be considered different entities even though they are supposed to stand for the same mathematical operation of taking the square root. Each morphism carries along with it its source (or domain) and target (or codomain). Functions can have certain properties and the most important ones are injectivity, surjectivity and bijectivity. Within the framework of set theory, these properties are defined in a way that references set elements. For example, a function from $A$ to $B$ is defined to be bijective, iff every element $a \in A$ maps to a unique element $b \in B$ and vice versa.

% Other examples:
% category of proofs: objects: propositions, arrows: deductions

\paragraph{Iso-, Mono- and Epimorphisms}
Abstracting the idea of bijective functions to a more general setting in which the objects are not necessarily sets and the morphisms are not necessarily functions leads to the categorical idea of an \emph{isomorphism} which is a particular kind of morphism with certain additional properties. The key here is that in the definition of what properties such an isomorphism must satisfy, we are not allowed to talk about "elements". The only things that we are allowed to talk about are categorical terms like objects, morphisms, compositions and identities. We must capture the idea of bijectivity in terms of these and only these ideas. It goes like this: A morphism $f: A \rightarrow B$ is an isomorphism, if there exists a morphism $g: B \rightarrow A$ such that $g \circ f = id_A$ and  $f \circ g = id_B$. In this context, $g$ is the inverse morphism of $f$ and itself an isomorphism as well. In terms of elements when we are in Set: we take an element $a \in A$, apply $f$ to map it to an element $b \in B$, then apply $g$ to map $b$ back to an element $a' \in A$, then $a' = a$, i.e. we're back to where we started so the whole roundtrip from set $A$ to set $B$ and back to $A$ reduces to an identity operation in $A$. Stated without mentioning elements and therefore generalizable: Applying $f$ first, then $g$ yields a composed morphism that equals the identity in $A$ and applying $g$ first, then $f$ yields the identity in $B$. Note how at no point are we talking about the internal structure of the objects like we did when referencing set elements in the definition of bijective functions. The categorical definition of an isomorphism is not even focusing on the objects at all but it's all about the morphisms. The objects are just the backdrop and our main attention goes to the morphisms. We are indeed looking at things from a higher level than in set theoretical descriptions where we looked \emph{inside} the details of the objects. This is typical of category theoretical definitions and theorems. Likewise, a monomorphism $f: A \rightarrow B$ is characterized by the property that for all morphisms $g,h: C \rightarrow A$, we have that $f \circ g = f \circ h$ implies $g = h$. Monomorphisms generalize the idea of an injective (i.e. left-unique) function. An epimorphism $f: A \rightarrow B$ is defined by the property that for all morphisms $g,h: B \rightarrow C$, we have that $g \circ f = h \circ f$ implies $g = h$. Epimorphisms generalize the idea of a surjective (i.e. right-total) function. [VERIFY] If you don't immediately see why these definitions indeed capture and generalize the ideas of injectivity and surjectivity (I certainly didn't), try reversing the implications. For injectivity/monomorphisms, rewrite $(f \circ g = f \circ h) \Rightarrow (g = h)$ as the equivalent statement $(g \neq h) \Rightarrow (f \circ g \neq f \circ h)$ and draw some picture where $g \neq h$. Then do something similar for surjectivity/epimorphisms. Then pat yourself on the back for having grasped a rather abstract and obscure categorical definition. [TODO: maybe add a figure that shows this].
%pg 482 in Ehrig et al

% https://www.youtube.com/watch?v=H0Ek86IH-3Y
% Initial objects have arrow to all objects, terminal object arrows from all objects

% endomorphism, automorphism, homomorphism - the set of morphisms is also sometimes called "Hom" - why?

%ToDo: 
%-explain the categorical analogs of injective and surjective functions (mono- and epimorphisms)
% -explain why the definitions of mono- and epimorphisms work, i.e. capture the desired idea
% -reverse the implications: g != h  ->  g°f != h°f etc. and draw pictures where g != h and show how that implies the RHS
%-give other examples of categories: FinSet, Grp, Vect, deductive systems, Graph
%-explain subcategories in this context - many other examples actually are subcategories of Set
%-examplin categorical products and coproducts
%-explain functors, natural transformations

% https://en.wikipedia.org/wiki/Category_theory
% https://plato.stanford.edu/entries/category-theory/#Exam

% https://www.youtube.com/watch?v=SmXB2K_5lcA  Category Theory for Programmers: Chapter 1 - Category

% https://www.youtube.com/watch?v=1TvNeFLGMrE  Abstrakter Unsinn? Was ist Kategorientheorie?

% https://math.jhu.edu/~eriehl/context.pdf

% https://texample.net/tikz/examples/tag/diagrams/
% https://texample.net/tikz/examples/labeled-chain/

% The Mathematician's Weapon | An Introduction to Category Theory, Abstraction and Algebra
%https://www.youtube.com/watch?v=FQYOpD7tv30
% "Category theory is the abstraction of composition"
% has some nice examples of categories
% https://www.youtube.com/watch?v=5Ykrfqrxc8o&list=PLoCKNPo3VR0I2wqT2wemCNIlpjdy_Ry_q&index=2

% Maybe draw a diagram for Set with some example objects (sets) and arrows (functions)
% Q\{0} x Q  ->  Q:  (a,b) -> a/b
% R  ->  {0,1}:  a -> 1, if a rational, 0 otherwise 

\medskip 
Of course, there's much more to say about category theory but this very brief overview shall suffice for a book that attempts to focus on applied math.