\section{Foundations} 

This book is about applied math, so we will not go deeply into the foundations of math. However, a brief and very superficial look into this topic is a good idea to set the stage for the material that follows. It also establishes the basics of the language which we will need to talk about mathematical concepts.

\subsection{Logic}
Math is the pursuit of finding truths, so it makes sense to have a framework, within which we can say that something is true or false. In mathematics, that framework is mathematical logic, more specifically propositional logic (sometimes called zeroth order logic) and predicate logic (a.k.a. first order logic). 

\paragraph{Propositional logic} deals with propositions which are statements that can be either true or false. You also have ways of combining given propositions $A,B$ to make new, more complex propositions. For example, you can combine two propositions with a logical "and" (usually denoted as $\wedge$). The resulting new proposition $A \wedge B$ is true, if and only if both of the input propositions $A,B$ are true, otherwise it's false. You also have a logical "or" (denoted as $\vee$), which in this context is taken to be an inclusive or: the combined proposition $A \vee B$ is true, if any one of the input propositions or both are true. You also have a logical "not" (denoted as $\neg$) which takes a single proposition as input and the result $\neg A$ is true, if the the input $A$ is false and vice versa. These operators that take in one or two propositions to produce a new proposition are called logical \emph{connectives}. In this context, the basic propositions like $A,B$ are sometimes called \emph{atomic}. Logic also provides the tools that are required to figure out whether a given proposition is true, given that some other propositions are true. That process of drawing conclusions from given (true) propositions is called deduction. In this process of deduction, yet another logical connective, called the \emph{implication} and denoted by $\Rightarrow$, plays a central role. A proposition like $A \Rightarrow B$ means: "$A$ implies $B$" which you may also interpret and read as "$B$ follows from $A$" or as "if $A$, then $B$". The combined proposition $A \Rightarrow B$ evaluates to "true" if, whenever $A$ is true, then $B$ is also true. It makes no statement about situations when $A$ is false, i.e. when $A$ is false, it doesn't matter, if $B$ is true or false - $A \Rightarrow B$ still evaluates to true. The only way that $A \Rightarrow B$ can evaluate to false is when $A$ is true and $B$ is false. Logical equivalence of two propositions $A,B$ is also sometimes important and denoted as $A \Leftrightarrow B$ and it can be decomposed into two implications that are "and"ed together: $(A \Rightarrow B) \wedge (B \Rightarrow A)$, i.e. into a mutual implication. The parentheses are actually optional due to suitably defined operator precedence rules: $\Rightarrow$ precedes $\wedge$. You can read the statement $A \Leftrightarrow B$ as "$B$ if and only if $A$", "$A$ if and only if $B$" or "$A$ is logically equivalent to $B$". To build up mathematics, propositional logic is not quite expressive enough, so...
% https://en.wikipedia.org/wiki/Propositional_calculus
%https://en.wikipedia.org/wiki/Logical_connective
% explain redundancy of the set of connectors - pick the topic up at where we see that the equivalence can be expressed as "and"ing two implications
% explain logical implications and the equivalent ways to express them: (A -> B)  <->  (~B -> ~A)  <->  (~A or B)  <->  (~(~B and A))  ...this is used later in the section about category theory
% read it as: "A implies B", "B follows from A", "A is logically equivalent to B"

% explain logical equivalences (A <-> B)  <-> (A -> B and B -> A)
% read it as "B iff A", "A iff B" where "iff" is short for "if and only if"

%introduce truth tables


\paragraph{Predicate Logic} builds up on propositional logic and a bit of set theory to let you talk about objects and relations between them. There, you have so called quantors like the symbol for "there exists an object such that..." (denoted as $\exists$) or a symbol for "for all objects it is true that..." (denoted as $\forall$). There are yet other levels and kinds of logic, but these two are enough for the moment. ...TBC...
%introduce the $\exists ! x$ quantor meaning "there is exactly one x, such that"

\subsection{Proofs} % find better title
%\subsection{Finding Truths} % find better title

\subsubsection{Axioms}
One has to start somewhere. That starting point is typically a set of \emph{axioms} together with the rules of logic. An axiom is a proposition that is just assumed to be true without further justification. Axioms should state things that are "obviously true". An example are the Peano axioms, some of which are: zero is a natural number, each natural number has a successor, any number is equal to itself, etc. If you really want to build up the whole tower of mathematics axiomatically, you have to \emph{choose} a set of axioms and from there, using only the rules of logic, i.e. deduction, find new propositions that are also true.
% Mention Peano Axioms, Zermelo-Frenkel(-Choice) ZFC, etc.

\subsubsection{Theorems and Proofs}
If you want to prove a proposition, the tools that you have in hand are all the propositions that are already known to be true together with the rules of logic. A proposition is known to be true if it is either an axiom or it has been previously proven by the same technique. A \emph{proof} for a proposition is a sequence of true propositions in which each one follows from known or previous ones by applying the rules of logic and the last of which is the one you actually wanted to prove in the first place. If a proposition has been proven to be true, it becomes a \emph{theorem}. The idea of a theorem is a fundamentally important concept in math - math is all about finding theorems. You may have observed a pattern by looking at a bunch of examples and you may \emph{conjecture} that the pattern is generally true. What you then have to do is to find a proof for your conjecture. If you have succeeded in this highly creative endeavour (i.e. your proof is determined to be correct by the mathematical community), your conjecture is elevated to the venerable status of a theorem. And if the theorem is important enough and you were the first to prove it, you will typically achieve immortality by having your name attached to the theorem for the rest of eternity. Thousands of years later, still everbody knows the name of Pythagoras today - although, it wasn't actually him who proved "Pythagoras' Theorem" - sometimes the world is a little unfair, too :'-(. Along the way of finding a proof, you may generate a whole bunch of proven propositions, some of which are only instrumental to your final goal, some of which are spinoffs, etc. There are some other terms for such "lesser theorems" such as "lemma", "corollary", etc. A theorem is usually a result with a certain level of importance, generality and usefulness. You wouldn't call something like $3+5=8$ a theorem, for example - although it manifestly is a true proposition (and can actually be proven).

\subsubsection{Definitions}
OK - this is kind of awkward. We now have to \emph{define} what \emph{a definition} is. A definition is actually just an agreement about certain conventions to be used in the following material, in particular about what a given term or symbol is supposed to mean. Definitions often stand at the beginning of the development of a new subject. Definitions cannot be right or wrong. They can just be more or less useful. For a definition to be useful, it should clearly encapsulate a concept that is important in the development of all the things further down the line. The so defined term or symbol shall be used a lot in the material to be developed and will be referred to often. It makes sense to pick definitions in such a way that theorems can be stated succinctly. What the most useful definitions for a particual (new) mathematical subject are is often not clear from the get go but instead crystallizes out over time as experience with the new subject grows and when a bit of hindsight is available. As users of math, we may take definitions for granted because smart mathematicians have already figured (and fought) them out for us (and for themselves, of course). But we should keep in mind that they are fundamentally just conventions to make it possible (and ideally convenient and easy) to talk about a given subject. They are not fundamental truths. They just establish the language that we will use. That's why sometimes different authors use different definitions. Sometimes there is just no universal consenus (yet or ever) about which definitions are the most useful ones. Which ones are more or less useful may also differ from field to field. So, care has to be taken when reading mathematical material from different sources - the definitions in use may not always agree.



\subsection{Set Theory}
Set theory is often said to be the foundation of all mathematics - even much more fundamental than the natural numbers. 
%In fact, it is possible to "construct" the natural numbers from sets. We will not go down this road though, since this is not really relevant in applied math. 
The idea of a set was initially introduced by Georg Cantor in an intuitive way. His way of establishing set theory later turned out to have some flaws which is why it was later rebuilt more formally. The result of this rebuild is called "axiomatic set theory" and is very abstract and formal. Fortunately, Cantor's view, which is today sometimes called "naive set theory", is good enough for us. 

\subsubsection{Sets}
In Cantor's definition "A set is a gathering together into a whole of definite, distinct objects of our perception or of our thought which are called elements of the set.". So, in essence, a set is just a bunch of things. A very general concept indeed. Sets are usually denoted in curly braces. For example, the set of the 3 letters a,b,c would be denoted as $\{a,b,c\}$. Two sets are considered equal, if and only if they contain the same elements. It does not matter in which order the elements are written down or if an element appears multiple times. So that means, for example, the sets $\{c,a,b\}$ or $\{a,c,a,a,b,c\}$ are in fact equal to the set $\{a,b,c\}$. By the way, the phrase "if and only if" appears sufficiently often in math texts that some authors use the abbreviation "iff" for that - yes, that's an "if" with a double-f. I may sometimes use that, too. Sets can be given names, for example, we may call our set above $S$ and we may write this as $S = \{a,b,c\}$. Element membership is denoted by an $\in$ symbol, so to express the fact that $b$ is an element of the set $S$, we would write $b \in S$. If we want to express that a certain object, say $d$, is not an element of a set $S$, we write this as $d \notin S$.

\medskip 
Sets can have other sets as elements and that nesting capability can be used recursively to build arbitrarily complex structures purely from sets. These structures also include the number systems that are used in math. For example, the number zero can be represented by the empty set: $0 = \{\}$, which is also denoted by $\emptyset$, the number one by the set that contains the empty set (i.e. zero): $1 = \{ 0 \} =  \{ \emptyset \}$, the number two by the set that contains zero and one: $2 = \{ 0, 1 \} = \{ \emptyset, \{ \emptyset \} \}$ and so on. Of course, that's super tedious and nobody actually thinks about numbers this way - but in principle, it can be done. Note that in this context $\emptyset$ and $\{ \emptyset \}$ are different things. The first is the empty set and the second is the set that contains the empty set. The nesting matters. One is an empty box, the other one is a box that contains something: namely, an empty box. If you really go down to the very lowest levels of math, then sets are actually the \emph{only} things that can occur as elements of (other) sets because sets are really the only thing that exists in this world. There is nothing else but sets. [VERIFY!]

\medskip 
In math, the sets we are dealing with are often sets of numbers and they may have many or even infinitely many elements. To denote very large or infinite sets compactly there are notations based on predicate logic. For example to denote the set of all numbers larger than 100 but less than 1000, we may write $\{x : x > 100 \wedge x < 1000\}$, but now we are getting ahead of ourselves. To understand that notation, we actually first have to understand what $>$ and $<$ means. I'm pretty sure, you already do know what they mean, but in the context of set theory, these symbols first need to be defined, too. To do so, we need some more tools...
% it's called "set builder notation", I think

\subsubsection{Tuples}
Sometimes, we may want to model situations in which the order of entries actually does matter. Sets are per definition not suitable for this (at least not directly), so we need something else. That other thing is the tuple. A tuple is typically denoted by listing the entries in parentheses. The 3-tuple $(a,b,c)$ is not equal to $(c,a,b)$. Tuples with two elements are also called ordered pairs, 3-tuples triples, 4-tuples quadruples and 5-tuples quintuples. When we talk about a tuple, the entries are no longer called "elements" but instead \emph{coordinates}. If you really want to be puristic and build \emph{everything} from sets, you need to use sets to model tuples, too. You could just pack the tuple members into another set with another object that serves as index, so $(a,b,c)$ would become $\{\{a,1\},\{b,2\},\{c,3\}\}$ and $(c,a,b)$ would become  $\{\{c,1\},\{a,2\},\{c,3\}\}$. These sets of sets would indeed be uniquely indentified purely by their elements regardless of order because the correct order could be reconstructed due to the second element in the inner sets wich serve as tags. While this definition, proposed by Hausdorff, is intuitive, it has a problem: We would need to require that our set of indices is disjoint from all the sets that make up the tuple's coordinates. Otherwise, we could not distiguish between $(1,2)$ and $(2,1)$, for example. There are other ways to model tuples purely via sets that avoid this problem but are less intuitive. The currently accepted definition is due to Kuratowski and defines the ordered pair $(a,b) = \{ \{a\}, \{a,b\} \}$. From ordered pairs, bigger tuples can be defined recursively. For example, a triple can be defined as $(a,b,c) = ((a,b),c)$. [VERIFY!] I won't expand this any further because it gets messy really quick. Nobody really thinks about tuples this way anyway. The point is to convince yourself once and for all that it is possible to express tuples via sets and from then on, use the higher level tuple notation.

% = (\{\{a\}, \{a,b\} \}, \; c)= \{ \{\{a\}, \{a,b\} \}, \; \{ \{\{a\}, \{a,b\},c \} \}\}$.

% https://en.wikipedia.org/wiki/Ordered_pair#Defining_the_ordered_pair_using_set_theory

%https://en.wikipedia.org/wiki/Tuple

\subsubsection{Relations}
A relation can formally be defined to be a set of tuples. Of particular importance are binary relations, i.e. sets of 2-tuples, aka ordered pairs. As an example, consider the two sets $A = \{1,2,5\}, B = \{0,2,4\}$. We can define a relation $R$ between the sets $A$ and $B$ as a set of ordered pairs where the first entry comes from $A$ and the second from $B$, for example: $R = \{(1,2),(1,4),(2,4)\}$. Such a relation can be visualized pictorially by drawing the two sets side by side and drawing an arrow between each pair of elements that is in the relation. This is shown for our relation $R$ in figure .... ...TBC...
% explain left/right totality, left/right uniqueness - see Bill Shillito's course on youtube
% use as example A = {1,2,3,4}, B = {0,2,4} and the < relation. R = {(1,2),(1,4),(2,4),(3,4)}
% not left-total due to (4,X) missing
% not right-total due to (X,0) missing
% not left-unique due to (1,4),(2,4),(3,4)
% not right-unique due to (1,2),(1,4)
% draw diagrams

\begin{figure}[h]
\centering
	
\begin{tikzpicture}
[mydot/.style={circle, fill=lightgray, inner sep=5pt}, >=latex, shorten >= 1pt, shorten <= 1pt]

% Left set:
\node[mydot,                    label={center:1}] (a1) {}; % 1
\node[mydot, below=0.3cm of a1, label={center:2}] (a2) {}; % 2
\node[mydot, below=0.3cm of a2, label={center:5}] (a3) {}; % 5
	
% Right set:	
\node[mydot, right=1.5cm of a1, label={center:0}] (b1) {}; % 0
\node[mydot, below=0.3cm of b1, label={center:2}] (b2) {}; % 2
\node[mydot, below=0.3cm of b2, label={center:4}] (b3) {}; % 4

% Arrows for the relations:
\path[->] (a1) edge (b2);                                  % 1 -> 2
\path[->] (a1) edge (b3);                                  % 1 -> 4
\path[->] (a2) edge (b3);                                  % 2 -> 4

% Ellipses around the sets:
\draw (0.0,-0.75) ellipse (0.5cm and 1.5cm);
\draw (2.0,-0.75) ellipse (0.5cm and 1.5cm);

\end{tikzpicture}
\end{figure}
% https://tikz.dev/tikz-transformations
% see:
% https://tex.stackexchange.com/questions/157450/producing-a-diagram-showing-relations-between-sets
% https://latex.org/forum/viewtopic.php?t=21987
There are a couple of important features that such a relation may or may not have. In a \emph{left-total} relation, every element in the left set has an outgoing arrow emanating from it. In a \emph{right-total} relation, every element in the right set has an incoming arrow. In a \emph{right-unique} relation, every element in the left set has at most one outgoing arrow. The naming convention reflects the idea that when you pick one element from the left set, the related  element of the right set, if any, is uniquely determined. We always know, where we have to go. Likewise, in a \emph{left-unique} relation, every element of the right set has at most one incoming arrow. For every element from the right set, the related element from the left set, if any, is uniquely determined. We always know, where we came from. As we can see, our example relation $R$ actually has none of these features.
% It's actually the "<"  relation - maybe mention it in the section about orders...but that doesn't really fit well because in orders domain and codomain are the same

% https://www.youtube.com/watch?v=Tk5_B7w5fiY&list=PLZzHxk_TPOStgPtqRZ6KzmkUQBQ8TSWVX&index=9

\paragraph{Functions} are a special kind of relations, namely those relations which are left-total and right-unique. You can think of functions as a definition of a map from a set $A$ to another set $B$. Each element of $A$ gets mapped to some unique element of $B$. You can throw any $a \in A$ at a function $f$ and it will unambiguously tell you, which $b \in B$ your given $a$ is mapped to. If a function is additionally left-unique, we can actually traverse the arrow back to figure out unambiguously, where we came from. We can undo the application of the functional mapping for those $b \in B$ which have an incoming arrow. Such left-unique functions are also called \emph{injective}. If, on the other hand, the function is right-total, i.e. every element of $B$ has an incoming arrow, we call the function \emph{surjective}. A function that is both injective and surjective is called \emph{bijective}. Such bijective functions can be inverted, i.e. undone, for any $b \in B$ and are therefore also called \emph{invertible}.
%explain domain, range, image, pre-image (of elements and whole sets)

\paragraph{Equivalences} are another special kind of relations. In equivalence relations, the left and right set are actually the same set and a couple of additional properties must be satisfied. The relation must be \emph{reflexive} meaning that every element $a$ must be in relation with itself. They must also be \emph{symmetric} meaning that if the tuple $(a,b)$ is in the relation, then the tuple $(b,a)$ must also be in it. Finally, they must be \emph{transitive} meaning that if the tuples $(a,b)$ and $(b,c)$ are in the relation, then the tuple $(a,c)$ must also be in it. The prototype of an equivalence relation is the usual equality denoted by $=$ but the concept of an equivalence is more general. It is meant to capture the idea that two objects are interchangable within a given context. They do not necessarily have to be the exact same object but certainly can be - this is ensured by the reflexivity. Symmetry captures the idea that if $a$ is equivalent to $b$ then $b$ is also equivalent to $a$. Transitivity captures the idea that if $a$ is equivalent to $b$ and $b$ is equivalent to $c$, then $a$ is also equivalent to $c$.

\paragraph{Orders} are yet another special kind of relations. They are also a type of relation defined on one set, i.e. the left and right sets are the same. ...TBC...
%trichotomy, partial and total orders

\subsubsection{Set Operations and Relationships}

\paragraph{Set Algebra} Just like we could combine logical propositions via the connectives $\wedge, \vee, \neg, \ldots$ to yield new propositions, there are operations that we can perform on sets to yield new sets. The \emph{intersection} of two sets $A, B$ is denoted as $A \cap B$ and defined to be the set with the elements that are present in $A$ and in $B$, i.e. the set of elements that $A$ and $B$ have in common. The visual resemblence of $\cap$ and $\wedge$ is, of course, no coincidence. The \emph{union} of two sets $A,B$ is the set of all elements that are in $A$ or in $B$ - where the "or" is again to be understood as an inclusive or. Imagine throwing all contents of sets $A$ and $B$ together into a bucket. You may get doublings for the common elements (i.e. the intersection) but that doesn't matter anyway because set membership does not care about potential multiplicities - but if you want, you can imagine to remove the doublings after throwing the sets together. The notation for the union of $A$ and $B$ is: $A \cup B$. The symbol looks a bit like a cup and resembles the "or" symbol $\vee$ from logic - again no coincidence. ...TBC: difference, complement, cartesian product, disjoint union
\begin{eqnarray}
 \overline{A}  =& \{x: \; x \notin A \}                    \qquad &\text{complement} \\	
 A \cap B      =& \{x: \; x \in A \wedge x \in    B \}     \qquad &\text{intersection} \\
 A \cup B      =& \{x: \; x \in A \vee   x \in    B \}     \qquad &\text{union} \\
 A \setminus B =& \{x: \; x \in A \wedge x \notin B \}     \qquad &\text{difference} \\
 A \times B    =& \{(x,y): \; x \in A \wedge y \in    B \} \qquad &\text{product}
\end{eqnarray}
% There are a lot of algebraic equations involving these operators - maybe list some of them...or maybe all - actually, the goal is to give a comprehensive list of useful equations
% explain how the set product is fomally non-associative, i.e. (A x B) x C yields nested pairs of the form ((a,b),c) while A x (B x C) yields nested pairs of the form (a, (b,c)) - but they are usually interpreted in their "flattened" form, i.e. as triples (a,b,c)

\paragraph{Subsets} The set algebra can be seen as providing some operations that we can perform on sets. Sometimes, we also want to express certain relationships between sets. Of particular interest is, when one set $B$ contains all the elements that another set $A$ contains (and possibly more). In such a case, we call $A$ a \emph{subset} of $B$ and we call $B$ a \emph{superset} of $A$. The set $B$ may or may not contain other elements, i.e. elements that are not in $A$. If the superset $B$ does indeed contain additional elements, then we call $A$ a \emph{strict subset} of $B$ and we call $B$ a \emph{strict superset} of $A$. These relations are expressed with the notation $\subseteq, \subset, \supset, \supseteq$ as follows:
\begin{eqnarray}
A \subseteq B \;\;  \Leftrightarrow \;\;   x \in B \Rightarrow x \in A&           \qquad & \text{$A$ is subset of $B$} \\
A \supseteq B \;\;  \Leftrightarrow \;\;   x \in A \Rightarrow x \in B&           \qquad & \text{$A$ is superset of $B$} \\
A \subset   B \;\;  \Leftrightarrow \;\;   x \in B \Rightarrow x \in A,& A \neq B \qquad & \text{$A$ is strict subset of $B$} \\
A \supset   B \;\;  \Leftrightarrow \;\;   x \in A \Rightarrow x \in B,& A \neq B \qquad & \text{$A$ is strict superset of $B$}
\end{eqnarray}
% Alignment is ugly! Maybe use a tabular environment
With the subset relation defined, we can also define yet another operation on sets: taking the set of all subsets of a given set $A$. This operation is called taking the \emph{power set} and the power set of a set $A$ is denoted by $\mathcal{P}(A)$ or by $2^A$ where the latter notation is motivated by the observation that, if the set $A$ has $n$ elements, then the power set will have $2^n$ elements. This also explains the name "power set".

\paragraph{Cardinality} While we are speaking of the number of elements of a set, i.e. the "size" of a set, it should be noted that this size has been given a special name: \emph{cardinality}. We do not simply call it "size" because there are different notions of set size in mathematics and cardinality is just one of them (another one would be the so called \emph{measure}, for example). In the case of finite sets, the cardinality is just the number of elements, i.e. a natural number (to be defined in the next chapter). For infinite sets, however, set sizes are given by the so called \emph{cardinal numbers} the investigation of which is a whole branch of math on its own. The natural numbers are the finite cardinal numbers - but there are infinite cardinal numbers, too. And yes, that means there are different sizes of infinity in math. In a sense that can be made rigorous, there are more real numbers than natural numbers, for example. The cardinal number of the set of real numbers is bigger than that of the set of natural numbers. But this stuff is beyond the scope of this book.

\paragraph{Relations vs Operations}
We defined a function to be a special kind of relation (left-total, right-unique) and we also intuitively characterized a function as a sort of operation with an input and an output. The operational viewpoint is the way, we usually think of functions: we put some object in and get some object out. It is important to realize that such an "operational" point of view is not necessarry. From a set-theoretic point of view, the "relational" point of view is all there is: a function is just a special kind of relation and a relation is just a special kind of set consisting of ordered pairs and ordered pairs can also be thought of a special kinds of sets. The definition really says nothing at all about a function being an input/output "operation". This is merely our interpretation. Or maybe it's more appropriate to say, that an input/output device is something that we want to \emph{model} with the concept of a function. At the core, it all just boils down to sepcific sorts of sets, because in set theory, sets are really the only thing that we have to work with anyway.

%there's not really an intrinsic input/output interpretation
%-explain the relational and operational aspects of 
% -functions (is-related vs input-output) 
% -equivalences (is-equivalent-comparisons vs assigments in programming and simplifications in math)
% -subset relations (is-a-subset vs form/extract-a-subset)
% -explain bi- and multivariate functions (using a set-product as input), 
% -functions of a scalar that yield vector outputs (parametric curves) using a set-product as output
% -explain multifunctions such as the n-th root of a complex number
% -explain the notation f: A -> B
% -explain how the interpretations of two-input/one-output functions: A x B -> C and one-input/two-output function A -> B x C can be though of as ternary relations, i.e. subsets of A x B x C where formally A -> B x C is *interpreted* as A x (B x C) and A x B -> C as (A x B) x C but the interpretation can be "flattened out" and we can do partial application, currying, etc.



% what about relational algebra? useful for databases, I think

% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
%Cardinality

\subsection{Category Theory}
Category theory is an area of math that attempts to structure and organize mathematics itself. This section here is meant to only give a very superficial birds eye overview. Category theory has been called the "mathematics of mathematics" and - less charitably - as "abstract nonsense". Its relation to set theory has been compared to that of higher level programming languages to machine code. Category theory describes a given field of mathematics in terms of a so called \emph{category}. Such a category consists of a class of \emph{objects} and relationships between those objects, called \emph{morphisms}. You can picture the objects as nodes (as in a multigraph, see [REF needed to Graph Theory section]) and the morphisms as directed edges, depicted as arrows. Morphisms are also sometimes called arrows. The category must also have a notion of composing morphisms. When there's an arrow from node $A$ to node $B$ and another arrow from node $B$ to node $C$ then this induces an arrow from node $A$ to node $C$. This sort of \emph{composition} of morphisms into other morphisms can be pictured as traversing a path in the multigraph. The composition of morphisms must be associative. As final ingredient for a category, we need for each node a special kind of morphism, called an \emph{identity}. An identity is a morphism/arrow that goes from a node $A$ to itself and is meant to convey an abstraction of the idea of an identity function - a sort of neutral element in the realm of morphisms.
%ToDo: composition (-> paths), identites (-> loopy edges)

% https://en.wikipedia.org/wiki/Multigraph

\paragraph{The category "Set"} is perhaps the most immediate and prototypical example of a category. In Set, the objects are sets and the morphisms are functions. Composition of morphisms is the usual compositon of functions and the identites are of course the respective identity functions for each set. Don't make the mistake of thinking about set elements as objects. The sets themselves are the objects and their internal structure is considered opaque. From a categorical point of view, we can't look into the sets. We can't talk about individual elements at all in categorical terms - the reason being that, in general, the objects can be of a completely different nature and don't need to have a concept of "elements". One object in Set would be $\mathbb{N}$, another one would be $\mathbb{R}$, another one would be $\{0,1\}$, etc. The category Set has a node for every possible set and for every possible ordered pair $(A,B)$ of nodes (i.e. sets), it has bunch of directed edges (arrows) where each such arrow stands for a possible function from set $A$ to set $B$. For example, there would be an arrow called "$\sqrt{\; \;}$" from $\mathbb{N}$ to $\mathbb{R}$. There would also be a "$\sqrt{\; \;}$" from $\mathbb{R}$ to $\mathbb{R}$ and one from $\mathbb{N}$ to $\mathbb{N}$. As morphisms, they would all be considered different entities even though the are supposed to stand for the same mathematical operation of taking the square root. Each morphism carries along with it its source (or domain) and target (or codomain). Functions can have certain properties the most important ones are injectivity, surjectivity and bijectivity. Within the framework of set theory, these properties are defined in a way that references set elements. For example, a function from $A$ to $B$ is defined to be bijective, iff every element $a \in A$ maps to a unique element $b \in B$ and vice versa.

\paragraph{Iso-, Mono- and Epimorphisms}
Abstracting the idea of bijective functions to a more general setting in which the objects are not necessarily sets and the morphisms are not necessarily functions leads to the categorical idea of an \emph{isomorphism} which is a particular kind of morphism with certain additional properties. The key here is that in the definition of what properties such an isomorphism must satisfy, we are not allowed to talk about "elements". The only things that we are allowed to talk about are categorical terms like objects, morphisms, compositions and identities. We must capture the idea of bijctivity in terms of these and only these ideas. It goes like this: A morphism $f: A \rightarrow B$ is an isomorphism, if there exists a morphism $g: B \rightarrow A$ such that $g \circ f = id_A$ and  $f \circ g = id_B$. In this context, $g$ is the inverse morphism of $f$ and itself an isomorphism as well. In terms of elements when we are in Set: we take an element $a \in A$, apply $f$ to map it to an element $b \in B$, then apply $g$ to map $b$ back to an element $a' \in A$, then $a' = a$, i.e. we're back to where we started so the whole roundtrip from set $A$ to set $B$ and back to $A$ reduces to an identity operation in $A$. Stated without mentioning elements and therefore generalizable: Applying $f$ first, then $g$ yields a composed morphism that equals the identity in $A$ and applying $g$ first, then $f$ yields the identity in $B$. Note how at no point are we talking about the internal structure of the objects like we did when referencing set elements in the definition of bijective functions. The categorical definition of an isomorphism is not even focusing on the objects at all but it's all about the morphisms. The objects are just the backdrop and our main attention goes to the morphisms. We are indeed looking at things from a higher level than in set theoretical descriptions where we looked \emph{inside} the details of the objects. This is typical of category theoretical definitions and theorems. Likewise, a monomorphism $f: A \rightarrow B$ is characterized by the property that for all morphisms $g,h: C \rightarrow A$, we have that $f \circ g = f \circ h$ implies $g = h$. Monomorphisms generalize the idea of an injective (i.e. left-unique) function. An epimorphism $f: A \rightarrow B$ is defined by the property that for all morphisms $g,h: B \rightarrow C$, we have that $g \circ f = h \circ f$ implies $g = h$. Epimorphisms generalize the idea of a surjective (i.e. right-total) function. [VERIFY] If you don't immediately see why these definitions indeed capture and generalize the ideas of injectivity and surjectivity (I certainly didn't), try reversing the implications. For injectivity/monomorphisms, rewrite $(f \circ g = f \circ h) \Rightarrow (g = h)$ as the equivalent statement $(g \neq h) \Rightarrow (f \circ g \neq f \circ h)$ and draw some picture where $g \neq h$. Then do something similar for surjectivity/epimorphisms. Then pat yourself on the back for having grasped a rather abstract and obscure categorical definition. [TODO: maybe add a figure that shows this].
%pg 482 in Ehrig et al

% endomorphism, automorphism

%ToDo: 
%-explain the categorical analogs of injective and surjective functions (mono- and epimorphisms)
% -explain why the definitions of mono- and epimorphisms work, i.e. capture the desired idea
% -reverse the implications: g != h  ->  g°f != h°f etc. and draw pictures where g != h and show how that implies the RHS
%-give other examples of categories: FinSet, Grp, Vect, deductive systems, Graph
%-explain subcategories in this context - many other examples actually are subcategories of Set
%-examplin categorical products and coproducts
%-explain functors, natural transformations

% https://en.wikipedia.org/wiki/Category_theory
% https://plato.stanford.edu/entries/category-theory/#Exam

% https://www.youtube.com/watch?v=SmXB2K_5lcA  Category Theory for Programmers: Chapter 1 - Category

% https://www.youtube.com/watch?v=1TvNeFLGMrE  Abstrakter Unsinn? Was ist Kategorientheorie?

% https://math.jhu.edu/~eriehl/context.pdf

% https://texample.net/tikz/examples/tag/diagrams/
% https://texample.net/tikz/examples/labeled-chain/

\medskip 
Of course, there's much more to say about category theory but this very brief overview shall suffice for a book that attempts to focus on applied math.