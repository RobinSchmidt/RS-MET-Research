

\subsection{Set Theory}
Set theory is often said to be the foundation of all mathematics - even much more fundamental than the natural numbers. 
%In fact, it is possible to "construct" the natural numbers from sets. We will not go down this road though, since this is not really relevant in applied math. 
The idea of a set was initially introduced by Georg Cantor in an intuitive way. His way of establishing set theory later turned out to have some flaws which is why it was later rebuilt more formally. The result of this rebuild is called "axiomatic set theory" and is very abstract and formal. Fortunately, Cantor's view, which is today sometimes called "naive set theory", is good enough for us - at least for the time being. 

\subsubsection{Sets}
In Cantor's definition "A set is a gathering together into a whole of definite, distinct objects of our perception or of our thought which are called elements of the set.". So, in essence, a set is just a bunch of things. A very general concept indeed. Sets are usually denoted in curly braces. For example, the set of the 3 letters $a,b,c$ would be denoted as $\{a,b,c\}$. Two sets are considered equal, if and only if they contain the same elements. It does not matter in which order the elements are written down or if an element appears multiple times. So that means, for example, the sets $\{c,a,b\}$ or $\{a,c,a,a,b,c\}$ are in fact equal to the set $\{a,b,c\}$. By the way, the phrase "if and only if" appears sufficiently often in math texts that some authors use the abbreviation "iff" for that - yes, that's an "if" with a double-f. I may sometimes use that, too. Sets can be given names. For example, we may call our set above $S$ and we may write this as $S = \{a,b,c\}$. Element membership is denoted by an $\in$ symbol, so to express the fact that $b$ is an element of the set $S$, we would write $b \in S$. If we want to express that a certain object, say $d$, is not an element of a set $S$, we write this as $d \notin S$.

%\medskip 
\paragraph{Sets of Sets}
Sets can have other sets as elements and that nesting capability can be used recursively to build arbitrarily complex structures purely from sets. These structures also include the number systems that are used in math. For example, the number zero can be represented by the empty set: $0 = \{\}$, which is also denoted by $\emptyset$, the number one by the set that contains the empty set (i.e. zero): $1 = \{ 0 \} =  \{ \emptyset \}$, the number two by the set that contains zero and one: $2 = \{ 0, 1 \} = \{ \emptyset, \{ \emptyset \} \}$ and so on. Of course, that's super tedious and nobody actually thinks about numbers this way - but in principle, it can be done. Note that in this context $\emptyset$ and $\{ \emptyset \}$ are different things. The first is the empty set and the second is the set that contains the empty set. The nesting matters. One is an empty box, the other one is a box that contains something: namely, an empty box. If you really go down to the very lowest levels of math, then sets are actually the \emph{only} things that can occur as elements of (other) sets because sets are really the only thing that exists in this world. There is nothing else but sets and all the rich and complex mathematical structures that exist can, in principle, be built recursively from these sets. 
%[VERIFY!]

%\medskip 
\paragraph{Set Builder Notation}
In math, the sets we are dealing with are often sets of numbers and they may have many or even infinitely many elements. To denote very large or infinite sets compactly there are notations based on predicate logic. For example to denote the set of all numbers larger than 100 but less than 1000, we may write $\{x : x > 100 \wedge x < 1000\}$. But now we are getting ahead of ourselves. To understand that notation, we actually first have to understand what $>$ and $<$ means. I'm pretty sure, you already do know what they mean, namely, \emph{less than} and \emph{greater than} but in the context of set theory, these symbols first need to be defined, too. To do so, we need to first define what an order relation is. We'll look at these soon. Generally, this kind of notation to define a set is called set builder notation and specifies a set by listing some properties that elements of the set must satisfy. In this notation, we use a placeholder or variable, here $x$, which stands in for some generic element of the set, then use a colon (or, alternatively a vertical bar) and then list the properties that this element should satisfy. So, a set defined by set builder notation generally look like: $\{x : P(x)\}$ or  $\{x \; | \; P(x)\}$ where $P(x)$ is some property or predicate that can be expressed using the machinery of predicate logic. [TODO: maybe use $\varphi$ instead of $P$]

% Maybe use 

% Soo - which of the two notations should we use in this book? The vertical bar looks more aesthetic but I have used the colon already in a few places and its also easier to typeset because we don't nee the spaces \; ..hmmmm

%https://en.wikipedia.org/wiki/Set-builder_notation

\subsubsection{Tuples}
Sometimes, we may want to model situations in which the order of entries actually does matter. Sets are per definition not suitable for this (at least not directly), so we need something else. That other thing is the tuple. A tuple is typically denoted by listing the entries in parentheses. The 3-tuple $(a,b,c)$ is not equal to $(c,a,b)$. Tuples with two elements are also called ordered pairs, 3-tuples triples, 4-tuples quadruples and 5-tuples quintuples. For brevity, in the following, I will just say "pair" when I mean "ordered pair", i.e. pairs are implicitly always assumed to be ordered. When we talk about a tuple, the entries are no longer called "elements" but instead \emph{coordinates} or \emph{components}. If you really want to be puristic and build \emph{everything} from sets, you need to use sets to model tuples, too. You could just pack the tuple members into another set with another object that serves as index, so $(a,b,c)$ would become $\{\{a,1\},\{b,2\},\{c,3\}\}$ and $(c,a,b)$ would become  $\{\{c,1\},\{a,2\},\{c,3\}\}$. These sets of sets would indeed be uniquely identified purely by their elements regardless of order because the correct order could be reconstructed due to the second element in the inner sets which serve as tags. While this definition, proposed by Hausdorff, is intuitive, it has a problem: We would need to require that our set of indices is disjoint from all the sets that make up the tuple's coordinates. Otherwise, we could not distinguish between $(1,2)$ and $(2,1)$, for example. There are other ways to model tuples purely via sets that avoid this problem but are less intuitive. The currently accepted definition is due to Kuratowski and defines the ordered pair $(a,b) = \{ \{a\}, \{a,b\} \}$. From ordered pairs, bigger tuples can be defined recursively. For example, a triple can be defined as $(a,b,c) = ((a,b),c)$. I won't expand this any further because it gets messy really quick. Nobody really thinks about tuples this way anyway. For set-theorists who work at the very foundations of mathematics, the point of this is to convince themselves once and for all that it is possible to express tuples via sets and from then on, use the higher level tuple notation - just like scientists and engineers do. The idea of forming tuples is important enough to have a special set operation with its own infix operator symbol associated with it. We will say more about this on page \pageref{Par:SetAlgebra}, where we will introduce the \emph{cartesion product} of two sets.

% = (\{\{a\}, \{a,b\} \}, \; c)= \{ \{\{a\}, \{a,b\} \}, \; \{ \{\{a\}, \{a,b\},c \} \}\}$.

% https://en.wikipedia.org/wiki/Ordered_pair#Defining_the_ordered_pair_using_set_theory

%https://en.wikipedia.org/wiki/Tuple

% What about sequences? are they yet another way to specify a collection of objects with order? I think, the difference is that sequences are, by default, infinitely long and if we want a finite sequence, we just extend it with zeros. A tuple has always a fixed number of elements.

\subsubsection{Multisets}
In a set, neither the order of the elements nor their multiplicity matters where by "multiplicity" we mean the number of times, an element occurs. For example: $\{ 1,2,2 \} = \{ 2,1,2 \} = \{ 1,2 \}$ when we assume the collections to be sets. In a tuple, both of these things matter: $(1,2,2) \neq (2,1,2) \neq (1,2)$. There is an intermediate situation where we may want to distinguish collections based on multiplicity but not based on the exact position of occurrence of an item. A structure suited for that purpose is the so called multiset. Like in a set, it is immaterial at which position we list an item - but it does matter whether an item occurs once or twice or whatever other number of times. For multisets, we would have: $\{ 1,2,2 \} = \{ 2,1,2 \} \neq \{ 1,2 \}$. Multisets are mostly denoted just like sets with curly braces and it must be inferred from the context that multisets are meant. I have also seen a notation using square-brackets to distinguish multisets from sets but that notation doesn't seem to be standard and not even widespread.

% maybe move this section before the tuples and edit the text accordingly - should not reference tuples - and the tuples-text may reference multisets - but nah - multi sets are less common, so it's appropriate to hav ethem at the bottom

% maybe explain how multisets can be modeled via sets.

% https://en.wikipedia.org/wiki/Multiset
% https://brilliant.org/wiki/multiset/#_=_
% https://www.statisticshowto.com/multiset/
% https://mathworld.wolfram.com/Multiset.html

% What operations do we have on multisets? There is the multiset sum explained in ACRS, pg 29. we just add the multiplicities where it is understood that objects which aren't in a multiset have a multplicity of zero. This is kinda like the set union for multisets. What about intersection? Maybe taking the minimum of the respective multiplicities of the elements could make sense?

% Applicaitons: Prime factorization, roots of polynomials, multiple traversals of a curve in algebraic geometry, e.g. the polynomial euqation  (x^2 + y^2 - 1)^n = 0  contains the unit circle n times (although there is no notion of "traversal" - I think, this idea comes in only when parameterizing the curve)


%---------------------------------------------------------------------------------------------------
\subsubsection{Relations}
A relation can formally be defined to be a set of tuples. Of particular importance are binary relations, i.e. sets of 2-tuples, aka pairs. As an example, consider the two sets $A = \{1,2,5\}, B = \{0,2,4\}$. We can define a relation $R$ between the sets $A$ and $B$ as a set of pairs where the first entry comes from $A$ and the second from $B$, for example: $R = \{(1,2),(1,4),(2,4)\}$. Such a relation can be visualized pictorially by drawing the two sets side by side and drawing an arrow between each pair of elements that is in the relation. This is shown for our relation $R$ in figure \ref{Fig:RelationLessThan_123_024}.
% BUG: This reference just says "figure I" in the rendered text. Maybe we need a caption for the figure
% ...OK...done

\begin{figure}[h]
\label{Fig:RelationLessThan_123_024}
\caption{Pictorial visualization of a relation}
\centering
	
\begin{tikzpicture}
[mydot/.style={circle, fill=lightgray, inner sep=5pt}, >=latex, shorten >= 1pt, shorten <= 1pt]

% Left set:
\node[mydot,                    label={center:1}] (a1) {}; % 1
\node[mydot, below=0.3cm of a1, label={center:2}] (a2) {}; % 2
\node[mydot, below=0.3cm of a2, label={center:5}] (a3) {}; % 5
	
% Right set:	
\node[mydot, right=1.5cm of a1, label={center:0}] (b1) {}; % 0
\node[mydot, below=0.3cm of b1, label={center:2}] (b2) {}; % 2
\node[mydot, below=0.3cm of b2, label={center:4}] (b3) {}; % 4

% Arrows for the relations:
\path[->] (a1) edge (b2);                                  % 1 -> 2
\path[->] (a1) edge (b3);                                  % 1 -> 4
\path[->] (a2) edge (b3);                                  % 2 -> 4

% Ellipses around the sets:
\draw (0.0,-0.75) ellipse (0.5cm and 1.5cm);
\draw (2.0,-0.75) ellipse (0.5cm and 1.5cm);

\end{tikzpicture}
\end{figure}
% https://tikz.dev/tikz-transformations
% see:
% https://tex.stackexchange.com/questions/157450/producing-a-diagram-showing-relations-between-sets
% https://latex.org/forum/viewtopic.php?t=21987
% It's actually the "<"  relation - maybe mention it in the section about orders...but that doesn't really fit well because in orders domain and codomain are the same
There are a couple of important features that such a relation may or may not have. In a \emph{left-total} relation, every element in the left set has an outgoing arrow emanating from it. In a \emph{right-total} relation, every element in the right set has an incoming arrow. In a \emph{right-unique} relation, every element in the left set has at most one outgoing arrow. The naming convention reflects the idea that when you pick one element from the left set, the related  element of the right set, if any, is uniquely determined. We always know, where we have to go. Likewise, in a \emph{left-unique} relation, every element of the right set has at most one incoming arrow. For every element from the right set, the related element from the left set, if any, is uniquely determined. We always know, where we came from. As we can see, our example relation $R$ actually has none of these features.

\medskip
Notationally, we may write $(a,b) \in R$ when we want to express the fact that $a$ is related to $b$ by the relation $R$. Some authors write this in the shorthand infix notation $a R b$ but that looks kinda ugly. For most relations, we'll use special symbols such as $<, \leq, =, \ldots$ such that the infix notation will look nice. 

\medskip
Our example relation above was an example for a general case where we have a relation between elements of a set $A$ and elements of a set $B$. We say that $R$ is a \emph{relation between} the sets $A$ and $B$. An important special case of that is when $B$ is actually the same set as $A$. We then say that we have a \emph{relation on} the set $A$. For a relation on a set $A$, there are a couple of more important named features that the relation may or may not have. In the following table, we use the generic infix symbol $\sim$ for our relation and $\nsim$ to indicate that the elements are not in the relation. 

\medskip
\begin{tabular}{c l}
\label{Tab:RelationFeatures}
  $a \sim a$                                       & Reflexive     \\
  $a \nsim a$                                      & Irreflexive   \\
  $a \sim b \Rightarrow b \sim a$                  & Symmetric     \\
  $a \sim b \Rightarrow b \nsim a$                 & Asymmetric    \\
  $a \sim b \wedge b \sim a \Rightarrow a = b $    & Antisymmetric \\
  $a \sim b \wedge b \sim c \Rightarrow a \sim c$  & Transitive    
\end{tabular}
\medskip

When a relation is \emph{reflexive}, it means that every element $a$ must be in relation with itself. In \emph{irreflexive} relations, no element is allowed to be in relation with itself. If a relation is \emph{symmetric}, then if the tuple $(a,b)$ is in the relation, then the tuple $(b,a)$ must also be in it. If the relation is \emph{asymmetric}, such a symmetry condition is not allowed for any pair $a,b$. In an \emph{antisymmetric} relation, the symmetry condition is only allowed in the special case of $a = b$. That means, counterintuitively, that asymmetry is actually a stronger requirement than antisymmetry. Every asymmetric relation is antisymmetric but not vice versa. Asymmetry also implies irreflexivity, by the way. Finally, transitive means that if the tuples $(a,b)$ and $(b,c)$ are in the relation, then the tuple $(a,c)$ must also be in it. 
...TBC...VERIFY
% what about anti-reflexive?
% https://en.wikipedia.org/wiki/Reflexive_relation
% https://en.wikipedia.org/wiki/Intransitivity#Antitransitivity

% https://math.stackexchange.com/questions/778164/is-an-anti-symmetric-and-asymmetric-relation-the-same-are-irreflexive-and-anti


%The relation must be \emph{reflexive} meaning that every element $a$ must be in relation with itself. They must also be \emph{symmetric} meaning that if the tuple $(a,b)$ is in the relation, then the tuple $(b,a)$ must also be in it. Finally, they must be \emph{transitive} meaning that if the tuples $(a,b)$ and $(b,c)$ are in the relation, then the tuple $(a,c)$ must also be in it. Reflexivity just means that every object is equivalent to itself. 

% https://de.wikipedia.org/wiki/Asymmetrische_Relation
% https://de.wikipedia.org/wiki/Antisymmetrische_Relation
% https://de.wikipedia.org/wiki/Transitive_Relation
% https://de.wikipedia.org/wiki/Reflexive_Relation

% explain left/right totality, left/right uniqueness - see Bill Shillito's course on youtube
% use as example A = {1,2,3,4}, B = {0,2,4} and the < relation. R = {(1,2),(1,4),(2,4),(3,4)}
% not left-total due to (4,X) missing
% not right-total due to (X,0) missing
% not left-unique due to (1,4),(2,4),(3,4)
% not right-unique due to (1,2),(1,4)
% explain inverse relations
% reflexive/irreflexive, symmetric/asymmetric/antisymmetric
%   https://www.youtube.com/watch?v=GvNGf9Gki7o 
% transitive
%   https://www.youtube.com/watch?v=O19RpfoxQpA
%   make table like at the end of this video - but verify the definitions - the asymmetry
%   definition looks fishy. Maybe antimsymmtery is also "wrongly" defined there?
%   draw diagrams

% https://byjus.com/maths/relations-and-its-types/
% https://www.toppr.com/guides/maths/relations-and-functions/types-of-relations/

% https://www.youtube.com/watch?v=Tk5_B7w5fiY&list=PLZzHxk_TPOStgPtqRZ6KzmkUQBQ8TSWVX&index=9

% Was sind angeordnete Körper? (Ordnungstheorie)
% https://www.youtube.com/watch?v=RyhUIoif_B8


\paragraph{Functions} A \emph{function} is a special kind of relation, namely a relation which is left-total and right-unique. You can think of functions as a definition of a map from a set $A$ to another set $B$. Each element of $A$ gets mapped to some unique element of $B$. You can throw any $a \in A$ at a function $f$ and it will unambiguously tell you, which $b \in B$ your given $a$ is mapped to. If a function is additionally left-unique, we can actually traverse the arrow back to figure out unambiguously, where we came from. We can undo the application of the functional mapping for those $b \in B$ which have an incoming arrow. Such left-unique functions are also called \emph{injective}. If, on the other hand, the function is right-total, i.e. every element of $B$ has an incoming arrow, we call the function \emph{surjective}. A function that is both injective and surjective is called \emph{bijective}. Such bijective functions can be inverted, i.e. undone, for any $b \in B$ and are therefore also called \emph{invertible}.

% ToDo:
% -explain domain, range, image, pre-image (of elements and whole sets)
% -give the condtions for functions in math notation
% -explain creation of binary functions
% -explain notation f: A -> B
% -explain partial functions - need not to be left-total
% -exaplain multifunctions - need not to be right-unique

\paragraph{Binary Operators} We sometimes need functions $f$ that map pairs of the form $(x,y)$ where $x$ and $y$ come from a set $A$ to another value $c$ also from the set $A$. Such a function is in certain contexts called \emph{binary operator} or \emph{binary operation} on the set $A$. Our usual arithmetic operators for addition and multiplication are of that kind. We already have all the tools to create such functions in our toolbag. We know how to create the set of tuples and we know how to create functions. Combining these two ideas, we have all the tools we need to create binary operations.

%, although the process of the creation of the set of tuples

\paragraph{Equivalences} An \emph{equivalence relation}, or in short just \emph{equivalence}, is another special kind of relation. In equivalence relations, the left and right set are actually the same set so these are relations \emph{on} a set and equivalence relations must be reflexive, symmetric and transitive. Reflexivity simply means that every element must be equivalent to itself. The symmetry condition captures the idea that if $a$ is equivalent to $b$ then $b$ is also equivalent to $a$. Transitivity captures the idea that if $a$ is equivalent to $b$ and $b$ is equivalent to $c$, then $a$ is also equivalent to $c$. The prototype of an equivalence relation is the usual equality denoted by $=$ but the concept of an equivalence is more general. It is meant to capture the idea that two objects are interchangeable within a given context. They do not necessarily have to be the exact same object but certainly can be - this is ensured by the reflexivity. For some equivalence relations, we may actually use the $=$ symbol even though another relation may be meant. Sometimes alternatives like $\sim, \equiv, \simeq, \cong,$ etc. are used. The notation for various equivalence relations may vary from field to field and from author to author. 

% ToDo: Move the explanations of transitive, reflexive, etc. under the tabular

% https://www.youtube.com/watch?v=Ogm711KWwaw
% at around 14:00 The identity is the smallest equivalence relation and AxA (the "all-relation"? "universal relation" is the largest)
% https://www.ask-math.com/universal-relation.html

% https://en.wikipedia.org/wiki/Equivalence_relation
% https://mathworld.wolfram.com/EquivalenceRelation.html

\paragraph{Orders} \label{Par:Orders} An \emph{order relation}, in short just \emph{order}, is yet another special kind of relation. An order is also a relation defined on one set, i.e. the left and right sets are the same. If a set $A$ is equipped with such an order relation, the set is said to be \emph{ordered}. An order relation is typically denoted by the symbol $\leq$ which we read as "less or equal" and $a \leq b$ reads as: "$a$ is less than or equal to $b$". An order relation must be reflexive, antisymmetric and transitive. Such an order can be \emph{partial} or \emph{total}, depending whether or not every pair $a,b$ can be compared via the order. In a total order, for every pair $a,b$, we always have $a \leq b$ or $b \leq a$. For a partial order, this is not necessarily the case - there may be pairs $a,b$ where neither $a \leq b$ nor $b \leq a$ is true. Total orders are also called \emph{linear orders}. A related concept is a \emph{strict order} which we typically denote by the symbol $<$ which we read as "less" and $a < b$ reads as: "$a$ is less than $b$". Such a strict order is characterized by being irreflexive, asymmetric and transitive. Note that, counterintuitively, a strict order is \emph{not} a special kind of order in the sense defined above. We required an order to be reflexive and here we require irreflexivity. These are incompatible requirements. A strict order is a different concept. Strict orders can also be partial or total. Another way to characterize a strict total order is by requiring transitivity and \emph{trichotomy}. The latter means that for every pair $a,b$, exactly one of the three conditions must be true: (1) $a < b$, (2) $b < a$, (3) $a = b$. To convert between an order $\leq$ and a corresponding strict order $<$, we define $a \leq b$ to mean $(a < b) \vee (a = b)$ and we define $a < b$ to mean $(a \leq b) \wedge \neg (a = b)$
 ...TBC...verify...explain Hasse diagram, chain
 
% https://www.youtube.com/watch?v=W8J4eEiAtIk

% https://mathworld.wolfram.com/PartialOrder.html
% https://mathworld.wolfram.com/TotalOrder.html

% https://en.wikipedia.org/wiki/Order_theory#Constructing_new_orders
% https://de.wikipedia.org/wiki/Ordnungsrelation#Totalordnung
%https://mathworld.wolfram.com/StrictOrder.html

% https://math.stackexchange.com/questions/2210560/orders-partial-orders-strict-partial-orders-total-orders-strict-total-orders

% -Strict total order: has transitivity and trichotomy, is not a "total order" because it's not
%  reflexive (this terminology is confusing!)
% -trichotomy
% -transitivity
% -it can be inverted into >
% -we have also "less than or equal to", "greater than or equal to"
% -partial and total orders 
% -well ordering - well ordered sets have a least element
% -compatibility with operations: if $a < b$ then $a + c < b + c$ for any $c \in A$

% https://en.wikipedia.org/wiki/Order_theory
% https://en.wikipedia.org/wiki/Total_order
% https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_order
% https://en.wikipedia.org/wiki/Ordered_field
% https://de.wikipedia.org/wiki/Ordnungsrelation

% https://math.libretexts.org/Bookshelves/Applied_Mathematics/Seven_Sketches_in_Compositionality%3A_An_Invitation_to_Applied_Category_Theory_(Fong_and_Spivak)/01%3A_Generative_Effects_-_Orders_and_Adjunctions/1.01%3A_What_is_Order

\medskip
\medskip
We'll now shift our perspective a little bit. In the previous paragraphs, we defined certain \emph{types} of relations in terms of what properties they need to have to qualify for the given type name. The relations themselves were not specified. There may be many of them and their details may depend on the contexts. We have looked at the relations from the perspective of putting the pairs $(a,b)$ of \emph{elements} of two sets $A$ and $B$ into the relation or not. 

\paragraph{The Subset Relation} 
What we will do now is to consider the pair $(A,B)$ itself and instead of defining properties of relations, we will explicitly define some very specific relations once and for all in which any such pair of sets may or may not be. We also do not really care where $A$ and $B$ came from - whether or not they are elements from some embedding sets is not of interest here. Instead, what we want to know is if the set $B$ contains all the elements that the other set $A$ contains. In such a case, we call $A$ a \emph{subset} of $B$ and we call $B$ a \emph{superset} of $A$. The set $B$ may or may not contain more elements, i.e. elements that are not in $A$. If the superset $B$ does indeed contain additional elements, then we call $A$ a \emph{strict subset} of $B$ and we call $B$ a \emph{strict superset} of $A$. These relations are expressed with the notation $\subseteq, \subset, \supset, \supseteq$ as follows:
\begin{eqnarray}
A \subseteq B \;\;  \Leftrightarrow \;\;   x \in A \Rightarrow x \in B&           \qquad & \text{$A$ is subset of $B$} \\
A \supseteq B \;\;  \Leftrightarrow \;\;   x \in B \Rightarrow x \in A&           \qquad & \text{$A$ is superset of $B$} \\
A \subset   B \;\;  \Leftrightarrow \;\;   x \in A \Rightarrow x \in B,& A \neq B \qquad & \text{$A$ is strict subset of $B$} \\
A \supset   B \;\;  \Leftrightarrow \;\;   x \in B \Rightarrow x \in A,& A \neq B \qquad & \text{$A$ is strict superset of $B$}
\end{eqnarray}
% Alignment is ugly! Maybe use a tabular environment
In contrast to orders, equivalences, functions and so on, the subset relation is an elementary set theoretical idea and not context dependent. If it is applied to a given set of sets, it is a partial order, by the way [VERIFY].

\paragraph{Disjointness}
Two sets $A$ and $B$ are said to be disjoint when they have no elements in common. That means formally $(x \in A \Rightarrow x \notin B) \wedge (x \in B \Rightarrow x \notin A)$. Like the subset relation, disjointness is another elementary set theoretical relation that is defined once and for all. It is not usually introduced in the context of relations, though. There is no special symbol to indicate that two sets are disjoint either\footnote{One could perhaps contemplate to use the perpendicular symbol $\perp$ for that purpose, but that is not part of standard notation}. One short way to symbolically convey that two sets $A,B$ are disjoint is via the equation $A \cap B = \emptyset$. That means that the \emph{intersection} between $A$ and $B$, i.e. the set of elements that $A$ and $B$ have in common, is the empty set. If we have a family of sets $F = A_1, A_2, A_3, \ldots$, we say that the $A_i$ are \emph{mutually disjoint} or \emph{pairwise disjoint} when $\forall i,j: i \neq j \Rightarrow A_i \cap A_j = \emptyset$. That is: every possible pair $A_i, A_j$ of sets that we can pick from $F$ is disjoint, with the sole exception to pick the same set twice.

%  Two sets are called \emph{disjoint} when they have no elements in common.
% https://en.wikipedia.org/wiki/Disjoint_sets
% https://testbook.com/maths/disjoint-set

% -exlpain mutual disjointness of a set of sets. Sets of sets are sometimes called systems
%  https://en.wikipedia.org/wiki/Family_of_sets
% -it implies that theri intersection is empty
% -is there a symbol for it?

\paragraph{The Element Relation} 
Take again two sets $A$ and $B$, not necessarily distinct and let $a \in A$ and $b \in B$. If the elements of $A$ and $B$ are themselves sets, and in modern set theory without atomic urelements, there's nothing else they could possibly be, then $a,A,b,B$ are all sets. That means we can ask questions like "is $A$ an element of $b$", that is $A \in b$. Or we could ask for whether or not $A \in B$ or $B \in A$ or $B \in a$, etc. We could even ask if $A \in A$ or $A \in a$ or $a \in a$. The answers to these latter questions will be "no", though because such recursive element inclusions are forbidden in axiomatic set theory - but it does make sense to ask the question. That is, our $\in$ symbol also defines a relation. The concepts in set theory begin to feel somewhat recursively self-referential by now and it is indeed the case that the whole imposing edifice of set theory somehow manages to bootstrap itself out of (almost) nothing.





\paragraph{Relational vs Operational Interpretation}
We defined a function to be a special kind of relation (left-total, right-unique) and we also intuitively characterized a function as a sort of operation with an input and an output. The operational viewpoint is the way, we usually think of functions: we put some object in and get some object out. It is important to realize that such an "operational" point of view is not necessary. From a set-theoretic point of view, the "relational" point of view is all there is: a function is just a special kind of relation and a relation is just a special kind of set consisting of ordered pairs and ordered pairs can also be thought of a special kinds of sets. The definition really says nothing at all about a function being an input/output "operation". This is merely our interpretation. Or maybe it's more appropriate to say, that an input/output device is something that we want to \emph{model} with the concept of a function. At the core, it all just boils down to specific sorts of sets, because in set theory, sets are really the only thing that we have to work with anyway.

\medskip
A similar consideration can be applied to equivalences. By definition, they are also just a special kind of relation. When we use equivalences, for example to simplify equations, we sometimes interpret an equivalence as a possible replacement rule that can be applied to (parts of) a formula, i.e. a sort of operation. That's not what an equivalence is, at its core, though. It's a common misconception to think of the equals sign $=$ as a prescription to do something like an assignment or replacement when in reality, it just expresses the fact that two things are equal. An equation like $E = m c^2$ can indeed be used to compute $E$ when $m$ and $c$ are known but $E$ is yet unknown and that's indeed how we often use equations: to compute an unknown value from known ones. But the equation in and of itself, at its core and by its nature, is not an algorithmic prescription for a computation. It's just a relation.

\medskip
As we see, relations are quite flexible tools and can be used to model several important concepts in mathematics by imposing some additional requirements on the broad and general concept of a relation.
% ToDo: mention some other kind special kinds/classes of relation
% -binary operations: (A x B) -> C: combine the ordered-pair/set-product mechanism do create
%  the domain D = A x B, then use the function creation mechanism to create the desired subset
%  of (D x C), the set-product between domain and codomain

%there's not really an intrinsic input/output interpretation
%-explain the relational and operational aspects of 
% -functions (is-related vs input-output) 
% -equivalences (is-equivalent-comparisons vs assigments in programming and simplifications in math)
% -subset relations (is-a-subset vs form/extract-a-subset)
% -explain bi- and multivariate functions (using a set-product as input), 
% -functions of a scalar that yield vector outputs (parametric curves) using a set-product as output
% -explain multifunctions such as the n-th root of a complex number
% -explain the notation f: A -> B
% -explain how the interpretations of two-input/one-output functions: A x B -> C and one-input/two-output function A -> B x C can be though of as ternary relations, i.e. subsets of A x B x C where formally A -> B x C is *interpreted* as A x (B x C) and A x B -> C as (A x B) x C but the interpretation can be "flattened out" and we can do partial application, currying, etc.

% what about relational algebra? useful for databases, I think

% https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/
%Cardinality

% Union set: A bing cup/union symbol U X means the union of all elements of X. We assume here that each element of X is itself a set - which in set theory is true because there are no other things than sets anyway

% https://www.youtube.com/watch?v=szfsGJ_PGQ0
% The Axiom of Choice

% https://www.youtube.com/watch?v=szfsGJ_PGQ0
% 26:00 - well ordering theorem - explain connection to statement that complex numbers can't be ordered - this seems to be a contradiction - but "being ordered" for complex numbers requires more - the order must be compatible with the arithemtic operations - this is a difference of meaning which is confusing and should be pointed out.

%---------------------------------------------------------------------------------------------------
\subsubsection{Set Operations}

\paragraph{Set Algebra} \label{Par:SetAlgebra} Just like we could combine logical propositions via the connectives $\wedge, \vee, \neg, \ldots$ to yield new propositions, there are operations that we can perform on sets to yield new sets. The \emph{intersection} of two sets $A, B$ is denoted as $A \cap B$ and defined to be the set with the elements that are present in $A$ and in $B$, i.e. the set of elements that $A$ and $B$ have in common. If the intersection between $A$ and $B$ is empty, i.e. the two sets have nothing in common, formally denoted as $A \cap B = \emptyset$, we say that $A$ and $B$ are \emph{disjoint}. The visual resemblence of $\cap$ and $\wedge$ is, of course, no coincidence. The \emph{union} of two sets $A,B$ is the set of all elements that are in $A$ or in $B$ where the "or" is again to be understood as an inclusive or. Imagine throwing all contents of sets $A$ and $B$ together into a bucket. You may get doublings for the common elements (i.e. the intersection) but that doesn't matter anyway because set membership does not care about potential multiplicities - but if you want, you can imagine to remove the doublings after throwing the sets together. The notation for the union of $A$ and $B$ is: $A \cup B$. The symbol looks a bit like a cup and resembles the "or" symbol $\vee$ from logic - again no coincidence. There is also a notion of a set \emph{difference} $A \setminus B$ which is the set $A$ minus those elements of $A$ which are also in $B$. You may read this as "$A$ take away $B$". It's also called the \emph{relative complement} of $B$ with respect to $A$. If we assume that we have some sort of universal set, i.e. the set of all things that we could possibly consider in the current context, we may also define the \emph{complement} of a set $A$, denoted as $\overline{A}$, which is the set of "everything" except the elements of $A$. Here, "everything" refers to our universal set which may depend on the context. If we call this universal set $U$, we could define $\overline{A} = U \setminus A$. The so called \emph{cartesian product} or \emph{set product} or \emph{cross product} or just \emph{product} of two sets $A$ and $B$, denoted as $A \times B$, is the set of all possible pairs $(a,b)$ where $a$ is an element of $A$ and $b$ is an element of $B$. Finally, the \emph{power set} operation $\mathcal{P}(A)$ takes the set of all subsets of a given set $A$. Here is a summary of these basic set operations:
\begin{eqnarray}
 \overline{A}  =& \{x: \; x \notin A \}                    \qquad &\text{complement} \\	
 A \cap B      =& \{x: \; x \in A \wedge x \in    B \}     \qquad &\text{intersection} \\
 A \cup B      =& \{x: \; x \in A \vee   x \in    B \}     \qquad &\text{union} \\
 A \setminus B =& \{x: \; x \in A \wedge x \notin B \}     \qquad &\text{difference} \\
 A \times B    =& \{(x,y): \; x \in A \wedge y \in    B \} \qquad &\text{product} \\
 \mathcal{P}(A) 
        = 2^A = &  \{x : \; x \subseteq A \}               \qquad &\text{power set}
\end{eqnarray}
The product may be iterated in the following way: Form the product $A \times B$ and then take the result of that and form the product with a third set $C$ to get: $(A \times B) \times C$. What we formally get would be a set of pairs where the first element is itself a pair of elements from $A$ and $B$ and the second element is an element from $C$. Elements of $(A \times B) \times C$ would look like $((a,b),c)$ where $a \in A, b \in B, c \in C$. On the other hand, elements of $A \times (B \times C)$ would be of the form $(a, (b,c))$. Formally, this is a different set, so our set product is formally not associative. However, the set $(A \times B) \times C$ is isomorphic (i.e. of the "same form") to $A \times (B \times C)$ and in many practical applications, what we actually want to form is not a set of nested pairs but rather a set of triples $(a,b,c)$ with no further inner structuring. We will adopt the convention that when we write a set product with multiple factors without any parentheses like: $A \times B \times C$, we mean the set of triples $(a,b,c)$ where $a \in A, b \in B, c \in C$. And this, of course, generalizes to quadruples, quintuples, etc. We will also use the notation $A^n$ to mean a set of $n$-tuples in which each element is from $A$.

\medskip
The power set $\mathcal{P}(A)$ may alternatively be denoted by $2^A$ where the latter notation is motivated by the observation that, if the set $A$ has $n$ elements, then the power set will have $2^n$ elements. This also explains the name "power set". The elements $x$ of the power set $2^A$ are themselves sets - namely, subsets of $A$. The empty set does also count as a subset of any nonempty set. If a set has $n$ elements, then the number of subsets with $k$ elements is given by the binomial coefficient "$n$-choose-$k$". [VERIFY, REF needed]. The fact that the power set of a finite set $A$ with $n$ elements has $2^n$ elements can also be understood as follows: For each subset, we may define a function $f$ from $A$ to the set $\{0,1\}$ which maps an element $a$ of $A$ to $1$, if the element $a$ is to be included into the subset and to $0$ if the element $a$ is not to be included. For such a function $f$, there are $2^n$ different possibilities because for each element $a \in A$ (of which there are $n$), we can choose either $0$ or $1$ as the mapped value. Including an element into a subset or not is a binary decision. We can establish a bijection between the subsets of $A$ and the set of possible functions $f: A \rightarrow \{0,1\}$, so these sets must have the same number of elements - namely, both have $2^n$ elements.

\medskip
In addition to these basic and most common set operations, there are a couple of more, less common ones. Some of them are:
\begin{eqnarray}
 A \triangle B =& (A \setminus B) \cup (B \setminus A)      \qquad &\text{symmetric difference} \\
 A \sqcup B    =& A \times \{ 0 \}  \cup  A \times \{ 1 \}  \qquad &\text{disjoint union} \\
 A ^ B         =& \{ f: \; B \rightarrow A  \}              \qquad &\text{functions from $B$ to $A$} 
\end{eqnarray}
The symmetric difference is what remains, if we first form the union of $A$ and $B$ and then subtract the intersection of $A$ and $B$ from that. That is, we also have: $A \triangle B = (A \cup B) \setminus (A \cap B)$. Other possible notations for the symmetric difference are $A \ominus B$ and $A \oplus B$. The disjoint union is an operation in which we throw together the contents of two sets $A,B$ while keeping the elements that came from set $A$ distinguishable from those that came from $B$ by not using the elements of $A$ and $B$ as is but rather using tuples of the form $(a,0)$ and $(b,1)$ where $a \in A, b \in B$. The appended second components serve as tags to indicate from which set the element originally came. The purpose of the disjoint union is that many desirable set theoretic constructions involving a set union work out as desired only for disjoint sets. That's why we first make the sets $A,B$ artificially disjoint via this tuple formation and then take the union of the so created sets of tuples. The disjoint union is sometimes also denoted by a simple plus sign $+$ which makes sense in the context of cardinalities which are explained below. The set of all possible functions from a set $B$ to a set $A$ is denoted by the exponential notation $A^B$. The notation $\{ f: \; B \rightarrow A \}$ is informal and intended to mean that $f$ is a function with domain $B$ and codomain $A$. The exponential notation is used for a good reason: for finite sets $A$ and $B$, the number of possible different functions from $B$ to $A$ is indeed given by $|A|^{|B|}$.

[TODO: quotient set i.e. set of equivalence classes, give a rather comprehensive list of useful set-algebraic equations, define big intersection and union for sets of sets]



% TODO: figure out, if there's a formal notation for the informal $A^B = \{ f: \; B \rightarrow A \}$
% notation. Maybe $A^B = \{ f \in \mathcal{P}(B \times A) : f \text{is function} \}$ as 
% semi-formal expression is ok? Here \mathcal{P}(B \times A) is the set of all possible relations
% between B and A. We then pick only those which are functions.

% ToDo: mention that the complement is sometimes also written as A^C ...with some different font for the C, mention that the complement must always be taken with respect to some universal set which may depend on the context

% https://en.wikipedia.org/wiki/Complement_(set_theory)#Relative_complement
% https://en.wikipedia.org/wiki/Disjoint_union
% https://en.wikipedia.org/wiki/Symmetric_difference
% https://math.stackexchange.com/questions/901735/meaning-of-a-set-in-the-exponent

% https://en.wikipedia.org/wiki/Exponentiation#Sets_as_exponents
% https://encyclopediaofmath.org/wiki/Exponential_law_for_sets
% A^(B x C) = (A^B)^C. This law corresponds to "currying" in functional programming

% https://math.stackexchange.com/questions/1631396/what-is-the-difference-between-disjoint-union-and-union

% Weitz uses a union symbol with a dot above to denote a union of disjoint sets. see:
% https://www.youtube.com/watch?v=RcDjuXLK-Jg&list=PLb0zKSynM2PDUcEEkjv48Y_4N9CBFyzsz&index=7
% at 9:00.  ...don't confuse this with the "disjoint union" operation which artificially makes sets disjoint before froming the union. Maybe explain both notations in the section of set theory 


\paragraph{Quotient Sets and Equivalence Classes} 
Assume we have a set $A$ and an equivalence relation $\sim$ on $A$. That is, we write $a \sim b$ whenever $a$ is equivalent to $b$ according to our equivalence $\sim$ where $a,b \in A$. Our equivalence $\sim$ \emph{partitions} the set $A$ into subsets, the so called \emph{equivalence classes}. These subsets are always mutually disjoint and their union gives us back the original set $A$. Every $a \in A$ belongs to exactly one equivalence class. No element of $A$ is missing and no element occurs more than once. This is what "partitions" means in this context. If $a$ is an element of $A$, then we denote by $[a]_{\sim}$ the set of all elements of $A$ that are equivalent to $a$ with respect to $\sim$. Formally, this means $[a]_{\sim} = \{ b \in A : b \sim a\}$. Mostly $\sim$ is clear from the context and in such cases we may just write $[a]$. The elements of such an equivalence class $[a]$ are called \emph{representatives} of $[a]$. Of course, due to the reflexivity of equivalence relations, $a$ itself is always an element of $[a]$ but any other element of $[a]$ may also serve as representative for $[a]$. They are all considered to be equivalent, after all. It may be tempting to think that $a$ is somehow the most natural and canonical representative for $[a]$, but observe that if $a \sim b$ then $[a] = [b]$ which may suggest that $b$ is just as canonical. That is, at some point, we have already made the arbitrary choice to name the equivalence class $[a]$ rather than $[b]$ and only that arbitrary choice makes $a$ look more natural as representative than $b$. Usually, we will have some means to pick a most natural representative, though. For example, if we have a strict total order on $A$ available, we could pick the smallest element of each equivalence class as representative. The set of all equivalence classes that our equivalence relation $\sim$ produces is called the "\emph{quotient set} of $A$ with respect to $\sim$". It is denoted by $A / \sim$. In a shorter language, we may also call it the "quotient of $A$ by $\sim$" or even shorter "$A$ modulo $\sim$". The surjective map that maps every $a \in A$ to its equivalence class $[a]$ is called the \emph{canonical surjection} or \emph{canonical projection}

%From each equivalence 
%is a subset

% https://en.wikipedia.org/wiki/Equivalence_class

% ~ partitions the set A. That means, it splits it into disjoint subsets that taken together give us back the whole set

% -give as example the modular integers with respect to some modulus m

\paragraph{Cardinality} While we are speaking of the number of elements of a set, i.e. the "size" of a set, it should be noted that this size has been given a special name: \emph{cardinality}. The operation of taking the cardinality of a set $A$ is denoted like taking an absolute value: $|A|$ and the result of that operation is a so called cardinal number\footnote{Cardinal numbers are, of course (you know the drill), also special kinds of sets}. We do not simply call it "size" because there are different notions of set size in mathematics and cardinality is just one of them (another one would be the so called \emph{measure}, for example). In the case of finite sets, the cardinality is just the number of elements, i.e. a natural number. That means, the natural numbers are also cardinal numbers. They are the finite cardinal numbers - but there are infinite cardinal numbers, too. And yes, that means there are different sizes of infinity in math. In a sense that can be made rigorous, there are more real numbers than natural numbers, for example. The cardinal number of the set of real numbers is bigger than that of the set of natural numbers. The notion of cardinality for infinite sets is defined in terms of existence of bijections: If for two infinite sets, you can find a bijective function between those sets, then these sets have, by definition, the same cardinality. This has a couple of counterintuitive consequences, one of which is that a strict subset of an infinite set can have the same cardinality as the whole set. The part is \emph{not} smaller than the whole. For example, the set of natural numbers and the set of even numbers have the same cardinality. The required bijection and its inverse are simply the functions $y = f(x) = 2 x$ and $x = f^{-1}(y) = y / 2$. We'll go deeper into this in a later chapter.

%But this stuff is beyond the scope of this book. [TODO: give equations for cardinalities]

%---------------------------------------------------------------------------------------------------
\subsubsection{Operations on Relations}
Now that we know what relations are and have familiarized ourselves with the common operations that we can perform on sets, we'll now have a look at some operations that we can perform on relations. In principle, since relations are just a specific kind of set, we have all the general set theoretic operations available. Of these, typically only the union, intersection and complement\footnote{If we have relation between two sets $A,B$, the complement is taken with respect to $A \times B$. [VERIFY]} are relevant in practice. There are also a couple of specific operations that are relevant only for relations and not for general sets.

% https://en.wikipedia.org/wiki/Relation_(mathematics)#Operations_on_relations
% https://en.wikipedia.org/wiki/Algebraic_logic#Calculus_of_relations

\paragraph{Converse} The converse relation of a given relation is obtained by just swapping $x$ and $y$ in the pairs $(x,y)$ that make up the relation. For example, if $<$ is the usual less-than relation, then the converse relation would be the greater-than relation $>$. The same is true for $\leq$ and $\geq$. The relations $=$ and $\neq$ are their own converses. 
...TBC...give notations

\paragraph{Restriction}
Given a set $A$ and a relation $\sim$ on $A$ and a strict\footnote{Technically, $B$ doesn't need to be a \emph{strict} subset of $A$ but in the case $B=A$, the restriction does nothing.} subset $B$ of $A$, i.e. $B \subset A$, then we can form the restriction of the relation $\sim$ to $B$. To do this, we take $\sim$, which is a set of pairs from $A \times A$, and just remove from it all those pairs $(x,y)$ for which $x \notin B \vee y \notin B$, i.e. at least one of the components of the pair $(x,y)$ is not an element of $B$. By doing so, we turn $\sim$ into a relation on $B$
...TBC...explain left and right restriction, notations, etc.

% Is there somethin similar for relations between two different sets, i.e. we have a relation between A and B and have subsets C,D of A,B respectively - could we do something like a "birestriction"?

\paragraph{Composition}
You may be familiar with the composition of functions. From the operational point of view, such a composition would just take an input, then apply the first function to obtain an intermediate result and then apply the second function to that to obtain the final result. Functions are specific relations and the notion of composing relations is a generalization of function composition to more general relations ...TBC...

% https://en.wikipedia.org/wiki/Composition_of_relations
% https://en.wikipedia.org/wiki/Function_composition

% Containment:
% -The subset relation, when applied to relations, is called containment. A relation can be 
%  contained in another




