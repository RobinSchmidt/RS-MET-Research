\section{Functions}
Functions are like machines that take some input and produce some output. In the case of mathematical functions, these inputs and outputs are usually numbers. There are more general notions, taking other types of inputs and producing other types of output, for example sets, vectors, matrices, etc. but here, we will mainly look at functions that take one number as input and produce another number as output and those two numbers will typically assumed to be real numbers.

\subsection{Domain and Range}
The two sets (of numbers) from which inputs can be taken and outputs will be produced are called the domain and range of the function respectively. Functions are usually denoted by lowercase letters such as $f,g,h$ etc. If a function takes real numbers as inputs and also produces real numbers as output, we write this as: $f: \mathbb{R} \rightarrow \mathbb{R}$. The argument is usually denoted by $x$ and of the function and is written in parentheses and the output is often denoted by $y$. When we write something like $y = f(x)$, we mean that the function $f$ produces the number $y$ as output when we give it the number $x$ as input. An example could be $y = f(x) = x^2$. The function woul just square the input and return that as result. 

\medskip
\subsubsection{Codomain and Image}
There is a little ambiguity with respect to the notion of range. It may mean either one these two things: sometimes it may mean the set of numbers from which outputs can \emph{potentially} be drawn and sometimes the (possibly smaller) set of numbers, that can \emph{actually} be the result. For example, if the domain of $f(x) = x^2$, is taken to be the set real numbers $\mathbb{R}$, we could think of $f$ as a mapping between real numbers and real numbers, i.e. $f: \mathbb{R} \rightarrow \mathbb{R}$. However, not all possible real numbers are reached because the result of $x^2$ will never be a negative number. We could also consider $f$ as a function from the real numbers to the \emph{nonnegative} real numbers, i.e. $f: \mathbb{R} \rightarrow \mathbb{R}^+_0$. To distinguish these two notions, sometimes the term "codomain" is used for the set that outputs could potentially be drawn from and "image" is used for the set from which numbers are actually produced by the function for a given input domain.

% https://en.wikipedia.org/wiki/Domain_of_a_function
% https://en.wikipedia.org/wiki/Range_of_a_function

\subsection{Taxonomy}
There are certain features that a function may or may not have which turn out to important for classification. A function is said to be \emph{monotonically increasing}, if from $a \geq b$ follows $f(a) \geq f(b)$. It means that the function either goes upward or stays constant but never goes downward. If the stronger condition  $a > b \Rightarrow f(a) > f(b)$ holds true, then $f$ is \emph{strictly monotonically increasing}. Such a function really goes upward everywhere. Not even plateaus are allowed. A \emph{(strictly) monotonically decreasing} function is defined analogously with $\leq, <$ rather than $\geq, >$. If a function is either (strictly) monotonically increasing or decreasing, it is called \emph{(strictly) monotonic}. A function $f$ is said to be \emph{bounded from above} when there is some finite number $U$ such that $f(x) \leq U$ for all $x$. The number $U$ is called an \emph{upper bound} for $f$. Likewise, a function $f$ is called bounded from below, when there's some number $L$ such that $f(x) \geq L$ for all $x$. In this case, $L$ is called a \emph{lower bound} for $f$. A function that is bounded from below and above is called \emph{bounded}. A function for which we have $f(x) = f(-x)$ has \emph{even symmetry} and a function with $f(x) = -f(-x)$ has \emph{odd symmetry}. Every function can be decomposed into an even part and odd part like so: 
\begin{equation}
\label{Eq:EvenOddFuncDecomp}
f_e(x) = \frac{f(x) + f(-x)}{2}, \;\;
f_o(x) = \frac{f(x) - f(-x)}{2}, \qquad
f(x) = f_e(x) + f_o(x)
\end{equation}
\emph{Injectivity}, \emph{surjectivity} and \emph{bijectivity} are also important features which have already been discussed in the section about set theory. Functions can also be \emph{periodic}. That means that there exists some number $p$, called the period, such that $f(x) = f(x + k p)$ for all inputs $x$ and all integers $k$. Such a function repeats itself over and over. Functions may have \emph{asymptotes} which are straight lines which the function approaches as the argument goes to plus or minus infinity. Asymptotes can also be vertical lines. In this case, the line is approached as the argument approaches a finite value and at that value itself, the function is usually undefined. When a function $f(x)$ has a vertical asymptote at some input value $x_0$, we call $x_0$ a \emph{pole} of the function. A function is said to be \emph{compactly supported}, if it is nonzero only in some finite interval and zero outside that interval. A function is called \emph{convex} if you can pick any two points $(a,f(a)),(b,f(b))$ on its graph, connect them with a straight line and the function will be below that line everywhere. The function behaves like a chain hanging down, i.e. it is curved downward like a bowl. The opposite of convex is \emph{concave}. Such functions are everywhere above the straight line. They look like an arch. A \emph{continuous} function is one that can be drawn without lifting the pencil or drawing vertical lines. Functions can also be classified according to their degree of \emph{smoothness} and/or according to whether or not they have a finite area under their graph. To define exactly what that means, we will need a few tools from calculus (differentiability, integrability), so we will defer further discussion about these features to later chapters.

% surjective: codomain coincides with image?

\subsection{Defining a Function}
One way to define a function is to prescribe a simple \emph{calculation rule} such as $f(x) = x^2 + 3 x - 5$ that applies equally to all possible inputs $x$. Up to until a few hundred years, this was the only way that was acceptable for mathematicians to define a function. Today, we have become a lot more liberal in that regard and allow functions to be defined in more esoteric ways. A slightly more complicated way to define a function would be to have different calculation rules for different subsets of the domain. If these subsets are succesive intervals of the real number like, this way of defining a function is called a \emph{piecewise definition}. They are written down like this:
\begin{equation}
f(x) = 
\begin{cases} 
 0 \quad& x < 0 \\
 -x     & 0   \leq x < 1 \\
 x^2    & x \geq 1
\end{cases}
\end{equation}
This means, the function will output $0$ for $x < 0$ and $-x$ when $x$ is between zero and one (including zero, excluding one) and $x^2$ when $x \geq 1$. The example function is discontinuous at $x=1$. It jumps from $-1$ to $+1$ there. A function can also be defined by recorded data points of some measurement. To turn these discrete data points into a continuous function, one would also have to prescribe some sort of \emph{interpolation} rule such as "connect the dots by straight lines". The data together with the interpolation rule constitute a bona fide function: we can give it an input $x$ and it will give back an output $y$. 

%A function can also be defined by various kinds of equations such as implicit, differential, functional, etc.

...tbc...

% maybe make these subsubsection of an "Elementary functions"  section


\subsection{Elementary Functions}
In general, a function can be any mapping. But there is a certain set of functions that permeates mathematics so thoroughly that it has been given a special name: the elementary functions. These are all functions that can be constructed by a finite formula involving only the 4 basic arithmetic operations, roots, exponentials, logarithms, trigonometric functions and the inverses of all these functions.

% https://www.youtube.com/watch?v=l6w868U8C-M

\subsubsection{Polynomials}
A polynomial can be written in various forms. Some important canonical forms are: (1) a weighted sum of integer powers of $x$, (2) a (scaled) product of so called linear factors, (3) a nested expression. So, a polynomial is a function of one of the forms:
\begin{equation}
 f(x) = \sum_{k=0}^n a_k x^k 
      = a_n \prod_{k=1}^{n} (x - r_k)
      = a_0 + x(a_1 + x(a_2 + \ldots + x(a_{n-1} + x a_n)))
\end{equation}
where the weights $a_k$ in the sum form are called the coefficients of the polynomial and the highest power $n$ for which there is a nonzero coefficient is called the degree of the polynomial. The $r_k$ in the product form are called the roots of the polynomial. These are the values of $x$ where the output of the function $y = f(x)$ is zero. This is immediately obvious from the fact that a product is zero as soon as any of its factors is zero, which will clearly be the case if $x = r_k$ for one of the $r_k$. The roots $r_k$ may be real or complex and are not necessarily distinct. The number of times by which a root appears in the product is called the multiplicity of the root. An important theorem, the fundamental theorem of algebra, states that every polynomial of degree $n$ has exactly $n$ such roots (counted with multiplicity, i.e. a double root counts twice, etc.). A polynomial of degree 1, i.e. a function of the form $f(x) = a_0 + a_1  x$, is a very simple special case which is of practical importance and is sometimes - somewhat inconsistently - called a linear function. This terminology is inconsistent because such a function satisfies the usual requirements for linearity - homogeneity and additivity - only when the additive constant is zero, i.e. when $a_0 = 0$. Nevertheless, you'll find that terminology used often. The practical importance of such linear functions is that we will often use them to approximate more complicated functions - a process known as linearization. 

\paragraph{Evaluation}
The evaluation of a polynomial is best done in nested form. This is the most efficient way and also one of the most stable ways numerically. We assume that the polynomial is given as a list or array of $n$ coefficients $a_0, \ldots, a_n$.
\begin{lstlisting}
def polyEval(p, x):
	"""Evaluates the polynomial p at the given x using Horner's algorithm"""
	k = len(p)-1           # last valid index
	if(k < 0):
		return 0
	y = p[k]
	while(k > 0):
		k -= 1
		y = y*x + p[k]
	return y
\end{lstlisting}
This way of evaluating a polynomial is also known as Horner's rule.
...hmm...not sure, if code should appear in the normal text. maybe put it into an appendix?

\paragraph{Addition and Subtraction}
Polynomials, given as arrays of coefficients, can be added and subtracted element wise. If the input arrays are of different length, i.e. the polynomials are of a different degree, the lower degree coefficient array has to be padded with zeros to match the length of the larger one.

...python code is buggy

\paragraph{Multiplication}
When we multiply two polynomials $f(x), g(x)$ with degrees $m,n$ together, we will get a new polynomial of degree $m+n$. The coefficients of this new polynomial can be found by an algorithm called \emph{convolution} ...TBC...

\begin{equation}
(a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \ldots)
(b_0 + b_1 x + b_2 x^2 + b_3 x^3 + \ldots)  \newline
= 
   a_0 b_0 
+ (a_0 b_1 + a_1 b_0) x 
+ (a_0 b_2 + a_1 b_1 + a_2 b_0) x^2 
+ (a_0 b_3 + a_1 b_2 + a_2 b_1 + a_3 b_0) x^3
+ \ldots
\end{equation}


ToDo: explain convolution, give Python code for it

\paragraph{Division}
Oh my dog! Polynomial division! That pesky algorithm that we all once had to learn in school and then have forgotten for good! Worry not - you won't have to relearn it here and now. Executing algorithms with pencil and paper is not something we humans like to do and I personally find it as pointless as the next guy. Instead, I'll give you computer code for it, so we'll never ever have to do it again by hand. ....

\paragraph{Composition}

%You will sometimes find a weaker form of this statement called fundamental theorem of algebra, namely a statement that each polynomial has a root. But the stronger statement immediately follows from the possibility of splitting off a linear factor.

%..tbc..
% product form, fundamental theorem of algebra, linear functions as special case
% factor out a linear factor, polynomial addition, multiplication, division - maybe with (pseudo) code or better: sage code, maybe provide als sage code for basic integer arithmetic - maybe in the section numbers in 

\subsubsection{Rational Functions}
A rational function is a ratio or quotient of two polynomials, i.e. function of the form:
\begin{equation}
 f(x) = \frac{\sum_{k=0}^n a_k x^k}{\sum_{k=0}^m b_k x^k}
\end{equation}

\paragraph{Partial Fraction Expansion}



\subsubsection{Algebraic Functions}
The set of algebraic functions contains all the functions that can be constructed from using the elementary operations and extracting roots....
% partial fraction expansion

\subsubsection{Exponential Functions}

\subsubsection{Trigonometric Functions}



\subsection{Special Functions}
% gamma, bessel, elliptic integrals


\begin{comment}

-polynomials, rational functions, power series, trig-functions, exp/log, floor/ceil/round, atan2
-maybe explain functions also as unary operators? abs, negation


How to define functions:
-explicitly, e.g. y = x^2
-piecewise (may also be explicit but that's actually an independent question)
-by an algorithm (see fractal functions (Bolzano function) Weitz video "Monster der Analysis")
-by data (togther with an interpolation rule)
-power series
-as the solution of a given problem (let f(x) be the solution of ..blabla)
-implicit functions ...maybe something like 
 y + exp(y) = x + sin(x), y - exp(y) = x + sin(x), y - exp(y) = x - sin(x), y + exp(y) = sqrt(x^x)
 is this an example?: https://en.wikipedia.org/wiki/Lambert_W_function
 if not, then maybe it should be considered as yet another way to define a function
-functional equations, examples: 
 f(x + y) = f(x) * f(y)  ->  f(x) = exp(x)
 f(a x) = b f(x)  ->  f(x) = x^(b/a) or x^(a/b)
-differential equations
-(parameter) integrals
-integral equations
-maybe it makes sense to have a section about equations - what type of equations we see in math...but for some types (e.g. differntial or integral equations), we need some calculus

\end{comment}

%Numbers (N,Z,Q,R,C), Polynomials, Rational Functions, Transcendental Functions