\section{Differentiation}

\subsection{The Derivative}
Consider an arbitrary function $f(x)$ and imagine that we want to compute the slope of the graph at a particular $x$ value that we denote by $x_0$. We could approximate it by considering the point $(x_0, f(x_0))$ and another point on the graph a small distance $h$ further to the right $(x_0+h, f(x_0+h))$ and compute the "rise over run" quotient $(f(x_0+h) - f(x_0)) / h$ which would give us the slope of a secant. For smaller and smaller $h$, the approximation would get better and better because the secant would approach the tangent. Now we consider the limit as $h$ approaches zero. If that limit exists, it actually \emph{is} our desired slope of the tangent. We denote that tangent slope at $x_0$ as $f'(x_0)$. This leads us to the following definition:
\begin{equation}
\label{Eq:Derivative}
  f'(x_0) = \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0)}{h}
\end{equation}
We note that we could compute this limit at any value of $x_0$ where that limit exists, so if the limit exists everywhere, we can actually replace $x_0$ by $x$ because $x_0$ just an arbitrary variable name anyway which we may choose any way we want, as long as the name doesn't collide with any other names in the equation. Then, this definition actually defines a whole new function $f'(x)$. This new function $f'$ ("f prime") is called the derivative of $f$.

\paragraph{Example} Let $f(x) = x^2$. Plugging it into the definition above, using $x$ in place of $x_0$ as explained, we get:
\begin{equation}
  f'(x) = \lim_{h \rightarrow 0} \frac{(x + h)^2 - x^2}{h}
        = \lim_{h \rightarrow 0} \frac{x^2 + 2 h x + h^2 - x^2}{h} 
        = \lim_{h \rightarrow 0} \frac{2 h x + h^2}{h} 
        = \lim_{h \rightarrow 0} 2 x + h
        = 2 x
\end{equation}
so the derivative of $f(x) = x^2$ is $f'(x) = 2 x$.

\paragraph{Notations} Writing $f'$ with the prime for the derivative of $f$ is only one of several different notations. It is called the Lagrange notation. Especially in physics and when the independent variable is time $t$, you will also often see a notation with a dot above the function name, here $f$, such that the derivative of $f(t)$ with respect to $t$ would be written as: $\dot{f}(t)$. This is called Newton notation. In Newton notation, we also typically use letters like $x$ or $y$ to denote the function, i.e. use $x$ instead of $f$. When you see something like $\dot{x}(t)$, it means that $x$ is a function of time $t$ and $\dot{x}(t)$ is its derivative with respect to time. So, in this notation, $x$ would be a dependent variable and it depends on $t$ which is the independent variable in this case. Yet another notation is due to Leibniz and that notation writes a $\frac{d}{dx}$ in front of the $f$ such that $f'(x) = \frac{d}{dx} f(x) = \frac{d f(x)}{dx}$. The Leibniz notation can be convenient when manipulating equations because in certain algebraic transformations, the $dx$ can indeed be treated like a denominator in a fraction\footnote{Care needs to be taken, though. It isn't \emph{really} a fraction. But let's leave the rigor to the mathematicians}. Leibniz notation is also useful when we later deal with functions that have multiple input variables because it explicitly tells us with respect to which variable we need to differentiate - an information that is not needed in Lagrange notation because this notation applies only to function with just one input variable (usually $x$). The $\frac{d}{dx}$ in front of the $f$ can actually also be seen as an \emph{operator} that "acts on" the function $f$. There is another such operator notation that uses just a $D$ instead, i.e. $f' = D f$. I have suppressed the argument here, i.e. wrote just $f$ instead of $f(x)$. This is also common practice. In another operator notation, the argument of the operator $D$, which is the function $f$, is put into brackets: $f' = D[f]$. Using brackets for the argument of the operator avoids confusion with multiplication. You may also encounter notations like this:
\begin{equation}
	\frac{d}{d x} f(x) \bigg\rvert_{x=a}
\end{equation}
This means: take the derivative of $f$ with respect to $x$, then evaluate the resulting expression at $x = a$. ToDo: Mention also Leibniz notation for multivariate functions $\partial / \partial x$ and the abbreviation $\partial_x$ that is common in tensor calculus. Also, in PDE contexts, partial derivatives are often denoted as subscript like $u_{xx}$ for the second derivative of $u = u(x,\ldots)$ wrt $x$

% ToDo: also introduce the notation with the brackets for operatoprs D[f] ...done


\subsection{Higher Order Derivatives}
With the derivative, we have an operation to produce a new "derived" function $f'$ from a given function $f$. There's nothing that stops us to apply the same operation to the derived function as well. And then iterate that process as often as we like. That's the way, higher order derivatives are created. 

\paragraph{Notations} The second derivative of a function $f(x)$ is denoted as $f''(x)$ in Lagrange notation, the third as $f'''(x)$. From the fourth one onwards, the "dash" or "prime" notation becomes impractical so one often sees the notation $f^{(n)}(x)$ to denote the $n$-th order derivative of the function $f$. In this notation, the $0$-th order derivative means the function $f$ itself. This makes sense because taking the derivative zero times should be a "do-nothing" operation. To translate this into Leibniz notation, one would write $f^{(n)}(x) = \frac{d^n}{d x^n} f(x) = \frac{d^n f(x)}{d x^n}$. In operator notation with suppressed argument $x$, one would write $f^{(n)} = D^n f$ or $D^n[f]$. That is: in operator notation, putting a superscript like $n$ on an operator usually means: apply the operator $n$ times. It does not mean: apply the operator and then take the $n$th power of the result. This is different to putting a superscript on functions. Something like $f^2(x)$ does indeed usually\footnote{To make matters worse, there are exceptions to that, too. For example: in complex dynamics, $f^n(x)$ indeed means: apply $f$ $n$ times.} means: apply the function $f$, then square the result. This is the reason why in Lagrange notation for arbitrary derivatives, we need to uses these additional parentheses in the superscript to avoid confusion. Higher order derivatives in Newton notation use multiple dots above the function as in $\dot{x}(t), \ddot{x}(t), \dddot{x}(t), \ddddot{x}(t)$. I have seen up to 4 dots but there, the usefulness of this notation ends (and for more than 2, it's already questionable). Time derivatives of order higher than 4 typically do not occur in physics which is the place where Newton's notation is used. Mostly, we deal only with first and second derivatives anyway - and for these, Newton notation works fine. 

%[ToDo: figure out if there's a general way to write an $n$-th time derivative in Newton notation - maybe something $\overset{n}{x}(t)$?]

% ToDo:
% -derive a formula for the definition of f'' based on using the definition of f'
% -interpret the formula in terms of a numeric 2nd derivative when h is small.

\subsection{Elementary Derivatives}
We'll now give a short list of some of the most important derivatives of elementary functions. This list, together with the differentiation rules above, will allow us to mechanically evaluate derivatives of all functions that can be constructed from these elementary functions by algebraic operations or composition (i.e. nesting). 

\medskip
$(x^a)' = a x^{a-1}$, $(a^x)' = \ln(a) \, a^x$, $(\e^x)' = \e^x$, $(\sin x)' = \cos x$, $(\cos x)' = -\sin x$, $(\sinh x)' = \cosh x$, $(\cosh x)' = -\sinh x$, ......TBC...Maybe format the list nicely and gives the names of the rules or some comments in the right column

% When constants like a occur, explaint what values they can take on (like being integer or real or something)

\medskip
Much more comprehensive versions of such a list can be found in many mathematical handbooks or on various websites. Also, computer algebra systems "know" all of these elementary derivatives as well and they also know all the rules how to find derivatives of more complicated functions such that they can be used as glorified tables as well. So, there is no need to waste a lot of space to give a comprehensive list here.

%todo: list derivatives of elementary functions
% x^n      ->  n x^{n-1}     does n need to be an integer? No - I don't think so
% a^x      ->  ln(a) a^x     implying that e^x  ->  e^x
% ln(x)    ->  1/x           what about \log_a (x) ?
% sin(x)   ->  cos(x)
% cos(x)   ->  -sin(x)
% sinh(x)  ->   cosh(x)
% cosh(x)  ->   sinh(x)

% maybe also list derivatives of tan, tanh, etc and also the inverses asin, acos, atan

% -Give links to more comprehensive lists
% -Metion that its easy to evaluate derivatives with a CAS such it can be used as a glorified table


% Maybe list also the antiderivatives, although we do not yet have introduced the concept

% State also some tricks to evaluate certain functions togther with their derivatives without much extra cost








%\subsection{Differentiability}

% Give examples of weird functions:
% continuous everywhere, differentiable nowhere: Weierstrass function, Bolzano function
% nowhere continuous: Dirichlet function D(x)
% continuous only at x=0: x * D(x)
% continuous only at r1,r2,..,rn: (x-r1)*(x-r2)* ... * (x-rn) * D(x)
% continuous only at the integers: sin(2 \pi x) * D(x)
% not differentiable at x = 0: 
%  |x|   because of corner 
%  cbrt(x)   (cube-root of x)  because of infinite slope - limit goes to infinity 
%
% [I think, that's wrong] An f(x) that is not continuous but nevertheless differntiable at x = 0:
% f(x) = x-1 for x < 0, 0 for x = 0, x+1 for x > 0. It has a jump at x = 0 but the slopes match
% the actual value at x = 0 doesn't matter for differentiability. Wait - no - that's wrong! The
% limit of the difference quotient approaches +inf from both sides. Maybe we can make sense of
% the derivative by looking at how fast it approaches inf when we come from left and right. I
% think, the speed will be the same, if the value at the jump discontinuity is exactly in the
% middle of the values immediately before and after the jump? But maybe we can make a definition 
% that indeed assigns a slope of 1 to x=0? Maybe 
% f'(x) = lim_{eps -> 0} lim_{h -> 0} (f(x+eps+h) - f(x+eps)) / h
% the idea is that we do not take the derivative directly at x, but at x+eps for an infinitesimal 
% eps, i.e. a small offset which then also approaches zero. Make an analoguous definition for the
% backward derivative - if both match then the value is the derivative. If they don't match, maybe
% take the average. The basic idea is to take the definition of the regular derivative but apply
% it to a point right next to x (offset by eps) and then, in a second limiting process, let that
% offset also go down to zero.
%
% In calculus, we are mostly insterested in functions that are continuous and differentiable.
% That's why these two features are sometimes lumped into the concept of "continuously
% differentiable". But these concepts are independent, as we have seen. There are functions that
% are discontinuous but differentiable (verify!)






\subsection{Differentiation Rules}
Instead of evaluating derivatives from first principles every time we see one, we have a lookup list of derivatives of all important elementary functions that has been assembled once and for all from first principles (i.e. using the definition via the limit) and we have a couple of rules how to compute derivatives of more complicated functions that are created in various ways from the more basic functions (btw: Did I already say that a big theme in math is finding computational shortcuts? This is a nice example). The most elementary differentiation rules for any two functions $f$ and $g$ and any constant $a$ are the following:

\medskip
\begin{tabular}{c c c l}
  $(a f)'$        &$=$& $ a f'$              & Homogeneity \\
  $(f + g)'$      &$=$& $f' + g'$            & Sum rule (aka Additivity) \\
  $(f \cdot g)'$  &$=$& $f' g + g' f$        & Product rule \\
  $(f / g)'$      &$=$& $(f' g - g' f)/g^2$  & Quotient rule \\
  $(f( g))'$      &$=$& $g' \cdot f'(g)$     & Chain rule \\
\end{tabular}
\medskip
% are there more rules? what about logarithmic differentiation and derivative of inverse?
% In other chapters where I use such tabulars, I usually write the name of the rule into the left column and the formula into the right one. Maybe it would be nice to make that consistent. But maybe having the formula in the left column is better suited for rules that have no name. Leaving the right column blank is less weird than leaving the left column blank

% % https://en.wikipedia.org/wiki/Differentiation_rules

% Maybe write down the reciprocal rule, see:
% https://www.youtube.com/watch?v=qOr-MPiXRaA  at 28:38
% ..it's actually a special case of the quotient rule but may make sense to have as separate rule
% 5 maybe write down the rule for inverse functions. It's alsoin the video. Maybe mention that one needs to take care to not confuse inverse functions with reciprocal functions - I once fell into this trap.



The two first properties, homogeneity and additivity, taken together, constitute the important property of linearity. We may state the linearity property also in a single formula: $(a f + b g)' = a f' + b g'$ where $b$ is another constant. It can easily be verified that this single formula is equivalent to the first two rules in the list above. One lesser known formula is [VERIFY!]:
\begin{equation}
\label{Eq:DerivativeViaH}
 \frac{d}{d x} f(x) 
 = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}
 = \frac{d}{d h} f(x + h) \bigg\rvert_{h=0}
\end{equation}
whose usefulness lies mostly in its generalizations to the directional derivative and Gateaux derivative which we will encounter later. There, we will see a similar definition via a limit and the RHS will tell us, how to actually evaluate the limit - but that's a topic for later. 

% ToDo: Maybe move that rule to the bottom of our list of rules. Maybe try to figure out if it has
% a name. If not, maybe invent one. Maybe shift-change-evaluate (shift by h, change derivative
% from wrt x to wrt h, eval at h=0). Maybe verify this rule in a couple of example cases. Maybe
% call it the dummy-offset rule? or the shift-by-zero rule?`

% Explain the motivation. I think, I have derived it from the Gateaux derivative or from the
% directional derivative in the special case of 1D functions - or something. I'm not sure 
% anymore.


\subsubsection{The Product Rule}
In the list above, we already mentioned the product rule which is also known as the Leibniz rule. In its basic form, it says:
\begin{equation}
 (f g)'  = f' g + g' f
\end{equation}

\paragraph{Products Of More Than 2 Functions}

There is a generalization for the product rule for products with more than two factors. The rule says to take the derivative of one function at a time and leave the other factors as is and then add up the results. For example, for a product of 3 functions, it says that:
\begin{equation}
(fgh)' =  f' g h + f g' h + f g h'
\end{equation}
For a general product of an arbitrary number $n$ of functions $f_1, f_2, \ldots, f_n$:
\begin{equation}
%\frac{d}{dx} \prod_k f_k(x) = 
\Bigl(  \prod_k f_k   \Bigr) '
= \sum_k \Bigl( f_k' \prod_{i \neq k} f_i  \Bigr) 
\end{equation}
where the indices $k,i$ range from $1$ to $n$.

% Do You ACTUALLY Know The Product Rule?
% https://www.youtube.com/watch?v=0dz5HTHWneo
% -Nice generalization of the product rule for a product of multiple functions. The rule says to 
%  take the derivative of one function at a time and leave the others as is and add the results.
%  For 3 functions (f g h)' = f' g h + f g' h + f g h'. In general for n functions, we have a sum of
%  n terms where each term is a product of (n-1) function values and one derivative value. In 
%  practice, we could evaluate it via first forming a product P of all n terms and then form a sum 
%  of terms of the form P * f_i' / f_i where i is the index of the function. This should be more
%  efficient than evaluating the n-1 different products from scratch. We need to take care of
%  possible division by zero, though. In the context evaluating the derivative for Newton iteration,
%  this may not be a problem because when one of f_i is zero, then so is the product which means 
%  that we have already found the root - the problematic case occurs only at the root itself.

\paragraph{Higher Order Derivatives of Products}
Another generalization of the product rule applies to higher order derivatives of products of two functions. It says:
\begin{equation}
 (f g)^{(n)} = \sum_{k=0}^n \binom{n}{k}   f^{(n-k)} g^{(k)}
\end{equation}
Note the structural similarity with the binomial theorem. The $n$-th power would be replaced by the $n$-th derivative and instead of a sum of two variables, we have a product of two functions inside the parentheses. 

%ToDo: Figure out the general case for derivatives of arbitrary order and an arbitrary number of factors. Maybe it would look structurally similar to the multinomial theorem?

% https://en.wikipedia.org/wiki/General_Leibniz_rule


\paragraph{Higher Derivatives for Multiple Factors}
The natural question to ask next is: How about higher order derivatives for products with arbitrary many factors? Yep - there's a formula for that, too. The formula for the $n$-th derivative for a product with $m$ factors is:
\begin{equation}
\Bigl( \prod_{k=1}^{n} f_k \Bigr)^{(n)} =
\sum_{\substack{k_1 + \cdots + k_m = n \\ k_i \geq 0}} 
\Bigl(
\frac{n!}{k_1! \cdots k_m!}
\prod_{i=1}^{m} f_i^{(k_i)} 
\Bigr)
\end{equation}
%\begin{equation}
%\frac{d^n}{d x^n} (f_1(x) \cdots f_m(x) ) =
%\sum_{\substack{k_1 + \cdots + k_m = n \\ k_i \geq 0}} 
%\frac{n!}{k_1! \cdots k_m!} \cdot 
%f_1^{(k_1)}(x) \cdots f_m^{(k_m)}(x)
%\end{equation}
...TBC... ToDo: explain multinomial coeffs, give example with 2nd derivative of 3 factors.
%Maybe write the formula down with product notation Maybe suppress argument x and use Lagrange notation





% the "ultimate" product rule
% https://www.youtube.com/watch?v=wBtff6I_YSI

% https://en.wikipedia.org/wiki/Multinomial_theorem

% Maybe write it down in multi-index notation:
% https://en.wikipedia.org/wiki/Multi-index_notation


\subsubsection{The Quotient Rule} 
As we have already seen in the list above, the rule for computing the derivative of a quotient of two functions $f = f(x)$ and $g = g(x)$ is given by:
\begin{equation}
\left( \frac{f(x)}{g(x)} \right) ' = 
\frac{f'(x) g(x) - g'(x) f(x)}{(g(x))^2}
\end{equation}
In the special case for $f(x) = 1$, we obtain the so called \emph{reciprocal rule} which says:
\begin{equation}
\left( \frac{1}{g(x)} \right) ' = 
- \frac{g'(x)}{(g(x))^2}
\end{equation}

% https://en.wikipedia.org/wiki/Reciprocal_rule

\paragraph{Higher Order Derivatives of Quotients}
There is also a generalization of the quotient rules to higher order derivatives of quotients of functions. It can be obtained from combining the (generalized) product rule with the reciprocal rule. This results in:
\begin{equation}
\left( \frac{f}{g} \right) ^{(n)} = 
\frac{1}{g} \sum_{k=0}^{n} (-1)^k \binom{n+1}{k+1} \frac{(f g^k)^{(n)}}{g^k}
\end{equation}
where we have again supressed the argument to make it bearable. VERIFY! TODO: maybe give different formulas - there are more. Maybe work through an example - maybe take the 3rd derivative. Q: Is there a genralization for quotients of multiple functions - but what would that even mean? Maybe a continued fraction expression?

% Quotient rule for higher derivatives:
% https://math.stackexchange.com/questions/5357/whats-the-generalisation-of-the-quotient-rule-for-higher-derivatives
% https://www.physicsforums.com/threads/quotient-rule-for-higher-order-derivatives.289320/
%
% https://en.wikipedia.org/wiki/Quotient_rule#Higher_order_derivatives
% -Has fomula for 2nd derivative of quotient which looks stiil okayish:
%
%  h'' = (f/g)'' = (f'' - g'' h - 2 g' h') / g
%
% 

% How Not to Repeatedly Differentiate a Reciprocal
% https://www.jstor.org/stable/2324425

% Is there a genralization for quotients of mupltiple functions - but what would that even mean? maybe a continued fraction expression?

\subsubsection{The Chain Rule}
We have also already seen the (basic form of the) chain rule in our table above. There, it was given in terse form with suppressed argument. With unsuppressed argument, it would look like:
\begin{equation}
 (f( g(x)))' = g'(x) \cdot f'(g(x))
\end{equation}
which may make it a bit clearer what is going on.

\paragraph{Compositions of More Than 2 Functions}
There is also a generalization of the chain rule for compositions of more than two functions. For a composition of 3 functions, it states that:
\begin{equation}
f(g(h(x)))' =  f'(g(h(x))) \cdot g'(h(x)) \cdot h'(x)
\end{equation}
I'll refrain from giving a general formula because I would have to introduce an uncommon notation for it. Instead, I'll appeal to your ability to see the pattern. It's basically just a product of all the derivatives and each derivative is taken at the appropriate argument of the respective function. For example, the derivative of $g$ is taken at $h(x)$ because $h(x)$ is the argument of $g$ etc. ...TBC... ToDo: explain relation to backpropagation algorithm. There, we make use of a multivariable version of it.

% https://en.wikipedia.org/wiki/Chain_rule#Composites_of_more_than_two_functions

% https://www.youtube.com/watch?v=-cedS03L17I
% -Logarithmic differentiation: if f > 0, then (log(f(x)))' = f'(x)/f(x)
% -Defining L[f] = f'/f, we get L[f g] = (f'g + g'f) / (fg) = L[f] + L[g]

%TODO: maybe mention backpropagation: to evaluate a derivative of an expression that is a compositions of 3 functions, we first make a forward propagation (inside-out) pass: 
% h(x) -> g(h) -> f(g)
% and then a backward propagation pass to evaluate the derivatives:
% f'(g) -> g'(h) -> h'(x)
% and the multiply them all together to obtain f'(x)
% I think, it doesn't really matter in which direction we evaluate the derivatives - we could also do that in a forward pass - but maybe in the multivariate case, we really need to do it backwards? Try it in the bivariate case

% f(g(h(x))) = f'(g(h(x))) * g'(h(x)) * h'(x)

% Where did I get this formula from? I think, I derived it myself? Maybe via a Taylor expansion of f? Or via specializing to 1D the directional derivative which itself is more easily to "see" intuitively? Or maybe we can define a bivariate function f(x,h) = f(x+h) and derive it from there using partial derivatives? Or maybe evaluate the limit directly from the bivariate function?

% This formula looks actually similar:
% https://en.wikipedia.org/wiki/Differential_of_a_function#General_formulation

\paragraph{Higher Order Derivatives of Composite Functions}
There is also a formula for higher order derivatives of a composition of two functions. It's called \emph{Faa di Bruno's formula}\footnote{Named after Faa di Bruno.} and it says:
\begin{equation}
\frac{d^n}{d x^n} f(g(x)) = 
\sum_{A \in P_n} 
%\left( 
\Bigl(
f^{(|A|)}(g(x)) \cdot \prod_{a \in A} g^{(|a|)}(x) 
\Bigr)
%\right)
\end{equation}
where $P_n$ is the set of all partitions\footnote{A partition of a set is a way of decomposing it into disjoint subsets. See page \pageref{Sec:PartitionsOfSets} for more details.} of the set $\{1,2\ldots,n\}$, $|A|$ the number of parts of partition $A$, i.e. the cardinality of $A$ and $|a|$ is the number of elements in the given part $a$ of a partition $A$. 

\medskip
There's a lot going on in this formula, so let's unpack it with the example case of $n=3$. First, we need to form the set $P_3$, i.e. the set of partitions of the set $\{1,2,3\}$. This is given by:
\begin{equation}
P_3 = \{ A_1, A_2, A_3, A_4, A_5 \}
\end{equation}
where
\begin{equation}
A_1 = \{ \{1\},\{2\},\{3\} \},  \;
A_2 = \{ \{1\},\{2,3\} \},      \;
A_3 = \{ \{2\},\{1,3\} \},      \;
A_4 = \{ \{3\},\{1,2\} \},      \;
A_5 = \{ \{1,2,3\} \}
\end{equation}
$P_3$ is a set of sets each of which is a partition $A_i$. A partition is also a set of sets - namely the set of subsets of, in this case, $\{1,2,3\}$. So, each element of a partition is itself a set. For each of these $5$ partitions $A_1,\ldots,A_5$, we will get a term in the outer sum. These terms look like:
%\begin{equation}
%\begin{aligned}
%&f'''(g(x)) & \cdot & (g'(x))^3        & \text{for $A_1$}  &\\
%&f''(g(x))  & \cdot & g''(x) g'(x)     & \text{for $A_2,A_3,A_4$} &\\
%&f'(g(x))   & \cdot & g'''(x)          & \text{for $A_5$} &\\
%\end{aligned}
%\end{equation}
\begin{eqnarray}
&f'''(g(x)) \cdot (g'(x))^3        & \text{for $A_1$} \\
&f''(g(x))  \cdot g''(x) g'(x)     & \text{for $A_2,A_3,A_4$} \\
&f'(g(x))   \cdot g'''(x)          & \text{for $A_5$} \\
\end{eqnarray}
% ToDo: try to format it better!
where we note that for $A_2,A_3,A_4$, we'll get the same expression. The first factor in these terms is always the $|A_i|$-th derivative of $f(g(x))$. The second factor comes from the product $\prod_{a \in A}$ in the general formula, so it's itself a product which has 3,2 and 1 factors respectively where in the 1st line, the 3 equal factors of $g'(x)$ are abbreviated by cubing it. Putting it all together, we obtain:
\begin{equation}
\frac{d^3}{d x^3} f(g(x)) = 
  f'''(g(x)) \cdot (g'(x))^3 +
3 f''(g(x))  \cdot g''(x)    g'(x)  + 
  f'(g(x))   \cdot g'''(x) 
\end{equation}
For more explanations including a proof sketch, see \cite{YT_FaaDiBruno}.

%Perhaps it would make sense to express it in the form of an algorithm in pseudocode or in python? Maybe figure out an even more general formula for higher order derivatives of composites of multiple functions - but that would then probably be unmanagably messy? Maybe implement it in C++ and link the implementation

% https://en.wikipedia.org/wiki/Fa%C3%A0_di_Bruno%27s_formula

\subsubsection{The Inverse Function Rule}
Given an invertible function $f$ that assigns a value $y = f(x)$ to every $x$ and has an inverse function  $f^{-1}$ that brings us back from $y$ to $x$ such that $x = f^{-1}(y)$, the \emph{inverse function rule} tells us how to find the derivative of  $f^{-1}$ at $y$ from the derivative of $f$ at $x = f^{-1}(y)$. It's the reciprocal:
\begin{equation}
 (f^{-1})'(y) = \frac{1}{f'(f^{-1}(y))}
\end{equation}
Be careful to not confuse the inverse function $f^{-1}$ with the reciprocal of $f$ given by $1/f(x) = (f(x))^{-1}$. These are very different things. Another potential source of confusion is that the evaluation point of $f'$ in the right hand side is obtained by applying the inverse $f^{-1}$ to the argument $y$. That means, the formula relates function values at different evaluation points. This is a noteworthy difference from the other rules that we have seen so far. On the left we evaluate $(f^{-1})'$ at $y$. On the right hand side, we evaluate $f'$ at the $x$ value that corresponds to that $y$. Note furthermore that the choice to call the argument to $f$ by the name $x$ and to call the argument of $f^{-1}$ by the name $y$ is just for clarity. These are just dummy names and we could also have written the rule as $(f^{-1})'(x) = \frac{1}{f'(f^{-1}(x))}$. 

%In the right hand side, we have to evaluate $f'(f^{-1}(x))$. This may be a bit confusing

%That means we first must find

% ...TBC...

% What about inverse function rule? (f^-1)'(x) = 1/f'(f^-1(x))
% https://en.wikipedia.org/wiki/Inverse_function_rule


\subsubsection{Logarithmic Differentiation} 
If a function $f$ is composed with the logarithm function such that $y = \ln(f(x))$, application of the chain rule gives the rule for logarithmic differentiation:
\begin{equation}
 \Bigl( \ln(f(x)) \Bigr)' = \frac{f'(x)}{f(x)}
\end{equation}
For this to work, $f(x)$ must be positive at the evaluation point $x$ because otherwise the logarithm is undefined (Q: What about complex logarithms? They work for negative numbers, too.). ...TBC...ToDo: give examples for using this rule

% https://en.wikipedia.org/wiki/Differentiation_rules#Logarithmic_derivatives
% https://en.wikipedia.org/wiki/Logarithmic_derivative
% https://en.wikipedia.org/wiki/Logarithmic_differentiation

% Give example to differentiate f(x) = x^x


\subsubsection{Generalized Power Rule} 
The power rule $(x^n)' = n x^{n-1}$ has the following generalization:
\begin{equation}
 (f^g)' = \Bigl( \exp(g \, \ln(f) \Bigr) '
        = f^g \Bigl(f^g \, \frac{g}{f} + g' \, \ln(f) \Bigr)
\end{equation}
Or, with unsurpressed argument $x$:
\begin{equation}
 \Bigl(f(x)^{g(x)}\Bigr)' 
 = \Bigl( \exp(g(x) \, \ln(f(x)) \Bigr) '
 = f(x)^{g(x)} \Bigl(f(x)^{g(x)} \, \frac{g(x)}{f(x)} + g'(x) \, \ln(f(x)) \Bigr)
\end{equation}
...TBC...VERIFY. Give examples and special cases, give sources. Q: Do we need to require $f$ to be positive? It appears inside a logarithm on the RHS.

% The reciprocal rule may be derived as the special case where g = -1

% https://en.wikipedia.org/wiki/Power_rule
% https://en.wikipedia.org/wiki/Differentiation_rules#Generalized_power_rule


\subsection{Generalizations, Alternatives and Analogies}
The derivative, as presented here, is an idea that applies to real functions $f: \mathbb{R} \rightarrow \mathbb{R}$. There are certain related notions that apply to other contexts and generalizations of the idea and alternative but equivalent definitions of the same idea.

\subsubsection{Alternative Definitions}

\paragraph{Backward Difference}
In (\ref{Eq:Derivative}), we have defined the derivative as:
\begin{equation}
 f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}
\end{equation}
but I have changed the notation here to use $x$ instead of $x_0$ which, as said, doesn't matter because $x$ or $x_0$ are only dummy variables in this context. We could have also used the definition using a "backward difference":
\begin{equation}
 f'(x) = \lim_{h \rightarrow 0} \frac{f(x) - f(x-h)}{h}
\end{equation}
But that doesn't actually make any difference because we didn't say anything about how $h$ should approach zero. It could approach it from above or from below and the limit is implicitly required to exist in both cases and has to have the same value - recall that that's how a (two-sided) limit was defined. But if some variable $h$ approaches $0$ from below, then $-h$ approaches zero from above. If we replace every occurrence of $h$ by $-h$ in either of the two definitions and do a couple of simple algebraic transformations, we'll arrive at the respective other definition, so we see that both definitions do indeed say the same thing.
% (f(x+h)-f(x))/h = (f(x-h)-f(x))/(-h) = (f(x)-f(x-h))/h

% ToDo: 
% -give nonstandard? names: forward derivative, backward derivative. 
% -Explain notation with superscript + or - for these one-sided derivatives
% -Explain again that *the* derivative only exists when both one-sided derivatives are
%  equal (and the function is continuous at that point). 
% -Explain cases where this is not the case and what the second best thing is ("continuously
%  extendable" or something?).

\paragraph{Central Difference} 
We could also use a central difference definition:
\begin{equation}
 f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x-h)}{2 h}
\end{equation}
...TBC...
% I think, a definition via a central difference could work also for corners. It would
% naturally produce the average of forward and backward difference at a corner. This is nice
% because this is indeed the most intuitive way to define a derivative at a corner. It would
% satisfy the reflection law of optics and it is the value that the Fourier series converges
% to at the step discontinuity of the derivative


\paragraph{Quantum Derivatives}
Common to the definitions of the derivative above is that they involved an initially finite, nonzero step size parameter $h$ and letting that step size $h$ approach zero. If one removes the limiting process, one arrives at the finite difference calculus which is also called $h$-calculus. An alternative definition that you won't usually find in general math textbooks is this:
\begin{equation}
 f'(x) = \lim_{q \rightarrow 1} \frac{f(q x) - f(x)}{ q x - x }
\end{equation}
Instead of letting an additive constant $h$ go to zero, we let a multiplicative constant $q$ go to $1$. Without the limit, we would get what is also known as $q$-calculus. If we apply this definition to the function $f(x) = x^2$, we get:
\begin{equation}
f'(x) = \lim_{q \rightarrow 1} \frac{(q x)^2 - x^2}{ q x - x }
      = \lim_{q \rightarrow 1} \frac{q^2 x^2 - x^2}{ q x - x }
      = \lim_{q \rightarrow 1} \frac{x^2 (q^2 - 1)}{ x (q - 1)}
      = x \lim_{q \rightarrow 1} \frac{(q^2 - 1)}{(q - 1)}         
\end{equation}
where in the last step, we noted that $x$ does not depend on $q$ so it could be dragged out of the limit. We now see that the derivative is proportional to $x$ and what is left is to figure out the constant of proportionality from the limit, that involves only $q$ but not $x$ anymore. We factor the numerator as $q^2 - 1 = (q-1)(q+1)$ to get:
\begin{equation}
f'(x) = x \lim_{q \rightarrow 1} \frac{(q-1)(q+1)}{(q - 1)}     
      = x \lim_{q \rightarrow 1} q+1
      = 2 x
\end{equation}
which gives us the same result. This alternative definition is also related to the so called quantum derivative which is basically the same definition just without taking the limit. Sometimes, this alternative definition is much easier to evaluate than the standard definition. That's why I wanted to mention it. This is especially true when $f(x) = x^n$. With the standard definition, you'd have to expand a term of the form $(x+h)^n$ using the binomial theorem whereas with the alternative definition, things cancel nicely. However, usually, we do not evaluate derivatives using the definition via a limit (standard or alternative) anyway. Instead, we use our table of elementary derivatives together with our differentiation rules.
%Fortunately, we don't have to go through this tedious procedure via the limit every time we need to compute a derivative.

% https://en.wikipedia.org/wiki/Q-derivative
% The definitions above are based on the $h$-calculus. There's also a $q$-calculus

% https://link.springer.com/chapter/10.1007/978-1-4613-0071-7_22
% https://en.wikipedia.org/wiki/Quantum_calculus#History


\subsubsection{Generalizations}

\paragraph{Partial Derivatives}

\paragraph{Complex Derivatives}

\paragraph{Derivative of a Function with Respect to Another}
Suppose we have a function $f$ of $x$, say $f(x) = x^6$, and we want to take the derivative of it - but not with respect to $x$ as usual, but instead with respect to some other function $g$ of $x$, say $g(x) = x^2$. What does it even mean to take such a derivative? We could intuitively imagine that we have some way to access $g(x)$ to wiggle it a little bit. We don't have direct access to $x$ for wiggling - only to $g(x)$ - but, of course, wiggling $g(x)$ will indirectly also wiggle $x$ itself. And therefore, our wiggle applied to $g(x)$ will also lead to a wiggle in $f(x)$. What we are asking for here is, how much does $f$ wiggle in response to our applied wiggle to $g$. Before deriving a general formula, let's first look at the example: $f(x) = x^6, g(x) = x^2$. We want to take the derivative of $x^6$ with respect to $x^2$. In this example, we can use a substitution $u = x^2$ in which case we get $f(u) = u^3$. Taking the derivative of $f$ with respect to $u$ (which is $x^2$), we get $3 u^2$ and backsubstituting, we get $3 x^4$ as our result. But that was an easy special case because $6$ is divisible by $2$ such that we could do nice simplifications. In general, the most straightforward approach is perhaps formal usage of the chain rule in Leibniz notation. For general given $f,g$ we want to compute $\frac{df}{dg}$. The chain rule tells us that $\frac{df}{dx} = \frac{df}{dg} \frac{dg}{dx}$. Now $\frac{df}{dx}$ and $\frac{dg}{dx}$ are just our ordinary derivatives of $f$ and $g$ with respect to $x$, so we already know how to compute them. Solving for our desired $\frac{df}{dg}$, we get:
\begin{equation}
\frac{df}{dg} = \frac{ \frac{df}{dx} }{ \frac{dg}{dx} }
              = \frac{ f'(x) }{ g'(x) }
\end{equation}

...TBC...


% That was all very formal. It shows the power of the Leibniz notation

% Imagine a mechanism with 3 gears. The angle of the central gear is x, the angle of the left gear is g(x) and the angle of the right gear is f(x). We can't wiggle x directly - but we can wiggle g.

% https://www.youtube.com/watch?v=M8iMROLjf-I
% This example f(x) = x^6, g(x) = x^2 is particularly nice because we can tackle it in two ways: first by doing a substitution u = x^2 and then recognizing that this just turns f into u^3. Then differentiate wrt u and backsubstitute. The other, general, way involves the cahin rule. In general, the result is: df/dg = f'(x) / g'(x). For g(x) = x, this simplifies to just f'(x)/1 = f'(x) as it should.

% We could perhaps also take an approach via the inverse function theorem rather than the chain rule. We can take the derivative of f(g^{-1}(x)) wrt to x...I think? ...Try it!

% https://math.stackexchange.com/questions/291376/differentiate-with-respect-to-a-function
% https://math.stackexchange.com/questions/954073/derivative-of-a-function-with-respect-to-another-function
% https://math.stackexchange.com/questions/2437638/derivative-of-a-function-with-respect-to-another-function


% See also:
% Differentiating with respect to... What? | Fractal Derivative
% https://www.youtube.com/watch?v=Nf4fQNNrWAk
% ...not sure, if it's the same idea, though.
% Maybe give an alternative approach based on the limit as h -> 0 of
% (f(x+h) - f(x)) / (g(x+h) - g(x)). First write the usual derivative (f(x+h) - f(x)) / h as
% (f(x+h) - f(x)) / (x+h - x) to make it obvious how this generalizes the normal derivative
% I think, it's about this:
%   https://en.wikipedia.org/wiki/Fractal_derivative
% not to be confused with this:
%   https://en.wikipedia.org/wiki/Fractional_calculus#Fractional_derivatives
% which has a similar name but is a different concept (I think)

% I think, the Leibniz notation is abiguous because df


% Gateaux derivative, Frechet derivative
% https://en.wikipedia.org/wiki/Derivative#Generalizations
% https://en.wikipedia.org/wiki/Generalizations_of_the_derivative

% https://en.wikipedia.org/wiki/Covariant_derivative

% Quantum derivative
% Arithmetic derivative (an analogy)
% Lie Derivative


% Every Type of Derivative Explained in 8 Minutes
% https://www.youtube.com/watch?v=HeCUgK6VdXM
% -Fast paced tour through the zoo of derivatives: classical, 1-sided, higher order, implicit, 
%  complex, partial, directional, covariant, Lie, exterior, material, weak, Frechet, Gateaux, 
%  variational, fractional, Radon-Nikodym, stochastic. 

%\subsubsection{$\star$ Analogies}
\subsubsection{$\star$ Analogies}
There are certain derivative-like operations that can be applied to other things than real functions. They are "derivative-like" in the sense that they obey certain rules that derivatives also obey. Mostly we are interested in operations that satisfy a product rule similar to the one of differentiation [VERIFY].

% Maybe the whole subsubsection starred

\paragraph{Arithmetic Derivatives} The arithmetic derivative is an operation that can be applied to a natural number. For any given natural number $n$, its arithmetic derivative is denoted by $D(n)$ and defined as follows: For prime numbers $p$, we have $D(p) = 1$. For composite numbers that can be factored as $m n$, we have $D(m n) = D(m) \cdot n + m \cdot D(n)$. This second definition, i.e. the one for composite numbers, \emph{is} the product rule. That means that the so defined arithmetic derivative satisfies the product rule by definition. To compute the arithmetic derivative of a given number, the second definition can be applied recursively until we hit the base case of a prime number. [VERIFY!] ToDo: explain why it's well defined - maybe take the example $D(12)$ and show that $D(2 \cdot 6) = D(3 \cdot 4)$ by going through the recursive steps. Explain how to generalize to integer and rational numbers.

% ToDo: mention extensions to the integer and rational numbers. Mention use cases for it.

% https://en.wikipedia.org/wiki/Arithmetic_derivative#Extensions_beyond_natural_numbers
% https://en.wikipedia.org/wiki/Arithmetic_derivative

% https://en.wikipedia.org/wiki/Arithmetic_function
% ...maybe belongs into number theory

\paragraph{Algebraic Derivations}
In the field of abstract algebra, specifically in \emph{differential algebra}, there is the notion of a so called \emph{derivation}. In this context, we start with a vector space and we assume that there is product $a b$ defined between any two vectors $a$ and $b$ which are elements of this vector space. When we equip such a vector space with a product (in addition to the usual vector space operations of addition and scalar multiplication), it is called an \emph{algebra}. When we equip that algebra with yet another (unary) operation $D$ that satisfies the product rule $D(a b) = a D(b) + D(a) b$, then we call the operation $D$ a \emph{derivation}. Note that the order of the factors in this formula may be important when the product is non-commutative. A commutative algebra equipped with a derivation is called an \emph{ordinary differential ring}. [VERIFY!]. If we have multiple derivations $D_1, D_2, \ldots$ which pairwise commute (i.e. $D_1(D_2(\ldots)) = D_2(D_1(\ldots))$), then we have a \emph{partial differential ring}. These structures model the behavior of ordinary or partial differential equations in the ways of abstract algebra. [VERIFY]. As an example for an ordinary differential ring, we can take the vector space of all polynomials (or maybe power series, if we allow "infinite polynomials"). The product would be defined by polynomial multiplication and the derivation would be defined by the usual way in which we take derivatives of polynomials. That immediately gives us a way to define products and derivations on (possibly infinite) sequences - we just interpret the sequence elements as polynomial coefficients and use the rules for multiplying and differentiating polynomials (or power series). That means: multiplication on sequences is convolution and derivation on sequences is multiplying by index and then left-shifting. [VERIFY!] ...TBC...

% https://en.wikipedia.org/wiki/Differential_algebra#Differential_rings

% I think, in this context, an "algebra" is a vector space in which there is a product defined 
% between any two vectors a,b. Then, it must hold that:
%
% D(a b) = a D(b) + D(a) b
%
% for the operation D to qualify as a "derivation". Note that the order of the factors in the 
% formula is important because the product is not required to be commutative in general. Maybe give
% as example the vector space of infinite real valued sequences and interpret the elements as 
% polynomial coefficients (or power series coeffs). The "product" would be convolution and the
% "derivation" would be our regular rule for computing derivatives of polynomials (or power series)

% Or maybe consider the vector space of all polynomials up to degree n. But no - limiting the degree
% to any number, we wouldn't get a vector space because by using the product, we could always
% produce polynomials of even higher degree. Maybe take the vector space of all polynomials.

% https://en.wikipedia.org/wiki/Derivation_(differential_algebra)
% https://en.wikipedia.org/wiki/P-derivation


\begin{comment}
	
ToDo:
- Use Lagrange notation ocnsistently throughout the chapter. Currently, there are places where we
  Lagrange notation, in other places we use Leibniz notation, in other places we use both. That's
  not nice!
	
This video has an interesting alternative definition of the derivative:

https://www.youtube.com/watch?v=XfWgfZ5V2qI  New Definition of the Derivative

f'(x) = \lim_{t \rightarrow 1} \frac{f(tx) - f(x)}{t x  - x}

which is often algebraically simpler to evaluate - especially for powers of x. No binomial theorem is needed. 

It has also this useful formula:
(t^n - 1) = (t-1) (1 + t + t^2 + t^3 + ... + t^{n-1})	
which might be integrated somewhere in the elementary algebra section

Differentiting a function f of x with respect to another function g of x:
https://www.youtube.com/watch?v=1Ci2z5YE6Rg
example: differentiate x^x with respect to x^2. Apply the chain rule to a function
f(g(x))  ->  df/dx = df/dg * dg/dx  ->  df/dg = (df/dx) / (dg/dx)
But what does that even mean? I think, we can interpret df/dg it as follows: We have two given functions f,g of x and ask: how much does f (infinitesimally) change, if we change g (infinitesimally)? An example could be: x is a signal amplitude, f is the signal power, i.e. f(x) ~ x^2 and g are decibels g(x) = 20*log10(x). The question would be: how much does the signal *power* change when we change the signal *level* in dB.

Maybe see also:
https://en.wikipedia.org/wiki/Riemann%E2%80%93Stieltjes_integral

https://en.wikipedia.org/wiki/Quantum_calculus



https://www.youtube.com/watch?v=-nkPCUoYwJo  Is it a fraction? What are dx and dy?

	
\end{comment}

