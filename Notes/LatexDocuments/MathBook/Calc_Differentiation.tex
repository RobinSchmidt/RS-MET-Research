\section{Differentiation}

\subsection{The Derivative}
Consider an arbitrary function $f(x)$ and imagine that we want to compute the slope of the graph at a given $x_0$. We could approximate it by considering the point $(x_0, f(x_0))$ and another point on the graph a small distance $h$ further to the right $(x_0+h, f(x_0+h))$ and compute the "rise over run" quotient $(f(x_0+h) - f(x_0)) / h$ which would give us the slope of a secant. For smaller and smaller $h$, the approximation would get better and better because the secant would approach the tangent. Now we consider the limit as $h$ approaches zero. If that limit exists, it actually \emph{is} our desired slope of the tangent. We denote that tangent slope at $x_0$ as $f'(x_0)$. This leads us to the following definition:
\begin{equation}
\label{Eq:Derivative}
  f'(x_0) = \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0)}{h}
\end{equation}
We note that we could compute this limit at any value of $x_0$ where that limit exists, so if the limit exists everywhere, we can actually replace $x_0$ by $x$ because $x_0$ just an arbitrary variable name anyway which we may choose any way we want, as long as the name doesn't collide with any other names in the equation. Then, this definition actually defines a whole new function $f'(x)$. This new function $f'$ ("f prime") is called the derivative of $f$.

\paragraph{Example} Let $f(x) = x^2$. Plugging it into the definition above, using $x$ in place of $x_0$ as explained, we get:
\begin{equation}
  f'(x) = \lim_{h \rightarrow 0} \frac{(x + h)^2 - x^2}{h}
        = \lim_{h \rightarrow 0} \frac{x^2 + 2 h x + h^2 - x^2}{h} 
        = \lim_{h \rightarrow 0} \frac{2 h x + h^2}{h} 
        = \lim_{h \rightarrow 0} 2 x + h
        = 2 x
\end{equation}
so the derivative of $f(x) = x^2$ is $f'(x) = 2 x$.

\paragraph{Notations} Writing $f'$ with the prime for the derivative of $f$ is only one of several different notations. It is called the Lagrange notation. Especially in physics and when the independent variable is time $t$, you will also often see a notation with a dot above the function name, here $f$, such that the derivative of $f(t)$ with respect to $t$ would be written as: $\dot{f}(t)$. This is called Newton notation. Yet another notation is due to Leibniz and that notation writes a $\frac{d}{dx}$ in front of the $f$ such that $f'(x) = \frac{d}{dx} f(x) = \frac{d f(x)}{dx}$. The Leibniz notation can be convenient when manipulating equations because in certain algebraic transformations, the $dx$ can indeed be treated like a denominator in a fraction. The $\frac{d}{dx}$ in front of the $f$ can actually also be seen as an \emph{operator} that "acts on" the function $f$. There is another such operator notation that uses just a $D$ instead, i.e. $f' = D f$. I have suppressed the argument here, i.e. wrote just $f$ instead of $f(x)$. This is also common practice. In another operator notation, the argument of the operator $D$, which is the function $f$, is put into brackets: $f' = D[f]$. You may also encounter notations like this:
\begin{equation}
	\frac{d}{d x} f(x) \bigg\rvert_{x=a}
\end{equation}
This means: take the derivative of $f$ with respect to $x$, then evaluate the resulting expression at $x = a$.

%ToDo: Introduce the "evaluated at" notation with the vertical bar on the right. [Done?]
% also introduce the notation with the brackets

\subsection{Differentiability}

% Give examples of weird functions:
% continuous everywhere, differentiable nowhere: Weierstrass function, Bolzano function
% nowhere continuous: Dirichlet function D(x)
% continuous only at x=0: x * D(x)
% continuous only at r1,r2,..,rn: (x-r1)*(x-r2)* ... * (x-rn) * D(x)
% continuous only at the integers: sin(2 \pi x) * D(x)
% not differentiable at x = 0: 
%  |x|   because of corner 
%  cbrt(x)   (cube-root of x)  because of infinite slope - limit goes to infinity 
%
% [I think, that's wrong] An f(x) that is not continuous but nevertheless differntiable at x = 0:
% f(x) = x-1 for x < 0, 0 for x = 0, x+1 for x > 0. It has a jump at x = 0 but the slopes match
% the actual value at x = 0 doesn't matter for differentiability. Wait - no - that's wrong! The
% limit of the difference quotient approaches +inf from both sides. Maybe we can make sense of
% the derivative by looking at how fast it approaches inf when we come from left and right. I
% think, the speed will be the same, if the value at the jump discontinuity is exactly in the
% middle of the values immediately before and after the jump? But maybe we can make a definition 
% that indeed assigns a slope of 1 to x=0? Maybe 
% f'(x) = lim_{eps -> 0} lim_{h -> 0} (f(x+eps+h) - f(x+eps)) / h
% the idea is that we do not take the derivative directly at x, but at x+eps for an infinitesimal 
% eps, i.e. a small offset which then also approaches zero. Make an analoguous definition for the
% backward derivative - if both match then the value is the derivative. If they don't match, maybe
% take the average. The basic idea is to take the definition of the regular derivative but apply
% it to a point right next to x (offset by eps) and then, in a second limiting process, let that
% offset also go down to zero.
%
% In calculus, we are mostly insterested in functions that are continuous and differentiable.
% That's why these two features are sometimes lumped into the concept of "continuously
% differentiable". But these concepts are independent, as we have seen. There are functions that
% are discontinuous but differentiable (verify!)


\subsection{Differentiation Rules}
Instead of evaluating derivatives from first principles every time, we see one, we have a lookup list of derivatives of all important elementary functions that has been assembled once and for all by the technique above or otherwise and we have a couple of rules how to compute derivatives of more complicated functions that are created in various ways from the more basic functions (btw: Did I already say that a big theme in math is finding computational shortcuts? This is a nice example). These rules are for any two functions $f$ and $g$ and any constant $a$:

\medskip
\begin{tabular}{c c c l}
  $(a f)'$        &$=$& $ a f'$              & Homogeneity \\
  $(f + g)'$      &$=$& $f' + g'$            & Sum rule (aka Additivity) \\
  $(f \cdot g)'$  &$=$& $f' g + g' f$        & Product rule \\
  $(f / g)'$      &$=$& $(f' g - g' f)/g^2$  & Quotient rule \\
  $(f( g))'$      &$=$& $g' \cdot f'(g)$     & Chain rule \\
\end{tabular}
\medskip
% are there more rules? what about logarithmic differentiation and derivative of inverse?
% In other chapters where I use such tabulars, I usually write the name of the rule into the left column and the formula into the right one. Maybe it would be nice to make that consistent. But maybe having the formula in the left column is better suited for rules that have no name. Leaving the right column blank is less weird than leaving the left column blank

% Maybe write down the reciprocal rule, see:
% https://www.youtube.com/watch?v=qOr-MPiXRaA  at 28:38
% ..it's actually a special case of the quotient rule but may make sense to have as separate rule
% 5 maybe write down the rule for inverse functions. It's alsoin the video. Maybe mention that one needs to take care to not confuse inverse functions with reciprocal functions - I once fell into this trap.

% What about inverse function rule? (f^-1)'(x) = 1/f'(f^-1(x))
% https://en.wikipedia.org/wiki/Inverse_function_rule

The two first properties, homogeneity and additivity, taken together constitute the important property of linearity. We may state the linearity property also in a single formula: $(a f + b g)' = a f' + b g'$ where $b$ is another constant. It can easily be verified that this single formula is equivalent to the first two rules in the list above. One lesser known formula is [VERIFY!]:
\begin{equation}
\label{Eq:DerivativeViaH}
 \frac{d}{d x} f(x) 
 = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}
 = \frac{d}{d h} f(x + h) \bigg\rvert_{h=0}
\end{equation}
whose usefulness lies mostly in its generalizations to the directional derivative and Gateaux derivative which we will encounter later. There, we will see a similar definition via a limit and the RHS will tell us, how to actually evaluate the limit....but that's for later... 
% Where did I get this formula from? I think, I derived it myself? Maybe via a Taylor expansion of f? Or via specializing to 1D the directional derivative which itself is more easily to "see" intuitively? Or maybe we can define a bivariate function f(x,h) = f(x+h) and derive it from there using partial derivatives? Or maybe evaluate the limit directly from the bivariate function?

% This formula looks actually similar:
% https://en.wikipedia.org/wiki/Differential_of_a_function#General_formulation


% Generalization of chain rule to higher oder derivatives - Faa di Bruno's formula:
% https://en.wikipedia.org/wiki/Fa%C3%A0_di_Bruno%27s_formula

%todo: list derivatives of elementary functions

\subsection{Elementary Derivatives}
We'll now give a list of some of the most important derivatives of elementary functions. This list (or a more comprehensive version of it that can be found elsewhere), together with the differentiation rules above, will allow us to mechanically evaluate derivatives of all functions that can be constructed from these elementary functions by algebraic operations or composition (i.e. nesting).
...

\subsection{Higher Order Derivatives}
With the derivative, we have an operation to produce a new "derived" function from a given function. There's nothing that stops us to apply the same operation to the derived function as well. And then iterate that process as often as we like. That's the way, higher order derivatives are created.

...TBC...

% ToDo:
% -derive a formula for the definition of f'' based on using the definition of f'
% -interpret the formula in terms of a numeric 2nd derivative when h is small.

% give all different notations $f'', f''', f^{(k)}, \frac{d^k}{d x^k}, D^k, \ddot{f}, \dddot{f}$



\subsection{Generalizations, Alternatives and Analogies}
The derivative, as presented here, is an idea that applies to real functions $f: \mathbb{R} \rightarrow \mathbb{R}$. There are certain related notions that apply to other contexts and generalizations of the idea and alternative but equivalent definitions of the same idea.

\subsubsection{Alternative Definitions}

\paragraph{Backward Difference}
In (\ref{Eq:Derivative}), we have defined the derivative as:
\begin{equation}
 f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}
\end{equation}
but I have changed the notation here to use $x$ instead of $x_0$ which, as said, doesn't matter because $x$ or $x_0$ are only dummy variables in this context. We could have also used the definition using a "backward difference":
\begin{equation}
 f'(x) = \lim_{h \rightarrow 0} \frac{f(x) - f(x-h)}{h}
\end{equation}
But that doesn't actually make any difference because we didn't say anything about how $h$ should approach zero. It could approach it from above or from below and the limit is implicitly required to exist in both cases and has to have the same value - recall that that's how a (two-sided) limit was defined. But if some variable $h$ approaches $0$ from below, then $-h$ approaches zero from above. If we replace every occurrence of $h$ by $-h$ in either of the two definitions and do a couple of simple algebraic transformations, we'll arrive at the respective other definition, so we see that both definitions do indeed say the same thing.
% (f(x+h)-f(x))/h = (f(x-h)-f(x))/(-h) = (f(x)-f(x-h))/h

% ToDo: 
% -give nonstandard? names: forward derivative, backward derivative. 
% -Explain notation with superscript + or - for these one-sided derivatives
% -Explain again that *the* derivative only exists when both one-sided derivatives are
%  equal (and the function is continuous at that point). 
% -Explain cases where this is not the case and what the second best thing is ("continuously
%  extendable" or something?).

\paragraph{Central Difference} 
We could also use a central difference definition:
\begin{equation}
 f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x-h)}{2 h}
\end{equation}
...TBC...
% I think, a definition via a central difference could work also for corners. It would
% naturally produce the average of forward and backward difference at a corner. This is nice
% because this is indeed the most intuitive way to define a derivative at a corner. It would
% satisfy the reflection law of optics and it is the value that the Fourier series converges
% to at the step discontinuity of the derivative


\paragraph{Quantum Derivatives}
Common to the definitions of the derivative above is that they involved an initially finite, nonzero step size parameter $h$ and letting that step size $h$ approach zero. If one removes the limiting process, one arrives at the finite difference calculus which is also called $h$-calculus. An alternative definition that you won't usually find in general math textbooks is this:
\begin{equation}
 f'(x) = \lim_{q \rightarrow 1} \frac{f(q x) - f(x)}{ q x - x }
\end{equation}
Instead of letting an additive constant $h$ got to zero, we let a multiplicative constant $q$ go to $1$. Without the limit, we would get what is also known as $q$-calculus. If we apply this definition to the function $f(x) = x^2$, we get:
\begin{equation}
f'(x) = \lim_{q \rightarrow 1} \frac{(q x)^2 - x^2}{ q x - x }
      = \lim_{q \rightarrow 1} \frac{q^2 x^2 - x^2}{ q x - x }
      = \lim_{q \rightarrow 1} \frac{x^2 (q^2 - 1)}{ x (q - 1)}
      = x \lim_{q \rightarrow 1} \frac{(q^2 - 1)}{(q - 1)}         
\end{equation}
where in the last step, we noted that $x$ does not depend on $q$ so it could be dragged out of the limit. We now see that the derivative is proportional to $x$ and what is left is to figure out the constant of proportionality from the limit, that involves only $q$ but not $x$ anymore. We factor the numerator as $q^2 - 1 = (q-1)(q+1)$ to get:
\begin{equation}
f'(x) = x \lim_{q \rightarrow 1} \frac{(q-1)(q+1)}{(q - 1)}     
      = x \lim_{q \rightarrow 1} q+1
      = 2 x
\end{equation}
which gives us the same result. This alternative definition is also related to the so called quantum derivative which is basically the same definition just without taking the limit. Sometimes, this alternative definition is much easier to evaluate than the standard definition. That's why I wanted to mention it. This is especially true when $f(x) = x^n$. With the standard definition, you'd have to expand a term of the form $(x+h)^n$ using the binomial theorem whereas with the alternative definition, things cancel nicely. However, usually, we do not evaluate derivatives using the definition via a limit (standard or alternative) anyway. Instead, we use our table of elementary derivatives together with our differentiation rules.
%Fortunately, we don't have to go through this tedious procedure via the limit every time we need to compute a derivative.

% https://en.wikipedia.org/wiki/Q-derivative
% The definitions above are based on the $h$-calculus. There's also a $q$-calculus

% https://link.springer.com/chapter/10.1007/978-1-4613-0071-7_22
% https://en.wikipedia.org/wiki/Quantum_calculus#History


\subsubsection{Generalizations}

\paragraph{Partial Derivatives}

\paragraph{Complex Derivatives}

\paragraph{Derivative of a Function with Respect to Another}
Suppose we have a function $f$ of $x$, say $f(x) = x^6$, and we want to take the derivative of it - but not with respect to $x$ as usual, but instead with respect to some other function $g$ of $x$, say $g(x) = x^2$. What does it even mean to take such a derivative? We could intuitively imagine that we have some way to access $g(x)$ to wiggle it a little bit. We don't have direct access to $x$ for wiggling - only to $g(x)$ - but, of course, wiggling $g(x)$ will indirectly also wiggle $x$ itself. And therefore, our wiggle applied to $g(x)$ will also lead to a wiggle in $f(x)$. What we are asking for here is, how much does $f$ wiggle in response to our applied wiggle to $g$. Before deriving a general formula, let's first look at the example: $f(x) = x^6, g(x) = x^2$. We want to take the derivative of $x^6$ with respect to $x^2$. In this example, we can use a substitution $u = x^2$ in which case we get $f(u) = u^3$. Taking the derivative of $f$ with respect to $u$ (which is $x^2$), we get $3 u^2$ and backsubstituting, we get $3 x^4$ as our result. But that was an easy special case because $6$ is divisible by $2$ such that we could do nice simplifications. In general, the most straightforward approach is perhaps formal usage of the chain rule in Leibniz notation. For general given $f,g$ we want to compute $\frac{df}{dg}$. The chain rule tells us that $\frac{df}{dx} = \frac{df}{dg} \frac{dg}{dx}$. Now $\frac{df}{dx}$ and $\frac{dg}{dx}$ are just our ordinary derivatives of $f$ and $g$ with respect to $x$, so we already know how to compute them. Solving for our desired $\frac{df}{dg}$, we get:
\begin{equation}
\frac{df}{dg} = \frac{ \frac{df}{dx} }{ \frac{dg}{dx} }
              = \frac{ f'(x) }{ g'(x) }
\end{equation}

...TBC...


% That was all very formal. It shows the power of the Leibniz notation

% Imagine a mechanism with 3 gears. The angle of the central gear is x, the angle of the left gear is g(x) and the angle of the right gear is f(x). We can't wiggle x directly - but we can wiggle g.

% https://www.youtube.com/watch?v=M8iMROLjf-I
% This example f(x) = x^6, g(x) = x^2 is particularly nice because we can tackle it in two ways: first by doing a substitution u = x^2 and then recognizing that this just turns f into u^3. Then differentiate wrt u and backsubstitute. The other, general, way involves the cahin rule. In general, the result is: df/dg = f'(x) / g'(x). For g(x) = x, this simplifies to just f'(x)/1 = f'(x) as it should.

% We could perhaps also take an approach via the inverse function theorem rather than the chain rule. We can take the derivative of f(g^{-1}(x)) wrt to x...I think? ...Try it!

% https://math.stackexchange.com/questions/291376/differentiate-with-respect-to-a-function
% https://math.stackexchange.com/questions/954073/derivative-of-a-function-with-respect-to-another-function
% https://math.stackexchange.com/questions/2437638/derivative-of-a-function-with-respect-to-another-function


% See also:
% Differentiating with respect to... What? | Fractal Derivative
% https://www.youtube.com/watch?v=Nf4fQNNrWAk
% ...not sure, if it's the same idea, though.
% Maybe give an alternative approach based on the limit as h -> 0 of
% (f(x+h) - f(x)) / (g(x+h) - g(x)). First write the usual derivative (f(x+h) - f(x)) / h as
% (f(x+h) - f(x)) / (x+h - x) to make it obvious how this generalizes the normal derivative
% I think, it's about this:
%   https://en.wikipedia.org/wiki/Fractal_derivative
% not to be confused with this:
%   https://en.wikipedia.org/wiki/Fractional_calculus#Fractional_derivatives
% which has a similar name but is a different concept (I think)

% I think, the Leibniz notation is abiguous because df


% Gateaux derivative, Frechet derivative
% https://en.wikipedia.org/wiki/Derivative#Generalizations
% https://en.wikipedia.org/wiki/Generalizations_of_the_derivative

% https://en.wikipedia.org/wiki/Covariant_derivative

% Quantum derivative
% Arithmetic derivative (an analogy)
% Lie Derivative



\subsubsection{Analogies}
There are certain derivative-like operations that can be applied to other things than real functions. They are "derivative-like" in the sense that they obey certain rules that derivatives also obey - mostly the product rule [VERIFY].

\paragraph{Arithmetic Derivatives}

\paragraph{Algebraic Derivations}
In the field of abstract algebra, specifically in \emph{differential algebra}, there is the notion of a so called \emph{derivation}. ...tbc...

%https://en.wikipedia.org/wiki/Derivation_(differential_algebra)

%

\begin{comment}
	
This video has an interesting alternative definition of the derivative:

https://www.youtube.com/watch?v=XfWgfZ5V2qI  New Definition of the Derivative

f'(x) = \lim_{t \rightarrow 1} \frac{f(tx) - f(x)}{t x  - x}

which is often algebraically simpler to evaluate - especially for powers of x. No binomial theorem is needed. 

It has also this useful formula:
(t^n - 1) = (t-1) (1 + t + t^2 + t^3 + ... + t^{n-1})	
which might be integrated somewhere in the elementary algebra section

Differentiting a function f of x with respect to another function g of x:
https://www.youtube.com/watch?v=1Ci2z5YE6Rg
example: differentiate x^x with respect to x^2. Apply the chain rule to a function
f(g(x))  ->  df/dx = df/dg * dg/dx  ->  df/dg = (df/dx) / (dg/dx)
But what does that even mean? I think, we can interpret df/dg it as follows: We have two given functions f,g of x and ask: how much does f (infinitesimally) change, if we change g (infinitesimally)? An example could be: x is a signal amplitude, f is the signal power, i.e. f(x) ~ x^2 and g are decibels g(x) = 20*log10(x). The question would be: how much does the signal *power* change when we change the signal *level* in dB.

Maybe see also:
https://en.wikipedia.org/wiki/Riemann%E2%80%93Stieltjes_integral

https://en.wikipedia.org/wiki/Quantum_calculus


	
\end{comment}

