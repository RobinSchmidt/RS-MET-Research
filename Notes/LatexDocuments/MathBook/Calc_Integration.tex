\section{Integration} 

\subsection{The Riemann Integral} 
Imagine we have a function $y = f(x)$ and want to compute the area that is enclosed between some interval of the $x$-axis, say $[a,b]$, and the graph of the function over this interval. We could approach this problem by approximating the area by a bunch of rectangles as follows: we split the interval $[a,b]$ into some number $n$ of equally sized subintervals, for each subinterval compute the area of a rectangle where one side is given by the width of the subinterval and the other by the height of the function somewhere inside the subinterval, then add up all those areas of the rectangles. When the number of intervals $n$ grows, the width of the intervals shrinks, the rectangles become thinner and the approximation becomes better. In the limit, as $n$ approaches infinity, the approximation becomes exact. We could define the integral as follows:
\begin{equation}
 I = \int_a^b f(x) \, dx = \lim_{n \rightarrow \infty}  \sum_{i=1}^n f(x_i) \, \Delta x_i
\end{equation}
Here, $\Delta x_i$ is the width of the $i$th subinterval and $x_i$ is a value somewhere inside the $i$th subinterval. This is not quite Riemman's exact definition, but it's close enough to capture the idea. Riemann does not require the intervals to be equally sized but only demands that the largest of them should shrink to a width of zero as $n$ approaches infinity. He also additionally demands that it should not matter, where exactly inside the $i$th interval we pick $x_i$ to be. It could be at the left or right boundary, right in the middle, at the lowest or highest point or whereever. The Riemann integral is only defined if it doesn't matter, i.e. if the above limit will always give the same result.

\medskip
The interpretation of the Riemann integral as an area is actually a somewhat sloppy interpretation: it works only when we assume that the function graph is above the $x$-axis for the whole integration interval. If this is not the case, the portions below the $x$-axis will actually produce negative contributions to the total area. If the function is more below the $x$-axis than above, the "area" may even become negative. It is more useful to think about the integral as a sort of "signed area". If we divide the integral by the width of the integration interval, i.e. by $b-a$, we obtain an average value of the function over the interval $[a,b]$.

% maybe mention the Lesbesgue integral, too - maybe other generalizations (Riemman-Stieljes, etc.)

\subsection{Antiderivatives and Indefinite Integrals}
We now want to consider a situation where the lower limit of the integration $a$ is fixed as before but the upper limit $b$ is taken to be a variable. In that case, the above expression actually defines a new function - namely, a function of $b$. Just like via the derivative, we have "derived" a new function $f'(x)$ from our given $f(x)$, we can derive a new function, let's call it $F$ from $f$ via the following operation:
\begin{equation}
F(b) = \int_a^b f(x) \, dx \qquad \text{or} \qquad
F(x) = \int_a^x f(u) \, du
\end{equation}
Note that $b$ and $x$ are just dummies or placeholders. The left and right version say exactly the same thing and I have just chosen different names for the dummy variables in both expressions. The functions $f$ and $F$ and their relationship is of interest here - not how we choose to call their arguments. OK, so we have created a new function $F$ from our given function $f$. That new function $F$ is called an \emph{indefinite integral} or an \emph{antiderivative} of $f$. I deliberately said "an" antiderivative and not "the" antiderivative because it is not uniquely determined: we could pick a different fixed $a$ and would get another antiderivative. The reason for calling it an antiderivative is, as you may have guessed, its relation to the derivative. This relation is captured in...(drum roll)...

\subsection{The Fundamental Theorem of Calculus}


\subsection{Improper Integrals} 


\subsection{Integration Techniques} 

\subsubsection{Integration by Parts} 
Write the function $f(x)$ to be integrated as a product $u(x)v(x)$ such that the remaining integral over $u' v$ in the formula is easier to evaluate than the original one. This usually means to choose a $u$ that simplifies under differentiation and/or to choose a $v'$ that simplifies under integration. The formula for a definite integral is:
\begin{equation}
  \int_a^b u(x) v'(x) \, dx = \Big[u(x) v(x)\Big]_a^b - \int_a^b u'(x) v(x) \, dx
\end{equation}
With $u = u(x), du = u'(x) dx, v = v(x), dv = v'(x) dx$, an indefinite integral can be written using the differentials $du, dv$ as:
\begin{equation}
  \int u \, dv \ =\ uv - \int v \, du
\end{equation}
The formulas can be derived from the product rule of differentiation.

\paragraph{Example} Let's find the indefinite integral of $f(x) = 2 x \ln x \, dx$ using $u' = 2 x, v = \ln x$ such that $u = x^2, v' = 1/x$ and $v' u = x^2/x = x$. Note that the roles of $u,v$ are reversed here. They are just dummy names and you'll find formulas using both conventions, so a bit of flexibility is good:
\begin{align}
  \int 2 x \ln x \, dx = x^2 \ln x - \int x \, dx = x^2 \ln x - \frac{x^2}{2} + C
\end{align}
...yeah, the $+ C$ has been neglected (i.e. taken to be zero) in the formulas above. It's more convenient to just add the integration constant at the very end of the calculation.


%\newline % we want some vertical space - newline doe not really cut it
See also:
\href{https://en.wikipedia.org/wiki/Integration_by_parts}{wikipedia.org/wiki/Integration\_by\_parts}

% Applications: finding adjoint operators (move integral from one function to the other)
% https://www.youtube.com/watch?v=aG5tFA8GJ78 Linear Operators and their Adjoints (Nathan Kutz)


% ToDo: Substitution, Logarithmic integration, integration of inverse?

\subsubsection{The Risch Algorithm}
Finding an antiderivative for a given function is hard. Unlike for differentiation, for integration there is no simple mechanical set of rules that we can just blindly apply and be sure that it will always lead us to the result. Except: there actually is one - but with a caveat: it is not really suitable for execution by hand using pencil and paper. It's too complicated and messy for that. However, computers can deal with it easily, provided they have the appropriate software installed. This set of rules is embodied in the Risch algorithm and some variation of this algorithm is usually integrated into every computer algebra system. Not all such systems implement the full algorithm
...TBC...

\medskip
In my humble personal opinion, I think nowadays with the widespread availability of computers and the Risch algorithm, it is pointless to try memorizing all these integration techniques and tricks. The problem has been solved for good - no need to torture students with that anymore. Maybe it's just as pointless as being able to calculate square roots with pencil and paper. Yes it is absolutely possible and back in the day, it was the way to go - but for dog's sake, we now have computers for that kind of stuff. I don't consider the hand calculation of square roots to be an essential mathematical skill and I tend to think about manual integration techniques similarly. 
%However, I do consider the ability to implement a square-root algorithm

%and tricks to find antiderivatives by hand

\begin{comment}

Differentiation is a purely mechanic process: given the list of elemenary derivatives and the differentiation rules, we can easily compute an expression for the derivative of any function. In may be tedious and the resulting expressions may become unwieldy, but in principle we know exactly what to do. This makes differentiation a perfect task for a computer and every computer algebra system will readily do this. Integration is much more difficult. Given an arbitrary function, it may not be immediately clear which rule should be applied in order to find an antiderivative. For many functions, an antiderivative that can be expressed as a closed form formula involving only elementary functions may not even exist. However, there is an algorithm that can compute any elementary antiderivative, if it exists or prove the nonexistence if it doesn't. That algorithm is the Risch algorithm and it is so complicated that, at the time of this writing, most computer algebra systems do not implement it in its full glory but rather in some simplified and less general form.

https://mathoverflow.net/questions/374089/does-there-exist-a-complete-implementation-of-the-risch-algorithm
https://mathematica.stackexchange.com/questions/140088/does-mathematica-implement-risch-algorithm-if-it-does-in-which-cases

-as example for computing a Riemman integral from the definiton, use f(x) = x^2 and integrate it from zero to some variable upper limit b
-define a sum S_N = \sum_{k=1}^N (k dx)^2 dx with dx defined as (b-a)/N with a=0, so it's just dx=b/N
-this leads to S_N = dx^3 \sum_k k^3
-using \sum_k k^3 = (2N^3+3N^2+N)/6 via Faulhaber's formula we can get rid of the sum
-this leads to an expression containing powers of N in the numerator and N^3 in the denominator
-when N -> inf only the leading term remains and we get \lim_{N -> inf} S_N = b^3/3
-this should just serve as a demonstartion that in princpile, it's possible to evaluate integrals from the definitions, using rules of limits and closed form sum formulas as an analogy how it is in princpile possible to evaluate derivatives using limits
-state that nobody actually evaluates integrals like that, just like with derivatives

Substitution:
https://en.wikipedia.org/wiki/Integration_by_substitution
https://de.wikipedia.org/wiki/Integration_durch_Substitution
https://de.wikipedia.org/wiki/Weierstra%C3%9F-Substitution
https://www.mathsisfun.com/calculus/integration-by-substitution.html
The Cycon Book has also a "2nd kind" of substitution, applicabel when the integrand is not of the form g'(x)*f(g(x))


\end{comment}