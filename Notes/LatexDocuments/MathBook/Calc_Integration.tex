\section{Integration} 
When mathematicians talk about \emph{integration}, what they mean is generally some process of aggregating or accumulating some stuff in the sense of summing it up into a sort of running total. But it's more complicated than a plain old sum because the thing that is integrated is usually not a discrete (ordered) list of numbers but something that is continuous in nature like a function $f: \mathbb{R} \rightarrow \mathbb{R}$. Such a 1D function is the simplest case of integration. In a more general setting, integration can also refer to things like computing a total mass of a 3D object given its density as function of the spatial $x,y,z$-coordinates or the total kinetic energy that is picked up by (or withdrawn from) a particle when it moves around in a force field. In any case, some continuous quantity is aggregated over space or over time or over some other independent variable(s). You may also hear the phrase "integrating a differential equation" which kind of means something similar, although I personally prefer to just call it "solving" rather than "integrating" in this context.

% Explain what the term "integration" means when applied to a differential equation

\subsection{The Riemann Integral} 
Imagine we have a function $y = f(x)$ and want to compute the area that is enclosed between some interval of the $x$-axis, say $[a,b]$, and the graph of the function over this interval. We could approach this problem by approximating the area by a bunch of rectangles as follows: we split the interval $[a,b]$ into some number $n$ of equally sized subintervals, for each subinterval compute the area of a rectangle where one side is given by the width of the subinterval and the other by the height of the function somewhere inside the subinterval, then add up all those areas of the rectangles. When the number of intervals $n$ grows, the width of the intervals shrinks, the rectangles become thinner and the approximation becomes better. In the limit, as $n$ approaches infinity, the approximation becomes exact. This area is also called the \emph{definite integral} of $f$ over the interval $[a,b]$. We could define the integral as follows:
\begin{equation}
\label{Eq:RiemannIntegral1D}	
 I = \int_a^b f(x) \, dx = \lim_{n \rightarrow \infty}  \sum_{i=1}^n f(x_i) \, \Delta x_i
\end{equation}
Here, $\Delta x_i$ is the width of the $i$th subinterval and $x_i$ is a value somewhere inside the $i$th subinterval. That means, the product $f(x_i)  \Delta x_i$ inside the sum can be intepreted as "height times width" where the height is given by the function value and the width by the subinterval length. This product is indeed an area of a rectangle. If we assume to break down the original interval $[a,b]$ into $n$ equally sized subintervals, we can compute the width as $\Delta x_i = (b-a)/n$. When $n$ gets large, each $\Delta x_i$ becomes small and the rectangles become narrow rectangular columns. The larger we make $n$, the more closely the sum of narrow rectangle areas approximates our actual desired area until in the limit, the approximation becomes exact.

\medskip
The given formula is not quite Riemman's exact definition, but it's close enough to capture the idea. Riemann does not require the intervals to be equally sized but only demands that the largest of them should shrink to a width of zero as $n$ approaches infinity. He also additionally demands that it should not matter, where exactly inside the $i$th interval we pick $x_i$ to be. It could be at the left or right boundary, right in the middle, at the lowest or highest point or whereever. The Riemann integral is only defined if it doesn't matter, i.e. if the above limit will always give the same result. If that isn't the case, the function is not considered to be Riemann-integrable. For continuous functions, it will always be the case, though.

\medskip
The interpretation of the Riemann integral as an area is actually a somewhat sloppy interpretation: it works only when we assume that the function graph is above the $x$-axis for the whole integration interval. If this is not the case, the portions below the $x$-axis will actually produce negative contributions to the total area because the $f(x_i)$ factors will be negative in this case. If the function is more below the $x$-axis than above, the "area" may even become negative. It is more useful to think about the integral as a sort of "signed area". If we divide the integral by the width of the integration interval, i.e. by $b-a$, we obtain an average value of the function over the interval $[a,b]$.

%\paragraph{The Lesbesgue Integral}

% maybe mention the Lesbesgue integral, too - maybe other generalizations (Riemman-Stieljes, etc.)
% but maybe it's better to save that for later - it would just interrupt the flow



\subsection{Antiderivatives and Indefinite Integrals}
We now want to consider a situation where the lower limit of the integration $a$ is fixed as before but the upper limit $b$ is taken to be a variable. In that case, the above expression actually defines a new function - namely, a function of $b$. Just like we have earlier "derived" a new function $f'(x)$ from our given $f(x)$ via the derivative, we can now derive another new function, let's call it $F$ from $f$ via the following operation:
\begin{equation}
F(b) = \int_a^b f(x) \, dx \qquad \text{or} \qquad
F(x) = \int_a^x f(u) \, du
\end{equation}
We can interpret this function $F(b)$ as a sort of "accumulated area" function where the argument $b$ determines, how far to the right we accumulate the area (starting at the left at our fixed $a$). Note that $b$ and $x$ are just dummies or placeholders. The left and right version say exactly the same thing and I have just chosen different names for the dummy variables in both expressions. The functions $f$ and $F$ and their relationship is of interest here - not how we choose to call their arguments. OK, so we have created a new function $F$ from our given function $f$. That new function $F$ is called an \emph{indefinite integral} or an \emph{antiderivative} of $f$. I deliberately said "an" antiderivative and not "the" antiderivative because it is not uniquely determined: we could pick a different fixed $a$ and would get another antiderivative. 
The reason for calling it an antiderivative is, as you may have guessed, its relation to the derivative. When we take \emph{the} derivative of \emph{an} antiderivative of a function, we get our original function back. Formally, \emph{the} indefinite integral of a function $f$ is not one single function but a whole set of functions: namely, the set of all those functions whose derivative is $f$. There are many functions that have the same derivative because adding a constant to a function does not affect its derivative. The indefinite integral of a function $f$ is written as an integral without integration limits and is defined formally as the following set:
\begin{equation}
 \int f(x) \, dx = 	\{ F(x) + C : F'(x) = f(x) \} = F(x) + C \qquad \text{where } C \in \mathbb{R}
\end{equation}
In this equation, the set builder notation in the middle is the formal definition and the right hand side is a less formal shorthand notation. The constant $C$ is called the integration constant. To relate that definition to our former expression with the variable upper integration limit $b$, we note that different choices of the lower integration limit $a$ would give rise to different integration constants $C$. Specifically, if we define $F_0(x)$ to be the particular antiderivative of $f$ with integration constant zero, then we would get $C = -F_0(a)$ in the general case. [VERIFY!]
% is it: C = F(a) or C = f(a) or C = a or something else? if a = b then F(b) = 0 when we choose C = 0...maybe C = F(b) - F(a)?
% Let f(x) = x^2, then F(x) = x^3/3 + C
% https://www.wolframalpha.com/input?i=integrate+x%5E2+dx+from+x%3Da+to+b
% gives int_a^b x^2 dx = b^3/3 - a^3/3
% so it seems that C = -F(a) 
% Explain interpretation of F as and "accumulated area" function

\subsection{The Fundamental Theorem of Calculus}
We have approached derivatives and integrals from geometric problems involving the graph of a function $f$: finding the slope of a tangent to the graph and finding the (signed) area between the graph and the $x$-axis. As it turns out, these two seemingly disparate questions are related to one another. Specifically, if $F$ is an antiderivative for $f$, then the following holds:
\begin{equation}
 \int_a^b f(x) \, dx = F(b) - F(a)
\end{equation}
With that theorem in hand, we can easily compute (signed) areas under the graph of a function $f$ provided that we know an antiderivative $F$ of $f$. We just need to evaluate our antiderivative at two points and subtract the results. To find such an antiderivative for some simple functions, we just need to read our table of derivatives backwards. For more complicated functions, there are certain laws that we can use. %Before we get to them, let's try to convince ourselves that this theorem is plausible. 

%...TBC...

% F being *an* antiderivative for f implies that f is *the* derivative of F. Given f, the antiderivative is determined only up to an additive constant. But given F, the derivative f is uniquely determined because the derivative of the constant is zero.

% give an intuitive understanding of why this should be true: if a function has a high value at the upper (i.e. right) integration limit, then shifting the right integration limit further to the right should increase the area under the graph faster than when it has a low value.

% Note how only the boundary values actually count - this will later generalize to other integral theorems

\subsection{Integration Laws} 
ToDo: Explain rules for linearity, reversal, splitting, ...

%\paragraph{Notations} 
% Maybe introduce a functional notation $I_a^b[f] = \int_a^b f(x) dx$ for definite integrals and an operator notation $I[f] = \int f(x) dx$ for indefinite integrals. It should be consistent with the notation used in the Operators section in the Functional Analysis chapter. The notation is more compact and allows the laws to be written down in smaller table. Also introduce the notation with the vertical bar to mean "evaluated at upper and lower limit, then subtracted"

\subsection{Improper Integrals} 
%-infinite integration limits
%-singularities within the integration interval


\subsection{Integration Techniques} 

\subsubsection{Integration by Parts} 
Write the function $f(x)$ to be integrated as a product $u(x)v(x)$ such that the remaining integral over $u' v$ in the formula is easier to evaluate than the original one. This usually means to choose a $u$ that simplifies under differentiation and/or to choose a $v'$ that simplifies under integration. The formula for a definite integral is:
\begin{equation}
  \int_a^b u(x) v'(x) \, dx = \Big[u(x) v(x)\Big]_a^b - \int_a^b u'(x) v(x) \, dx
\end{equation}
With $u = u(x), du = u'(x) dx, v = v(x), dv = v'(x) dx$, an indefinite integral can be written using the differentials $du, dv$ as:
\begin{equation}
  \int u \, dv \ =\ uv - \int v \, du
\end{equation}
The formulas can be derived from the product rule of differentiation.

\paragraph{Example} Let's find the indefinite integral of $f(x) = 2 x \ln x \, dx$ using $u' = 2 x, v = \ln x$ such that $u = x^2, v' = 1/x$ and $v' u = x^2/x = x$. Note that the roles of $u,v$ are reversed here. They are just dummy names and you'll find formulas using both conventions, so a bit of flexibility is good:
\begin{align}
  \int 2 x \ln x \, dx = x^2 \ln x - \int x \, dx = x^2 \ln x - \frac{x^2}{2} + C
\end{align}
...yeah, the $+ C$ has been neglected (i.e. taken to be zero) in the formulas above. It's more convenient to just add the integration constant at the very end of the calculation. In many cases, you will probably forget about it. This amounts to taking it to be zero which is often the most natural choice anyway. However, you should keep in the back of your head that you can always add an integration constant and in some cases (e.g. when solving differential equations), that constant actually has a role and matters.


%\newline % we want some vertical space - newline doe not really cut it
%See also:
%\href{https://en.wikipedia.org/wiki/Integration_by_parts}{wikipedia.org/wiki/Integration\_by\_parts}

% Applications: finding adjoint operators (move integral from one function to the other)
% https://www.youtube.com/watch?v=aG5tFA8GJ78 Linear Operators and their Adjoints (Nathan Kutz)

% is inversion of the product rule

% ToDo: Substitution (inversion of the chain rule), Logarithmic integration, integration of inverse?

\subsubsection{Integration by Substitution} 

\subsubsection{Integration by Partial Fraction Expansion} 



\subsubsection{The Risch Algorithm}
Finding an antiderivative for a given function is hard. Unlike for differentiation, for integration there is no simple mechanical set of rules that we can just blindly apply and be sure that it will always lead us to the result. Except: there actually is one - but with a caveat: it is not really suitable for execution by humans using pencil and paper. It's too complicated and messy for that. However, computers can deal with it easily, provided they have the appropriate software installed. This set of rules is embodied in the Risch algorithm and some variation of this algorithm is usually integrated into every computer algebra system. The Risch algorithm can be given an elementary function as input and if the function has an elementary antiderivative, the algorithm will find it and return it as output. If the algorithm didn't find an elementary antiderivative, then it has proven that none exists. Not all such systems implement the full algorithm and some may implement custom extensions (on top of the full or partial algo) to enable it to deal also with certain non-elementary functions, also kown as "special functions". When faced with some integral, my advice would be to reach for such a software. Wolfram's "Mathematica" is pretty good at integration and has a free web interface called "Wolfram Alpha" which is often good enough. If it doesn't succeed, you may have to look further, but that should probably be the first thing to try. I don't think, it implements the full Risch algo, though. On the other hand, it apparently implements some extensions that allow it to also deal with some special functions. The free and open source system "Axiom" does indeed implement the full Risch algo - or so I have heard. You may also want to try "SageMath" which, at the time of this writing and to the best of my knowledge, is currently the most extensive free and open source math software. Its more like an umbrella software that lets you access multiple other math software systems under the hood with a unified, Python-like syntax. Its computer algebra engine is by default "Maxima" but it can access others, too. Supposedly, Axiom is actually among them, but I couldn't get that to work yet. I did not yet try very hard, though. $:-|$

ToDo: give some example code to let SageMath and Wolfram Alpha calculate antiderivatives, try to figure out how Sage can be convinced to use the Axiom engine





%¯\_(ツ)_/¯

% maybe for the statement that Mathematica does not implement the full Risch algo, bring the example from Weitz's video at around 23 min:
% https://www.youtube.com/watch?v=l6w868U8C-M

%\medskip
%\paragraph{Personal Rant} In my humble personal opinion, I think nowadays with the widespread availability of computers and the Risch algorithm, it is pointless to try memorizing all these integration techniques and tricks. The problem has been solved for good - no need to torture students with that anymore. Maybe it's just as pointless as being able to calculate square roots with pencil and paper. Yes, it is absolutely possible and back in the day, it was the way to go - but for dog's sake, we now have computers for that kind of stuff. I don't consider the hand calculation of square roots to be an essential mathematical skill anymore and I tend to think similarly about manual integration techniques. In the mission statement, I said that one of the aims of this book is to give rather comprehensive collections of useful formulas. However, I will abstain from even trying to give a comprehensive list of known basic, elementary integrals. Such lists can fill dozens of pages and even whole books. In Germany, there's a running gag circulating that a function is said to be "Bronstein integrable" if its antiderivative can be found in "the Bronstein" which is a famous math reference book originally written by Ilja Nikolajevic - you guessed it - Bronstein. My copy has 32 pages of tables of antiderivatives. In the age of computers and the Risch algorithm, this would feel like listing tables of function values for the logarithm. Yes, such tables were a thing at some time, too. My Bronstein actually also still has such tables for some "special functions" like the Gamma- and Bessel-function. I felt great relief when I first learned about the existence of the Risch algorithm. Not one of my university professors even mentioned its existence. Ever. Not in math lectures, not in physics lectures, not in engineering lectures, not even in computer science lectures. Never. That was in the 2000s and the algo was known since the early 1970s and already had been implemented in a lot of computer algebra systems. 

%However, I do consider the ability to implement a square-root algorithm

%and tricks to find antiderivatives by hand

\subsubsection{Finding Definite Integrals}
The techniques explained above are for finding antiderivatives also known as indefinite integrals. Once you have found an indefinite integral, it is trivial to evaluate a corresponding definite integral by just evaluating your antiderivative at two points and subtracting the results. There are cases where all you want is a definite integral, i.e. basically just a number as opposed to a full-blown function. For this, going through the process of finding an antiderivative first might be overkill. It may sometimes even be impossible to find one, at least in terms of elementary functions. Yet, it may still be perfectly possible to give a neat value for the desired definite integral. One important technique to achieve this involves using the "residue theorem" which is one of the important results of complex analysis. We will see this later...

...TBC...what other techniques are there?


ToDo: give Sage and Mathematica code to calculate definite integrals - maybe some for which antiderivatives cannot be found

% Maybe use \int_{-\infty}^{\infty} e^{-x^2} = \sqrt{\pi} as example. Mention, that the erf-function ahs beend defined to be the antiderivative of the (normalized?) Gaussian bell function

% What about techniques to find definite integrals without having to find an antiderivative first? I think, going through the complex plane and using the residue theorem is a way to do this.

\subsection{The Lebesgue Integral} 
Most functions that we will typically encounter in applications are indeed Riemann-integrable, so the definition of the Riemann integral is all we need to integrate them. However, in certain areas of mathematics, some more exotic functions pop up which are not Riemann integrable. An example is the Dirichlet function which is defined to be $1$ at the rationals and $0$ at the irrationals. But not all is lost: a more powerful (albeit more complicated) definition of an integral exists under which some of these wilder functions become integrable. This is the Lebesgue integral. For Riemann-integrable functions, it produces the same results as the Riemann integral but it can be applied to a wider range of functions. Some functions that are not Riemann-integrable are Lebesgue-integrable. The general idea is to not split the integration interval on the $x$-axis into many subintervals but instead split the $y$-axis into intervals and add up horizontal slabs rather than vertical columns. To define the Lebesgue integral formally, we will first need to define the notion of a measure.

%https://en.wikipedia.org/wiki/Lebesgue_integration

\subsubsection{Measure Theory}
Let's assume, we have a given set $A$ such as $\mathbb{R}^n$ for some positive natural number $n$. Informally, a measure on $A$ is a function, typically denoted as $\mu(X)$, into which we can plug in a subset $X$ of $A$ as input and it spits out a nonnegative real number or infinity as output. A first attempt to formalize it could look like: $\mu: \mathcal{P}(A) \rightarrow \mathbb{R}^+_0 \cup \{  \infty \}$ where $\mathcal{P}(A)$ denotes the power set of $A$, i.e. the set of all subsets of A. However, that is not yet quite right. The domain is actually not the set of \emph{all} subsets of $A$, but a set of subsets with some constraints. These constraints are encapsulated in the definition of a so called $\sigma$-algebra. With this definition in place, we will be able to state the actual formal definition of what a measure is.

\paragraph{Definition: $\sigma$-Algebra} Given a set $A$, a $\sigma$-algebra $\Sigma$ on $A$ is a set of subsets of $A$ such that: (1) $\Sigma$ contains $A$ itself, (2) $\Sigma$ is closed under taking the complement, (3) $\Sigma$ is closed under taking countable unions. From this definition, it follows that $\emptyset$ is also in $\Sigma$ and that $\Sigma$ is closed under taking countable intersections, too.

\paragraph{Definition: Measure} Given a set $A$ and a $\sigma$-algebra $\Sigma$ on $A$, a measure $\mu$ on $A$ is a function $\mu: \Sigma \rightarrow \mathbb{R}^+_0 \cup \{  \infty \}$ such that: (1) $\mu$ is non-negative: $\forall X: \mu(X) \geq 0$, (2) $\mu$ assigns zero to the empty set: $\mu(\emptyset) = 0$, (3) $\mu$ is countably additive aka $\sigma$-additive:
\begin{equation}
\mu \left( \bigcup_{k=1}^{\infty} X_k \right) = \sum_{k=1}^{\infty}	\mu(X_k)
\end{equation}
for any countable collection $\{X_k\}_{k=1}^{\infty}$ of pairwise disjoint sets $X_k$ where each such $X_k$ is a subset (or element? FIGURE OUT!) of $\Sigma$.

...TBC...

% $\Cup_{k=1}^{\infty} = \sum_{k=1}^{\infty}$

% Outer mu_o(A) vs inner measure mu_i(A) vs measure mu(A):
% -outer measure is defined for all subsets of X whereas inner measure only for measurable subsets
% -outer measure corresponds to upper and inner to lower Riemann sums
% -outer measure is countably subadditive, inner measure (countably?) superadditive
% -measure without qualifier is countably additive

%https://math.stackexchange.com/questions/714193/what-is-the-difference-between-outer-measure-and-inner-measure

%https://www.quora.com/What-is-the-intuition-behind-outer-and-inner-measure-of-a-set-How-can-you-show-that-measure-of-2-3-on-R-is-1-only-using-definition-without-using-the-theorem-that-measure-equals-length

%https://en.wikipedia.org/wiki/Outer_measure
%https://en.wikipedia.org/wiki/Inner_measure
%https://en.wikipedia.org/wiki/Measure_(mathematics)
%https://en.wikipedia.org/wiki/%CE%A3-algebra



\begin{comment}

Differentiation is a purely mechanic process: given the list of elemenary derivatives and the differentiation rules, we can easily compute an expression for the derivative of any function. In may be tedious and the resulting expressions may become unwieldy, but in principle we know exactly what to do. This makes differentiation a perfect task for a computer and every computer algebra system will readily do this. Integration is much more difficult. Given an arbitrary function, it may not be immediately clear which rule should be applied in order to find an antiderivative. For many functions, an antiderivative that can be expressed as a closed form formula involving only elementary functions may not even exist. However, there is an algorithm that can compute any elementary antiderivative, if it exists or prove the nonexistence if it doesn't. That algorithm is the Risch algorithm and it is so complicated that, at the time of this writing, most computer algebra systems do not implement it in its full glory but rather in some simplified and less general form.

https://mathoverflow.net/questions/374089/does-there-exist-a-complete-implementation-of-the-risch-algorithm
https://mathematica.stackexchange.com/questions/140088/does-mathematica-implement-risch-algorithm-if-it-does-in-which-cases

-as example for computing a Riemman integral from the definiton, use f(x) = x^2 and integrate it from zero to some variable upper limit b
-define a sum S_N = \sum_{k=1}^N (k dx)^2 dx with dx defined as (b-a)/N with a=0, so it's just dx=b/N
-this leads to S_N = dx^3 \sum_k k^3
-using \sum_k k^3 = (2N^3+3N^2+N)/6 via Faulhaber's formula we can get rid of the sum
-this leads to an expression containing powers of N in the numerator and N^3 in the denominator
-when N -> inf only the leading term remains and we get \lim_{N -> inf} S_N = b^3/3
-this should just serve as a demonstartion that in princpile, it's possible to evaluate integrals from the definitions, using rules of limits and closed form sum formulas as an analogy how it is in princpile possible to evaluate derivatives using limits
-state that nobody actually evaluates integrals like that, just like with derivatives

Substitution:
https://en.wikipedia.org/wiki/Integration_by_substitution
https://de.wikipedia.org/wiki/Integration_durch_Substitution
https://de.wikipedia.org/wiki/Weierstra%C3%9F-Substitution
https://www.mathsisfun.com/calculus/integration-by-substitution.html
The Cycon Book has also a "2nd kind" of substitution, applicabel when the integrand is not of the form g'(x)*f(g(x))


\end{comment}