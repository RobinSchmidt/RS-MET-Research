\section{Integration} 
When mathematicians talk about \emph{integration}, what they mean is generally some process of aggregating or accumulating some stuff in the sense of summing it up into a sort of running total. But it's more complicated than a plain old sum because the thing that is integrated is usually not a discrete (ordered) list of numbers but something that is continuous in nature like a function $f: \mathbb{R} \rightarrow \mathbb{R}$. Such a 1D function is the simplest case of integration. In a more general setting, integration can also refer to things like computing a total mass of a 3D object given its density as function of the spatial $x,y,z$-coordinates or the total kinetic energy that is picked up by (or withdrawn from) a particle when it moves around in a force field. In any case, some continuous quantity is aggregated over space or over time or over some other independent variable(s). You may also hear the phrase "integrating a differential equation" which kind of means something similar, although I personally prefer to just call it "solving" rather than "integrating" in this context.

% Explain what the term "integration" means when applied to a differential equation

\subsection{The Riemann Integral} 
Imagine we have a function $y = f(x)$ and want to compute the area that is enclosed between some interval of the $x$-axis, say $[a,b]$, and the graph of the function over this interval. For the time being, imagine that the graph of $f(x)$ lies above the $x$-axis everywhere. We could approach this problem by approximating the area by a bunch of rectangles as follows: we split the interval $[a,b]$ into some number $n$ of equally sized subintervals, for each subinterval compute the area of a rectangle where one side is given by the width of the subinterval and the other by the height of the function somewhere inside the subinterval, then add up all those areas of the rectangles. When the number of intervals $n$ grows, the width of the intervals shrinks, the rectangles become thinner and the approximation becomes better. In the limit, as $n$ approaches infinity, the approximation becomes exact. This area is also called the \emph{definite integral} of $f$ over the interval $[a,b]$. We could define the integral as follows:
\begin{equation}
\label{Eq:RiemannIntegral1D}	
 I = \int_a^b f(x) \, dx = \lim_{n \rightarrow \infty}  \sum_{i=1}^n f(x_i) \, \Delta x_i
\end{equation}
In this notation, the $\int$ symbol is called the integral sign and it is meant to be an elongated letter $S$ for "sum"\footnote{Recall that the $\Sigma$ in the sum notation also stands for an $S$. It's an uppercase sigma which is the greek version of the latin letter s.}. This notation was invented by Leibniz. In the RHS of the formula, $\Delta x_i$ is the width of the $i$th subinterval and $x_i$ is a value somewhere inside the $i$th subinterval. That means, the product $f(x_i)  \Delta x_i$ inside the sum can be intepreted as "height times width" where the height is given by the function value and the width by the subinterval length. This product is indeed an area of a rectangle. If we assume to break down the original interval $[a,b]$ into $n$ equally sized subintervals, we can compute the width as $\Delta x_i = (b-a)/n$. When $n$ gets large, each $\Delta x_i$ becomes small and the rectangles become narrow rectangular columns. The larger we make $n$, the more closely the sum of narrow rectangle areas approximates our actual desired area until in the limit, the approximation becomes exact.
% maybe use \xi_i instead of x_i. That's common in math books

\medskip
The given description is not quite Riemann's exact definition, but it's close enough to capture the idea. Riemann does not require the intervals to be equally sized but only demands that the largest of them should shrink to a width of zero as $n$ approaches infinity. He also additionally demands that it should not matter, where exactly inside the $i$th interval we pick $x_i$ to be. It could be at the left or right boundary, right in the middle, at the lowest or highest point\footnote{More strictly speaking, one would use the infimum and supremum for the lowest and highest $y$-value within the subinterval - but you may ignore that technical detail.} or whereever. The Riemann integral is only defined if it doesn't matter, i.e. if the above limit will always give the same result. If that isn't the case, the function is not considered to be Riemann-integrable. For continuous functions, it will always be the case, though.

% Your Calc Teacher SPARED YOU: Here's the REAL Definition of the Integral
% https://www.youtube.com/watch?v=cIqcGM_MTTg
% -Gives the formal definition in terms of the lower and upper sums (using the infimum and 
%  supremumu in each interval)

\medskip
The interpretation of the Riemann integral as an area is actually a somewhat sloppy interpretation: it works only when we assume that the function graph is above the $x$-axis for the whole integration interval. If this is not the case, the portions below the $x$-axis will actually produce negative contributions to the total area because the $f(x_i)$ factors will be negative in this case (the $\Delta x_i$ factors are always positive by construction). If the function has more "weight" below the $x$-axis than above, the "area" may even become negative. It is more useful to think about the integral as a sort of \emph{"signed area"} or an \emph{"area balance"} over the interval $[a,b]$. If we divide the value $I$ of the integral by the width of the integration interval, i.e. by $b-a$, we obtain an \emph{average} value of the function over the interval $[a,b]$. This average would be the unique value $c$ that a constant function would need to have, such that the (signed) area of the rectangle with width $b-a$ and height $c$ gives rise to the same signed area as the integral over $f$.

%\paragraph{The Lesbesgue Integral}

% maybe mention the Lesbesgue integral, too - maybe other generalizations (Riemman-Stieljes, etc.)
% but maybe it's better to save that for later - it would just interrupt the flow



\subsection{Antiderivatives and Indefinite Integrals}
We now want to consider a situation where the lower limit $a$ of the integration is fixed as before but the upper limit $b$ is taken to be a variable. In that case, the above expression actually doesn't define a mere real number anymore. Instead, it defines a new function - namely, a function of $b$. Just like we have earlier "derived" a new function $f'(x)$ from our given $f(x)$ via the derivative, we can now derive another new function, let's call it $F$ from $f$ via the following operation:
\begin{equation}
F(b) = \int_a^b f(x) \, dx \qquad \text{or} \qquad
F(x) = \int_a^x f(u) \, du
\end{equation}
We can interpret this function $F(b)$ as a sort of "accumulated area" function where the argument $b$ determines, how far to the right we accumulate the area (starting at the left at our fixed $a$). Note that $b$ and $x$ are just dummies or placeholders. The left and right version say exactly the same thing and I have just chosen different names for the dummy variables in both expressions. The functions $f$ and $F$ and their relationship is of interest here - not how we choose to call their arguments. OK, so we have created a new function $F$ from our given function $f$. That new function $F$ is called an \emph{indefinite integral} or an \emph{antiderivative} of $f$. I deliberately said "an" antiderivative and not "the" antiderivative because it is not uniquely determined: we could pick a different fixed $a$ and would get another antiderivative. 
The reason for calling it an antiderivative is, as you may have guessed, its relation to the derivative. When we take \emph{the} derivative of \emph{an} antiderivative of a function, we get our original function back. Formally, \emph{the} indefinite integral of a function $f$ is not one single function but a whole set of functions: namely, the set of all those functions whose derivative is $f$. There are many functions that have the same derivative because adding a constant to a function does not affect its derivative. The indefinite integral of a function $f$ is written as an integral without integration limits and is defined formally as the following set:
\begin{equation}
 \int f(x) \, dx = 	\{ F(x) + C : F'(x) = f(x) \} = F(x) + C \qquad \text{where } C \in \mathbb{R}
\end{equation}
In this equation, the set builder notation in the middle is the formal definition and the right hand side is a less formal shorthand notation. The constant $C$ is called the integration constant. To relate that definition to our former expression with the variable upper integration limit $b$, we note that different choices of the lower integration limit $a$ would give rise to different integration constants $C$. Specifically, if we define $F_0(x)$ to be the particular antiderivative of $f$ with integration constant zero, then we would get $C = -F_0(a)$ in the general case. [VERIFY! See Arens, pg 388, beware of $c$ occurring on the other side of the equation there]


%Specifically, we get $C = -F(a)$. [VERIFY, explain why - see Arens, pg. 388]

%
% is it: C = F(a) or C = f(a) or C = a or something else? if a = b then F(b) = 0 when we choose C = 0...maybe C = F(b) - F(a)?
% Let f(x) = x^2, then F(x) = x^3/3 + C
% https://www.wolframalpha.com/input?i=integrate+x%5E2+dx+from+x%3Da+to+b
% gives int_a^b x^2 dx = b^3/3 - a^3/3
% so it seems that C = -F(a) 
% Explain interpretation of F as and "accumulated area" function

\subsection{The Fundamental Theorem of Calculus}
We have approached derivatives and integrals from geometric problems involving the graph of a function $f$: finding the slope of a tangent to the graph and finding the (signed) area between the graph and the $x$-axis. As it turns out, these two seemingly disparate questions are related to one another. Specifically, if $F$ is an antiderivative of $f$, i.e. $f$ is the derivative of $F$: $f = F'$, then the following holds:
\begin{equation}
 \int_a^b f(x) \, dx = F(b) - F(a)
\end{equation}
This relationship is called the \emph{fundamental theorem of calculus}. As the name suggests, it's very important. It's sometimes abbreviated as FTC. For the difference of $F$ evaluated at the endpoints $a,b$ that appears on the right hand side, the following notations are common:
\begin{equation}
 F(b) - F(a) = \Big[ F(x) \Big]_a^b = F(x) \bigg\rvert_a^b
\end{equation}
This notation is useful when instead of the symbolic $F(x)$, we deal with a written out and possibly complicated expression for $F(x)$, like e.g. $x + e^x \cos(2 x^2 + \pi x /2 )$, because then we don't need to write it out twice. With the fundamental theorem of calculus in hand, we can easily compute (signed) areas under the graph of a function $f$ over an interval $[a,b]$ provided that we know an antiderivative $F$ of $f$. We just need to evaluate our antiderivative $F$ at two points $b$ and $a$ and subtract the results. To find such an antiderivative for some simple functions, we just need to read our table of derivatives backwards. For more complicated functions, there are certain laws that we can use. %Before we get to them, let's try to convince ourselves that this theorem is plausible. 

% I think, it's called an integral bar - or evaluation bar?
% https://math.stackexchange.com/questions/52651/what-is-the-name-of-the-vertical-bar-in-x21-vert-x-4-or-left-left

%...TBC...

% F being *an* antiderivative for f implies that f is *the* derivative of F. Given f, the antiderivative is determined only up to an additive constant. But given F, the derivative f is uniquely determined because the derivative of the constant is zero.

% give an intuitive understanding of why this should be true: if a function has a high value at the upper (i.e. right) integration limit, then shifting the right integration limit further to the right should increase the area under the graph faster than when it has a low value.

% Note how only the boundary values actually count - this will later generalize to other integral theorems

% Introduce the two notations with the brackets and the vertical line for F(b) - F(a)

% Maybe write the theorem also as
% \int_a^b F'(x) = F(b) - F(a)   or   \int_a^b f'(x) = f(b) - f(a)
% the 1st follows directly from our assumption f = F' and the 2nd is just a matter of renaming. F and f are just dummy letters anyway.


\subsection{Integration Laws} 
ToDo: Explain rules for linearity, reversal, splitting, ...

% https://www.magnamathematica.com/2024/04/23/kings-rule/
% \int_a^b f(x) dx = \int_a^b f(a+b-x) dx
% -"is especially powerful when combined with trigonometric identities."

%\paragraph{Notations} 
% Maybe introduce a functional notation $I_a^b[f] = \int_a^b f(x) dx$ for definite integrals and an operator notation $I[f] = \int f(x) dx$ for indefinite integrals. It should be consistent with the notation used in the Operators section in the Functional Analysis chapter. The notation is more compact and allows the laws to be written down in smaller table. Also introduce the notation with the vertical bar to mean "evaluated at upper and lower limit, then subtracted"

%\subsection{Improper Integrals} 
%-infinite integration limits
%-singularities within the integration interval



% https://www.youtube.com/watch?v=vmt5Zb4GSes  Integration Is Easy, Actually


% https://www.youtube.com/watch?v=2dwQUUDt5Is  What Lies Between a Function and Its Derivative? | Fractional Calculus
% at 4:50 Cauchy's formula for repeated integration

% Maybe call this section here "Basic Integration Laws" and make a later section with
% "More Advanced Integration Laws" which could also include variuos tricks. There is something
% about integrating theinverse function. IIRC
%
% See:
%
% https://www.youtube.com/watch?v=4vbXadolbOE  
% The Most Overlooked Concept in Calculus | Calculus of Inverse Functions
%
% https://www.youtube.com/watch?v=TKyOsHqAxf0
% Integral of Inverse Functions

\subsection{Integration Techniques} 

\subsubsection{Integration by Parts} 
Write the function $f(x)$ to be integrated as a product $u(x)v(x)$ such that the remaining integral over $u' v$ in the formula is easier to evaluate than the original one. This usually means to choose a $u$ that simplifies under differentiation and/or to choose a $v'$ that simplifies under integration. The formula for a definite integral is:
\begin{equation}
  \int_a^b u(x) v'(x) \, dx = \Big[u(x) v(x)\Big]_a^b - \int_a^b u'(x) v(x) \, dx
\end{equation}
The formulas can be derived from the product rule of differentiation. Another way of writing this rule that is not commonly seen but I consider more instructive is obtained by simultaneously replacing $v'$ by $v$ and $v$ by $V$ where it is understood that $V$ means an antiderivative of $v$:
\begin{equation}
  \int_a^b u(x) v(x) \, dx = \Big[u(x) V(x)\Big]_a^b - \int_a^b u'(x) V(x) \, dx
\end{equation}
Writing it this way, the formula tells us more directly what we have to do when we want to integrate a product of two given functions $u(x), v(x)$: we have to differentiate $u$ and integrate $v$. Therefore, when we see a function that is built as a product of other functions, we should look out for a factor that is easy to differentiate (and call it $u$) and a factor that is easy to integrate (and call it $v$ - or $v'$ if you prefer the first notation) - and then use the rule. The factors $u,v$ should be chosen in such a way that $u'(x) V(x)$ is easier to integrate than the original product $u(x) v(x)$. Recognizing how to split a given function into such factors that make usage of this rule worthwhile is one of the ingredients of the black art of finding integrals. There is yet another way to state the rule: with $u = u(x), du = u'(x) dx, v = v(x), dv = v'(x) dx$, an indefinite integral can also be written using the differentials $du, dv$ as:
\begin{equation}
  \int u \, dv \ =\ uv - \int v \, du
\end{equation}
TODO: explain that notation in more detail - does it work also for definite integrals? Explain differentials! TODO: give some heuristics for assigning $u$ and $v$. 

%Maybe give an (not too simple) example.

% ToDo: maybe formulate it without using primes. Instead use $u,v$ in the LHS and $u,U,v,V$ in the RHS. Explain the notation with the big brackets for evaluating at the endpoints and subtracting. Or maybe use the notation with the right vertical line

\paragraph{Example} Let's find the indefinite integral of $f(x) = 2 x \ln x \, dx$ using $u' = 2 x, v = \ln x$ such that $u = x^2, v' = 1/x$ and $v' u = x^2/x = x$. Note that the roles of $u,v$ are reversed here. They are just dummy names and you'll find formulas using both conventions, so a bit of flexibility is good:
\begin{align}
  \int 2 x \ln x \, dx = x^2 \ln x - \int x \, dx = x^2 \ln x - \frac{x^2}{2} + C
\end{align}
...yeah, the $+ C$ has been neglected (i.e. taken to be zero) in the formulas above. It's more convenient to just add the integration constant at the very end of the calculation. In many cases, you will probably forget about it completely and often that's fine. It amounts to taking it to be zero which is often the most natural choice anyway. However, you should keep in the back of your head that you can always add an integration constant and in some cases (e.g. when solving differential equations), that constant actually has a role and matters.
TODO: maybe use the 2nd notation (we currently use the 1st)


%\newline % we want some vertical space - newline doe not really cut it
%See also:
%\href{https://en.wikipedia.org/wiki/Integration_by_parts}{wikipedia.org/wiki/Integration\_by\_parts}

% Applications: finding adjoint operators (move integral from one function to the other)
% https://www.youtube.com/watch?v=aG5tFA8GJ78 Linear Operators and their Adjoints (Nathan Kutz)

% is inversion of the product rule

% ToDo: Substitution (inversion of the chain rule), Logarithmic integration, integration of inverse?

\subsubsection{Integration by Substitution} 



% These Four Calculus Rules are Actually All the Same.
% https://www.youtube.com/watch?v=QIXnY7o-XDg


\subsubsection{Integration by Partial Fraction Expansion} 
Partial fraction expansion is often listed as an integration technique and therefore taught in the context of integration. I personally think that partial fraction expansion itself fits better into a general section about the algebra of rational functions. Integration \emph{by} partial fraction expansion is then just usage of that purely algebraic technique in the context of integration - but not an integration technique per se. In fact, partial fraction expansions can be useful in several other contexts. But be that as it may - here is how it works: we assume that we are dealing with a rational function, i.e. a function defined by a quotient of polynomials. ...TBC...


%This technique is often listed as an "integration technique" although it is at it core just an algebraic transformation

% What else is there? Logarithmice integration? Intgration of the inverse?

\subsubsection{Integration of Inverse Functions}
\begin{equation}
\int f^{-1}(x) dx = x f^{-1}(x) - F( f^{-1}(x) ) + C
\end{equation}
ToDo: explain by example, maybe start with a rule definite integrals

% https://en.wikipedia.org/wiki/Integral_of_inverse_functions
% https://mathworld.wolfram.com/InverseFunctionIntegration.html

% The Most Overlooked Concept in Calculus - Calculus of Inverse Functions
% https://www.youtube.com/watch?v=4vbXadolbOE
% 5:09: inverse rule for derivatives
% 6:30: inverse rule for integrals


% https://www.youtube.com/watch?v=TKyOsHqAxf0
% Integral of Inverse Functions

\subsubsection{The Risch Algorithm}
Finding an antiderivative for a given function is hard. Unlike for differentiation, for integration there is no simple mechanical set of rules that we can just blindly apply on autopilot and be sure that it will always lead us to the result. Except: there actually is one - but with a caveat: it is not really suitable for execution by humans using pencil and paper. It's too complicated and messy for that. However, computers can deal with it just fine, provided they have the appropriate software installed. This set of rules is embodied in the Risch algorithm and some variation of this algorithm is usually integrated into every computer algebra system. The Risch algorithm takes an elementary function as input and if the function has an elementary antiderivative, the algorithm will find it and return it as output. If the algorithm didn't find an elementary antiderivative, then it has proven that none exists. Not all such systems implement the full algorithm and some may implement custom extensions (on top of the full or partial algo) to enable it to deal also with certain non-elementary functions, also kown as "special functions". When faced with some integral, my advice would be to reach for such a software. Wolfram's "Mathematica" is pretty good at integration and has a free web interface called "Wolfram Alpha" which is often good enough. If it doesn't succeed, you may have to look further, but that should probably be the first thing to try. I don't think, it implements the full Risch algo, though. On the other hand, it apparently implements some extensions that allow it to also deal with some special functions. The free and open source system "Axiom" does indeed implement the full Risch algo - or so I have heard. You may also want to try "SageMath" which, at the time of this writing and to the best of my knowledge, is currently the most extensive free and open source math software. Its more like an umbrella software that lets you access multiple other math software systems under the hood with a unified, Python-like syntax. Its computer algebra engine is by default "Maxima" but it can access others, too. Supposedly, Axiom is actually among them, but I couldn't get that to work yet. I did not yet try very hard, though. $:-|$

TODO: give some example code to let SageMath and Wolfram Alpha calculate antiderivatives, try to figure out how Sage can be convinced to use the Axiom engine





%¯\_(ツ)_/¯

% maybe for the statement that Mathematica does not implement the full Risch algo, bring the example from Weitz's video at around 23 min:
% https://www.youtube.com/watch?v=l6w868U8C-M

%\medskip
%\paragraph{Personal Rant} In my humble personal opinion, I think nowadays with the widespread availability of computers and the Risch algorithm, it is pointless to try memorizing all these integration techniques and tricks. The problem has been solved for good - no need to torture students with that anymore. Maybe it's just as pointless as being able to calculate square roots with pencil and paper. Yes, it is absolutely possible and back in the day, it was the way to go - but for dog's sake, we now have computers for that kind of stuff. I don't consider the hand calculation of square roots to be an essential mathematical skill anymore and I tend to think similarly about manual integration techniques. In the mission statement, I said that one of the aims of this book is to give rather comprehensive collections of useful formulas. However, I will abstain from even trying to give a comprehensive list of known basic, elementary integrals. Such lists can fill dozens of pages and even whole books. In Germany, there's a running gag circulating that a function is said to be "Bronstein integrable" if its antiderivative can be found in "the Bronstein" which is a famous math reference book originally written by Ilja Nikolajevic - you guessed it - Bronstein. My copy has 32 pages of tables of antiderivatives. In the age of computers and the Risch algorithm, this would feel like listing tables of function values for the logarithm. Yes, such tables were a thing at some time, too. My Bronstein actually also still has such tables for some "special functions" like the Gamma- and Bessel-function. I felt great relief when I first learned about the existence of the Risch algorithm. Not one of my university professors even mentioned its existence. Ever. Not in math lectures, not in physics lectures, not in engineering lectures, not even in computer science lectures. Never. That was in the 2000s and the algo was known since the early 1970s and already had been implemented in a lot of computer algebra systems. 

%However, I do consider the ability to implement a square-root algorithm

%and tricks to find antiderivatives by hand

%---------------------------------------------------------------------------------------------------
\subsubsection{Techniques for Definite Integrals}
The techniques explained above are for finding antiderivatives also known as indefinite integrals. Once you have found an indefinite integral, it is trivial to evaluate a corresponding definite integral by just evaluating your antiderivative at two points and subtracting the results. There are cases where all you want is a definite integral, i.e. basically just a number as opposed to a full-blown function. For this, going through the process of finding an antiderivative first might be overkill. It may sometimes even be impossible to find one, at least in terms of elementary functions. Yet, it may still be perfectly possible to give a neat value for the desired definite integral. One important technique to achieve this involves using the "residue theorem" which is one of the important results of complex analysis. We will see this later...TBC...what other techniques are there?


\paragraph{Improper Integrals}
Improper integrals are definite integrals in which one or both of the integration limits are infinity or there may be points inside the integration interval where the integrand is undefined. In the former case, there are some subcases to consider. It may be the case the the lower or upper limit is finite and the other limit is infinite. In both cases, the value of the integral is defined by replacing the infinite limit by some variable and then considering the limit as this variable approaches infinity:
\begin{equation}
\int_a^{\infty} f(x) \; dx = \lim_{b \rightarrow \infty} \int_a^b f(x) \; dx
\qquad \text{or} \qquad
\int_{-\infty}^b f(x) \; dx = \lim_{a \rightarrow -\infty} \int_a^b f(x) \; dx
\end{equation}
It may also be the case, that both integration limits are infinite. In such cases, it is possible to split the integral at some intermediate point into two integrals of the type above:
\begin{equation}
\int_{-\infty}^{\infty} f(x) \; dx = \int_{-\infty}^c f(x) \; dx + \int_c^{\infty} f(x) \; dx
\end{equation}
which can each be evaluated separately via the limit technique. It can also be evaluated as a double limit:
\begin{equation}
\int_{-\infty}^{\infty} f(x) \; dx = \lim_{a \rightarrow -\infty} \lim_{b \rightarrow \infty} \int_a^b f(x) \; dx
\end{equation}
[Q: Does it matter in which order the limits are taken? It better should not!], [TODO: explain improper integrals with singularities at finite values]


% https://en.wikipedia.org/wiki/Improper_integral

% A limitation of the technique of improper integration is that the limit must be taken with respect to one endpoint at a time. Thus, for instance, an improper integral of the form

% It may also be the case that the undefined point occurs somewher in the middle of the interval. In such a case, one would just split the intgral into two at that critical point 

\paragraph{Cauchy's Principal Value}
We have just seen how an improper integral of the form $\int_{-\infty}^{\infty} f(x) \; dx$ can sometimes be evaluated by splitting it into two and adding the results:  $\int_{-\infty}^{\infty} = \int_{-\infty}^c + \int_c^{\infty}$ . But what if the two summands diverge in opposite directions such that we end up with an indeterminate form of the type $\infty - \infty$? In such a case, we say that the \emph{Cauchy principal value} of the integral is given by:
\begin{equation}
PV \int_{-\infty}^{\infty} f(x) \; dx = \lim_{a \rightarrow \infty} \int_{-a}^a f(x) \; dx
\end{equation}
That is: we pick a single parameter $a$, derive an expression for the definite integral in the symmetric interval from $-a$ to $+a$ and then take the limit of that expression as $a$ approaches infinity. ....TBC...verify that!

% https://en.wikipedia.org/wiki/Cauchy_principal_value
% https://de.wikipedia.org/wiki/Cauchyscher_Hauptwert
% https://mathworld.wolfram.com/CauchyPrincipalValue.html


\paragraph{Glasser's Master Theorem} tells us something about the definite integral of a function $f: \mathbb{R} \rightarrow \mathbb{R}$ over the whole real number line. A nice special case is this formulation:
\begin{equation}
 \int_{-\infty}^{\infty} f(x) \; dx  = 
 \int_{-\infty}^{\infty} f(x - \frac{a}{x}) \; dx 
 \qquad \forall a > 0
\end{equation}
The theorem applies if the integral converges absolutely. If necessary, it can be interpreted in the sense of a Cauchy principal value. The special case above can be generalized as follows:
\begin{equation}
 \int_{-\infty}^{\infty} f(x) \; dx  = 
 \int_{-\infty}^{\infty} f \left( x - \sum_{n=1}^N \frac{a_n}{x - b_n} - c \right) \; dx 
 \qquad \text{if } \forall n: a_n > 0
\end{equation}
which is the theorem proper. Here $a_n, b_n, c$ are all real numbers. The special case can recovered from the general one by setting $N=1, a_1=a, b_1=0, c=0$. See \cite{WK_GlasserMaster} for more information. This identity can be useful for solving integrals in which....TBC...
[VERIFY!]



% https://www.youtube.com/watch?v=yilsH_GrscM

% https://en.wikipedia.org/wiki/Glasser%27s_master_theorem
% https://mathworld.wolfram.com/GlassersMasterTheorem.html
% The equations on wikipedia and wolfram are different! On wikipedia, a is subtracted and on wolfram, x is multiplied by a. What is going on? Is one of them wrong?

% applicable when the integral converges absolutely. 





% Maybe use \int_{-\infty}^{\infty} e^{-x^2} = \sqrt{\pi} as example. Mention, that the erf-function has been defined to be the antiderivative of the (normalized?) Gaussian bell function

% What about techniques to find definite integrals without having to find an antiderivative first? I think, going through the complex plane and using the residue theorem is a way to do this.



\paragraph{Using Computer Algebra Systems}

TODO: give Sage and Mathematica code to calculate definite integrals - maybe some for which antiderivatives cannot be found


%---------------------------------------------------------------------------------------------------
\subsubsection{Non Riemann-Integrable Functions} ToDo: explain why the Dirichlet-function is not Riemann-integrable. Maybe give a couple of more examples - preferably some with relevance in applications, i.e. some not so contrived examples.



%===================================================================================================
\subsection{The Lebesgue Integral} 
Most functions that we will typically encounter in applications are indeed Riemann-integrable, so the definition of the Riemann integral is all we need to integrate them. However, in certain areas of mathematics, some more exotic functions pop up which are not Riemann integrable. An example is the Dirichlet function which is defined to be $1$ at the rationals and $0$ at the irrationals. But not all is lost: a more powerful (albeit more complicated) definition of an integral exists under which some of these wilder functions become integrable. This is the Lebesgue integral. For Riemann-integrable functions, it produces the same results as the Riemann integral but it can be applied to a wider range of functions. Some functions that are not Riemann-integrable are Lebesgue-integrable. The general idea is to not split the integration interval on the $x$-axis into many subintervals but instead split the $y$-axis into intervals and add up horizontal slabs rather than vertical columns. To define the Lebesgue integral formally, we will first need to define the notion of a measure.

%https://en.wikipedia.org/wiki/Lebesgue_integration

\subsubsection{Measures}
Let's assume, we have a given set $A$. For the time being, you may think of $A = \mathbb{R}^n$ for some positive natural number $n$ as an example. Informally, a measure on $A$ is a function, typically denoted as $\mu(X)$, into which we can plug in a subset $X$ of $A$ as input and it spits out a nonnegative real number or infinity as output. A first attempt to formalize it could look like: $\mu: \mathcal{P}(A) \rightarrow \mathbb{R}^+_0 \cup \{  \infty \}$ where $\mathcal{P}(A)$ denotes the power set of $A$, i.e. the set of all subsets of A. However, the actual definition is slightly more complicated and more general. The domain of $\mu$ is not necessarily the set of \emph{all} subsets of $A$, although it could be. Instead, it is some set of subsets that satisfies some constraints. These constraints are encapsulated in the definition of a so called $\sigma$-algebra. With this definition in place, we will be able to state the actual formal definition of what a measure is.

\paragraph{Definition: $\sigma$-Algebra} \label{Def:SigmaAlgebra} Given a set $A$, a $\sigma$-algebra $\Sigma$ on $A$ is a set of subsets of $A$ such that: (1) $\Sigma$ contains $A$ itself, (2) $\Sigma$ is closed under taking the complement, (3) $\Sigma$ is closed under taking countable unions.

\medskip
From this definition, it follows that the empty set $\emptyset$ is also in $\Sigma$ and that $\Sigma$ is closed under taking countable intersections, too. The extreme examples of such $\sigma$-algebras on a set $A$ are $\{\emptyset, A\}$ and $\mathcal{P}(A)$. The former is the smallest possible and the latter is the largest possible $\sigma$-algebra on $A$. Now, if $\mathcal{P}(A)$ qualifies as a $\sigma$-algebra, you might wonder why we go through the hassle of making such a definition instead of just always using $\mathcal{P}(A)$ as the domain of our measure function $\mu$. Why not just always take the largest possible domain? Why throw in such a complication just to restrict $\mu$ to possibly smaller domains? The answer is that for some choices of the set $A$, $\mathbb{R}$ being among these choices, it is impossible to construct a measure function $\mu$ on $\mathcal{P}(A)$ with certain desired properties. More on that later.

\medskip
By the way, there is also a notion of a so called $\Sigma$-algebra in computer science which is an entirely different concept that has nothing to do with the $\sigma$-algebras of this section, so don't get confused. I also have no idea why it's called an "algebra" anyway. In abstract algebra, an algebra is typically a vector space with some product defined on it - but I don't see the connection to this concept of $\sigma$-algebra here. [ToDo: Figure out if there is one! If not, rant about bad naming (again).]

% TODO: explain how the values of f on sets of measure zero do not affect the value of a (definite)
% integral. That implies that it makes no difference if we integrate over open, half-open or closed
% intervals


% https://www.youtube.com/watch?v=1BhSQiHTNbg  The Mathematician's Measure

% Contrast it to:
% https://encyclopediaofmath.org/wiki/Sigma-algebra_(Computer_Science)


%...TBC...[I don't really know why this is - maybe some weird sets $A$ have subsets that are not measurable and therefore need to be excluded from the domain or something? Figure out!]


% According to this video:
% https://www.youtube.com/watch?v=FtEmLexUw3Y&list=PLBh2i93oe2quIJS-j1NpbzEvQCmN00F5o
%   Measure Theory - Part 1 - Sigma Algebras [dark version]
% at around 11 min, the power set of a set A is actually a sigma-algebra (namely the largest possible sigma algebra) so it seems, we could indeed use our "first attempt" is actually workable. We *could* use the set of all subsets as the domain for the measure mu. But we could also use other sets of subsets. So, the initial attempt is not really wrong but the actual attempt is somehow more general rather than more restricted. ...but this generality makes it somehow unnecessarily complicated. The video says that using the power set is the best-case scenario in the sense that all subsets of A are measurable.

% ToDo: explain, why this complication with the sigma-algebra is necessary. Why don't we just always use P(A) as the domain. Does it have to do with non-measurable sets? At the end of the video, he says that he'll explain this in later videos.

% At the end (11:14) of the 2nd video, he says, we have to use the Borel sigma algebra and not the power set because mu should satisfy some rules. Maybe after makeing these 2 definitions (sigma-alg, measure), define the Borel-algebra for R^n and explain why the power-set itself is not suitable. 

% the 3rd video at 10:20 gives examples for measures: counting measure, Dirac measure,
% at 12:00, it gives calculation rules in [0, inf], especially important: 0*inf = 0 specifically in measure theory when multiplying volumes

% 4th video:



% Understanding Measure Theory and the Lebesgue Integral
% https://www.youtube.com/watch?v=gHUZFXvy4yE&list=PLWlFhVTefTJhSivCpbDWl5DjGLWUuA9GN&index=1




\paragraph{Definition: Measure} Given a set $A$ and a $\sigma$-algebra $\Sigma$ on $A$, a measure $\mu$ on $A$ is a function $\mu: \Sigma \rightarrow \mathbb{R}^+_0 \cup \{  \infty \}$, which we may also write as $\mu: \Sigma \rightarrow [0, \infty]$, such that: (1) $\mu$ is non-negative: $\forall X: \mu(X) \geq 0$, (2) $\mu$ assigns zero to the empty set: $\mu(\emptyset) = 0$, (3) $\mu$ is countably additive aka $\sigma$-additive:
\begin{equation}
\mu \left( \bigcup_{k=1}^{\infty} X_k \right) = \sum_{k=1}^{\infty}	\mu(X_k)
\end{equation}
for any countable collection $\{X_k\}_{k=1}^{\infty}$ of pairwise disjoint sets $X_k$ where each such $X_k$ is an element of $\Sigma$.

\medskip
Here "countable" means either countably infinite or finite. That means that the above equation should also hold for finite unions and finite sums, i.e. it's possible the replace the $\infty$ symbol by some finite upper limit.

% You may sometimes encounter the notation $\bigsqcup_{k=1}^{\infty} X_k$ rather than $\bigcup_{k=1}^{\infty} X_k$ to emphasize the disjointness of the sets. If the sets $X_k$ are not already disjoint, $\bigsqcup$ would make them so. ...but it's kinda informal because $ $\bigsqcup$ changes the data type, so to speak

% see here:
% https://www.youtube.com/watch?v=zJk4yuzJU3s
% Demystifying the Dirac Delta - #SoME2

%...TBC...
% todo:
% Bring example: counting measure, Dirac measure (work for all sets)
% for R^n: require additionally: 
%   \mu([0,1]^n) = 1 (unit hypercube as unit hypervoulme), 
%   \mu(X + x) = \mu(X)  (translation invariance)
% Define the Lebesgue measure on the Borel algebra on R^n, explain why P(R^n) is not suitable as domain




\subsubsection{The Lesbesgue Measure}
After these general considerations, we now want to construct a measure $\mu$ on the set of real numbers $\mathbb{R}$. That means, we want to define a function into which we plug in a subset of $\mathbb{R}$ and it spits out a nonegative real number or infinity. Intuitively, that measure should suitably generalize the idea of a length of an interval $[a,b]$, so we would certainly expect that $\mu([a,b]) = b-a$. Furthermore, we would expect $\mu$ to be translation invariant, i.e. $\mu(X + t) = \mu(X)$ for any measurable set $X \subseteq \mathbb{R}$ and any real number $t \in \mathbb{R}$ that acts as a translation or offset that shifts the set $X$ by an amount of $t$ along the real number line. The construction of such a function $\mu$ on the whole power set $\mathcal{P}(\mathbb{R})$ turns out to be impossible, though. That's the reason why we need this complication with the $\sigma$-algebras. So, if we can't use $\mathcal{P}(\mathbb{R})$ as our $\sigma$-algebra, i.e. as the domain of $\mu$, what other set of subsets of $\mathbb{R}$ could be suitable to serve as our $\sigma$-algebra? The answer turns out to be the so called \emph{Borel algebra} ...TBC...

% Tn multiple dimensions, it should generalize the idea of areas (2D), volumes (3D) and hypervolumesn (nD)


% https://de.wikipedia.org/wiki/Borelsche_%CF%83-Algebra
% https://en.wikipedia.org/wiki/Borel_set
% https://mathworld.wolfram.com/BorelSet.html
% https://math.stackexchange.com/questions/1080473/borel-sigma-algebra-definition


% $\Cup_{k=1}^{\infty} = \sum_{k=1}^{\infty}$

% Outer mu_o(A) vs inner measure mu_i(A) vs measure mu(A):
% -outer measure is defined for all subsets of X whereas inner measure only for measurable subsets
% -outer measure corresponds to upper and inner to lower Riemann sums
% -outer measure is countably subadditive, inner measure (countably?) superadditive
% -measure without qualifier is countably additive

%https://math.stackexchange.com/questions/714193/what-is-the-difference-between-outer-measure-and-inner-measure

%https://www.quora.com/What-is-the-intuition-behind-outer-and-inner-measure-of-a-set-How-can-you-show-that-measure-of-2-3-on-R-is-1-only-using-definition-without-using-the-theorem-that-measure-equals-length

%https://en.wikipedia.org/wiki/Outer_measure
%https://en.wikipedia.org/wiki/Inner_measure
%https://en.wikipedia.org/wiki/Measure_(mathematics)
%https://en.wikipedia.org/wiki/%CE%A3-algebra


%\subsubsection{The Dirac Measure}

% also called point mass measure

% Dirac's delta measure | Measure Theory
% https://www.youtube.com/watch?v=rKEEa4z9MAk
% -The sigma-alg over X is just the power set of X
% -There's a special element of X, called x_0
% -The measure of a subset A of X is 1 iff x_0 is in A and 0 iff x_0 is not in A

% the video is part of this playlist about measure theory:
% https://www.youtube.com/playlist?list=PL-hE1FcESINflUwv1d8G3njuZU7kQ1oh5

% The Dirac measure is also a probaility measure


%\subsubsection{The Laplace Measure}
% I think, it assigns the quotient of cardinatlities |S|/|A| to the subset S of A.

% Probability Measures | properties, [Laplace, Dirac, Borel]-probability, discrete case
% https://www.youtube.com/watch?v=02TqmIQQAmc


% Math's Strangest Set
% https://www.youtube.com/watch?v=hs3eDa3_DzU
% -A set with no measure - Vitali set

% Other measures:
% https://en.wikipedia.org/wiki/Borel_measure
% https://en.wikipedia.org/wiki/Probability_measure
% https://en.wikipedia.org/wiki/Fuzzy_measure_theory
% https://en.wikipedia.org/wiki/Haar_measure
% https://en.wikipedia.org/wiki/Lebesgue_measure
% https://en.wikipedia.org/wiki/Risk-neutral_measure

%===================================================================================================
\subsection{Other Integral Definitions} 

\subsubsection{The Darboux Integral}
% https://en.wikipedia.org/wiki/Darboux_integral

% -Equivalent to the Riemann definition
% -Uses upper and lower sums



%3 Integrals You Won't See in Calculus (And the 2 You Will)
%https://www.youtube.com/watch?v=le2IVvEGcaQ
%-Riemann, Darboux, Riemann-Stieltjes, Lebesgue, Ito

%How to integrate with respect to any function!
%https://www.youtube.com/watch?v=bWYEOfo0_A4


\subsubsection{The Riemann-Stieltjes Integral}



\subsubsection{The Ito Integral}
% https://en.wikipedia.org/wiki/It%C3%B4_calculus
% https://math.nyu.edu/~goodman/teaching/DerivSec10/notes/week6.pdf


% https://www.youtube.com/watch?v=T72K4mrH8to   Are These All of the Types of Integrals?
% -Has over 30 integral definitions. Some of them are not really new though but rather special
%  cases of normal integrals (like Fourier- or Laplace-integral). But some really are different
%  definitions. ..and it may miss some: how about non-Newtonian integrals - like geometric integral
% ...or integrals that arise in exterior and geometric calculus?


\begin{comment}

Differentiation is a purely mechanic process: given the list of elemenary derivatives and the differentiation rules, we can easily compute an expression for the derivative of any function. In may be tedious and the resulting expressions may become unwieldy, but in principle we know exactly what to do. This makes differentiation a perfect task for a computer and every computer algebra system will readily do this. Integration is much more difficult. Given an arbitrary function, it may not be immediately clear which rule should be applied in order to find an antiderivative. For many functions, an antiderivative that can be expressed as a closed form formula involving only elementary functions may not even exist. However, there is an algorithm that can compute any elementary antiderivative, if it exists or prove the nonexistence if it doesn't. That algorithm is the Risch algorithm and it is so complicated that, at the time of this writing, most computer algebra systems do not implement it in its full glory but rather in some simplified and less general form.

https://mathoverflow.net/questions/374089/does-there-exist-a-complete-implementation-of-the-risch-algorithm
https://mathematica.stackexchange.com/questions/140088/does-mathematica-implement-risch-algorithm-if-it-does-in-which-cases

-as example for computing a Riemman integral from the definiton, use f(x) = x^2 and integrate it from zero to some variable upper limit b
-define a sum S_N = \sum_{k=1}^N (k dx)^2 dx with dx defined as (b-a)/N with a=0, so it's just dx=b/N
 ...I think, the dx should actzally be a \Delta x
-this leads to S_N = dx^3 \sum_k k^3
-using \sum_k k^3 = (2N^3+3N^2+N)/6 via Faulhaber's formula we can get rid of the sum. Reference
 the formula for sums of powers of x somewhere in an earlier section
-this leads to an expression containing powers of N in the numerator and N^3 in the denominator
-when N -> inf only the leading term remains and we get \lim_{N -> inf} S_N = b^3/3
-this should just serve as a demonstration that in principle, it's possible to evaluate integrals from the definitions, using rules of limits and closed form sum formulas as an analogy how it is in principle possible to evaluate derivatives using limits
-state that nobody actually evaluates integrals like that, just like with derivatives

Explain what happens when we use closed or open intervals to integrate over and/or when we change the function values at isolated points - the value of the integral doesn't change. Introduce the notation where the set that we integrate over is written as subscript to the integral sign (typically using R for region - or maybe I for interval. In general, it's a subset of the domain of f)

Explain improper integrals with singularities in the integration interval.



Substitution:
https://en.wikipedia.org/wiki/Integration_by_substitution
https://de.wikipedia.org/wiki/Integration_durch_Substitution
https://de.wikipedia.org/wiki/Weierstra%C3%9F-Substitution
https://www.mathsisfun.com/calculus/integration-by-substitution.html
The Cycon Book has also a "2nd kind" of substitution, applicabel when the integrand is not of the form g'(x)*f(g(x))

This video has an interesting formula for the integral of the inverse:
https://www.youtube.com/watch?v=CroUJo54L5A  RARE Integration Strategy - You won't learn this in Calculus

\int f^{-1} (y) dy = x y - \int f(x) dx

This may indeed simplify some integrals, if the inverse of a givne function is easier to
integrate than the function itself


Good video Cauchy's formula for repeated integration:
https://www.youtube.com/watch?v=jNpKKDekS6k
at the end, it leads to the idea of fractional integrals and is continued here:
https://www.youtube.com/watch?v=2dwQUUDt5Is
These things fall into the general scheme of extending definitions. Here is another nice example:
https://www.youtube.com/watch?v=9p_U_o1pMKo
and that channel has even more of that stuff

There's also some thing called the tabular method of integration by parts, Google:
integration by parts tabular method
and see what shows up.

Analogy for difficulty of *finding* a solution vs *verifying* one:
-path through a labyrinth - also hard to find but easy to verify


Every calculus teacher I know skips this!!
https://www.youtube.com/watch?v=fgvTYb6Dre8

Every Type of Integral Explained in 7 Minutes
https://www.youtube.com/watch?v=eyhtBSUspqc

Ranking Every Integration Technique by Most Satisfying
https://www.youtube.com/watch?v=I7UpJWaxPEY

Every Integration Technique Explained in 8 minutes (part 1)
https://www.youtube.com/watch?v=IQY5kj_eYQk



\end{comment}