\section{Ordinary Differential Equations} 
Ordinary differential equations (abbreviated as ODEs) are equations in which the unknown quantity is not just a single variable like $x$, i.e. a number, but rather a full function $f:\mathbb{R} \rightarrow \mathbb{R}$. The equation may contain the function itself as well as derivatives of it. The presence of derivatives is what makes it a "differential" equation. The goal is to find a function $f = f(x)$ that obeys a certain given relationship between itself and its own derivatives. For example, the function $f(x) = \e^x$ obeys the relation $f = f'$ and the function $f(x) = \sin (x)$ obeys the relation $f = -f''$. These two are perhaps the simplemost nontrivial examples of ODEs and are practically very relevant. In ODEs, what we are \emph{given} is the relation like $f = -f''$ and what is to be found, i.e. what is \emph{unknown}, is $f$ itself. Consider the slightly more complicated example $f + a_1 f' + a_2 f'' = 0$ for some given constant coefficients $a_1, a_2$. We see that for $a_1 = 0, a_2 = 1$, this ODE reduces to the ODE for the sine: $f = -f''$ and for $a_1 = -1, a_2 = 0$, we get the ODE for the exponential function $f = f'$. Solving the differential equation means to find a function $f = f(x)$, which, when it is being plugged into the ODE and all the derivatives are evaluated, yields a true statement. It can straightforwardly (albeit tediously) be verified by evaluating the derivatives that $f = c_1 \e^{-\sigma x} + c_2 \e^{\sigma x}$ is indeed a solution to $f + a_1 f' + a_2 f'' = 0$ when we choose $\sigma = (a_1 + \sqrt{a_1^2 - 4 a_2})/(2 a_2)$. We will actually go through this example in more detail later. For now, I'll just say that \emph{verifying} that some given function is a solution to an ODE is an easy task but \emph{finding} such a solution in the first place is hard. This is a bit like with finding vs verifying antiderivatives but even more so - meaning that finding the solution is even harder in general. Some people even refer to the process of finding a solution to a differential equation as "integrating the differential equation" and integration is indeed often needed as a subtask in the process of solving an ODE. I'm not so fond of that terminology (it sounds unnecessarily obtuse) and will just call it "solving the ODE" rather than "integrating the ODE".

...TBC...

% give the solution to the damped oscillator equation and encourage the reader to verify that it is a solution indeed. Give sage and mathematic code to find the solution

% Wolfram alpha gives the solution only in complex form:
% DSolve[f[x] + a_1 f'[x] + a_2 f''[x] == 0, f[x], x]

% maybe use concrete coeffs: 9 y + 2 y' + 5 y'' = 0, y(0)=0, y'(0)=1
% https://www.wolframalpha.com/input?i=9+y+%2B+2+y%27+%2B+5+y%27%27+%3D+0%2C+y%280%29%3D0%2C+y%27%280%29%3D1
% this gives a nice damped oscillation

% https://en.wikipedia.org/wiki/Harmonic_oscillator#Damped_harmonic_oscillator
% https://en.wikipedia.org/wiki/Ordinary_differential_equation

%===================================================================================================
\subsection{Canonical Forms}


\paragraph{Explicit Form}
In the explicit form to write down an $n$-th order ODE, the highest derivative, i.e. the $n$-th derivative, of the unknown function $f$ appears alone on the left hand side and it is expressed as some function $F(\ldots)$ on the right hand side which takes as arguments the independent variable $x$ and well as $f$ and all of its derivatives of orders less than $n$:
\begin{equation}
f^{(n)}(x) = F(x, f'(x), f''(x), \ldots, f^{(n-1)}(x))
\end{equation}
If the function $F$ does not explicitly depend on the independent variable $x$, the system is called \emph{autonomous}. If the independent variable is time, which is often the case, then we may also write $t$ instead of $x$. In this case, being "autonomous" can be interpreted as "not depending on absolute time". The system just has the same set of potential behaviors at all times. It doesn't have a clock which could potentially alter its behavior. In the context of signal processing, such a system is also called \emph{time-invariant}.

\paragraph{Implicit Form}
For some systems, it is not possible to express it in explicit form. In such cases we must resort to the implicit form which is often less convenient to handle. In this case, we just put the $n$-th derivative also into the function $F$ as additional argument and equate everything to zero:
\begin{equation}
0 = F(x, f^{(n)}(x), f'(x), f''(x), \ldots, f^{(n-1)}(x))
\end{equation}
By the way: It may also occur that the left hand side is not zero but some other given function $g(x)$ which in this context is often interpreted as some sort of driving function that is applied to the system from the outside. But notice that this driving function can actually be absorbed into $F$ by just bringing it over to the other side. The math doesn't really change whether a term appears on one side of the equation or with opposite sign on the other side. Our interpretation or mental model about what is happening may change, though.

%---------------------------------------------------------------------------------------------------
\paragraph{Newton Notation}
It has been said that the independent variable is often time in which case we denote it by $t$. This is because ODEs are often used to model physical processes where some quantity changes as a function of time. In the text above, I have used the so called Lagrange notation for derivatives - the one with the dash or prime as in $f', f''$ etc. Newton proposed to use dots over dependent variable to denote time derivatives. Moreover, now that the independent variable has become $t$, the letter $x$ if free again and is often used for the (one of the) dependent variable(s) such that we write things like $\dot{x}$. That would mean $x'(t)$ in Lagrange notation or $\frac{d}{dt} x(t)$ in Leibniz notation. Newton notation is often the most convenient when we are dealing with low order ODEs but gets inconvenient for higher orders. Fortunately, most ODEs in physics are only second order anyway and moreover, there is actually a way to rewrite an ODE of any order as a system of first order ODEs as we will see next.

%---------------------------------------------------------------------------------------------------
\subsubsection{First Order Systems}
One form to write down an ODE has turned out to be particularly convenient. It is the form in which many ODE solver implementations expect their input. The general format is:
\begin{eqnarray}
 y_1' &= f_1(x, y_1, y_2, \ldots, y_n) \\
 y_2' &= f_2(x, y_1, y_2, \ldots, y_n) \\
 \quad \ldots & \ldots \\
 y_n' &= f_n(x, y_1, y_2, \ldots, y_n)
\end{eqnarray}
where $n$ is the number of variables and also the number of equations. To an ODE solver, you would somehow have to pass an array of (pointers to) the functions $f_1, f_2, \ldots, f_n$. How that works in detail will depend on the programming language and API of the solver but generally, a system of first order ODEs is the format, you want to express your ODE in if you want to use such solvers.

% https://www.mathworks.com/help/matlab/math/choose-an-ode-solver.html

\paragraph{Transforming High Order ODEs into 1st Order Systems}
Fortunately, it is possible to transform higher order ODEs into this convenient format of a first order system - at least when the ODE is given in explicit form. [Q: what about implicit forms?]. Let's see how this works by way of an example. Consider the third order ODE:
\begin{equation}
 y''' + y y'' + \beta (1 - (y')^2 ) = 0
\end{equation}
This is the so called Falkner-Skan equation that arises in fluid dynamics. But where the equation comes from doesn't matter here. I just wanted to point out that this is a real life example. The first thing we do, it to write it in explicit form which is a trivial transformation in this case:
\begin{equation}
 y''' =  - y y'' - \beta (1 - (y')^2 ) 
\end{equation}
Next, we introduce some auxiliary variables for $y$ and its derivatives: $y_1 = y, y_2 = y', y_3 = y''$. We observe that with this definition, we have: $y_1' = y_2, y_2' = y_3$. So we can write:
\begin{eqnarray}
 y_1' &=& y_2 \\
 y_2' &=& y_3 \\
 y_3' &=& -y_1 y_3 - \beta (1 - y_2^2)
\end{eqnarray}


%$y_0 = y, y_1 = y', y_2 = y'', y_3 = y'''$. We observe that with this definition, we have: $y_0' = y_1, y_1' = y_2, y_2' = y_3$. So we can write:
%\begin{equation}
% y_2' = -y_0 y_2 - a (1 - y_1^2), \qquad y_1' = y_2, \qquad y_0' = y_1
%\end{equation}
%This is a system of 3 ODEs

% Todo: introduce the system of 1st order ODEs first and only then explain how tow transform single higher order ODEs into such systems

%When an ODE is given to us in explicit form, then we can always transform it into

% use as example a 3rd order nonlinar ODE
%
% see:
% https://www.johndcook.com/blog/2023/01/13/third-order-odes/

% Start with the 3rd order ODE
%   y''' + y * y'' + a * (1 - (y')^2) = 0    
% Make it explicit:
%   y''' = -y * y'' - a * (1 - (y')^2 )
% Introduce new variables:
%   y_3 = y''', y_2 = y'', y_1 = y', y_0 = y
% Observe that y_3 = y_2', y_2 = y_1', y_1 = y_0' to write:
%   y_2' = -y_0 y_2 - a * (1 - (y')^2)
%   y_1' = y_2
%   y_0' = y_1
% Instead of a single 3rd order ODE, we now have a system of 3 1st order ODEs. One of them captures our original ODE. The other two just copy variables around. What to do, if the ODE is in implcity form?



% https://en.wikipedia.org/wiki/Falkner%E2%80%93Skan_boundary_layer
% https://en.wikipedia.org/wiki/Blasius_boundary_layer



%===================================================================================================
\subsection{Visualization Methods}

\subsubsection{Direction Fields}

% streamline plots, isoklines

\subsubsection{Phase Space}





%===================================================================================================
\subsection{Modeling Behavior}
The main purpose of differential equations - ordinary or partial - is model the behavior of systems that may occur in nature, technology, economics, society or whereever. Certain types of behavior are of particular interest.

%---------------------------------------------------------------------------------------------------
\subsubsection{Growth}

\paragraph{Exponential Growth} 
Imagine a situation in which some quantity $y$ reproduces itself and the production rate is proportional to the amount of quantity that is already present. One example of such a situation could be the population of some species when we assume that the environment provides an unlimited amount of food and all the other necessities. The situation can be modeled by the following ODE: $y' = c y$ from some positive constant $c$ which determines the rate of growth. It can easily be verified by differentiation that $y(x) = \e^{c x}$ is a solution to this ODE.

%[TODO: interest rates and/or bacteria growth]
% standard example: bacteria or money

\paragraph{Exponential Decay}
When the growth-rate is zero, nothing ever happens. Mathematically, there's nothing stopping us from also considering a negative "growth" rate. In such a case...TBC...[TODO: radiactive decay and/or RC-circuit]
% standard example: radioactive decay


\paragraph{Logistic Growth} [TODO: growth in environment of limited capacity]

% standard example: a species in an environment with limited capacity

\paragraph{Disease Spreading} [TODO: SIR model]

% https://www.khanacademy.org/science/ap-biology/ecology-ap/population-ecology-ap/a/exponential-logistic-growth

% https://en.wikipedia.org/wiki/Logistic_function

% ToDo: bring SIR model example as attack-decay

%---------------------------------------------------------------------------------------------------
\subsubsection{Oscillation}
Oscillatory behavior is something that we observe a lot in nature and a lot of our technology builds on systems that oscillate. If we want to build mathematical models of such processes, we will need a mathematical framework that can mimic this sort of behavior. It turns out that ODEs are very suitable for this.

\paragraph{A Single Second Order ODE}
The quintessential mathematical description of an oscillation is the sine function $f(x) = a \sin(\omega x + \varphi)$. We have already included some parameters for flexibility: $a$ for the overall amplitude, $\omega$ to control the frequency and $\varphi$ for the initial phase at $x=0$. We have already seen that such sinusoidal functions happen to arise as solutions of simple ODEs like $f = -f''$. Here, we will consider the ODE:
\begin{equation}
 y'' + a y' + b y = 0
\end{equation}
This equation is called a linear second order differential equation. It is linear because it does not contain any products between $f$ and its derivatives. It is second order because the highest derivative of $f$ that appears in the equation is the second derivative. ...TBC...



%\paragraph{A Mass-Spring-Damper System}
%After these general considerations for modeling oscillations with ODEs, let's now look the prime example of a system whose behavior is modeled by that kind of ODE. ...TBC...

%[TODO: explain the system, the ODE and its solution, point out what other systems are described by the exact same ODE like RLC-circuits or the linearized pendulum]


\paragraph{A System of Two First Order ODEs}
We have seen that it is possible to re-express an ODE of any order $n$ as a system of $n$ first order differential equations. Applying this method to 


%This form is a quite convenient canonical form for solving an ODE numerically. It happens to the form that most (if not all) generic numerical ODE solvers expect their input. So, if we want to use numerical solvers, we need to be familiar with that form and we need to know how to produce it, when the ODE is given to us in a different form.
...TBC...

% Maybe treat the conversion between single high-order ODE and system of 1st order ODEs in a different section - the conversion is not limited to linear ODEs.

\paragraph{A Single First Order Complex ODE}
We have seen a second order ODE can produce oscillatory behavior and that a system of two first order ODEs can reproduce that behavior as well. It seems like we need either a second derivative or two equations but there is actually a third way to produce oscillatory behavior with ODEs that requires only single first order ODE. The key is to allow the coefficient to be imaginary. If we have an ODE in a complex variable, that variable actually has two parts - the real and imaginary part - and they in which these parts interact resembles the way our two variables from our system of two ODEs interact, as we will see....TBC...

% ToDo: show how splitting the complex ODE into really and imaginary parts recovers the system of 2 first order ODES

%the complex multiplication actually packages up interactions




\paragraph{Resonance}

\paragraph{Nonlinear Oscillations}
[TODO: Predator-Prey (Volterra-Lotka) model, Duffing oscillator, pendulum (not linearized)]

% pendulum:
% y'' + k sin(y) = 0  ->  y'' = - k sin(y)

%---------------------------------------------------------------------------------------------------
\subsubsection{Chaos}

\paragraph{Weather Forecast} [TODO: Lorenz system]

\paragraph{Double Pendulum}

\paragraph{Three Body Problem}

% https://de.wikipedia.org/wiki/Lorenz-Attraktor
% https://en.wikipedia.org/wiki/Lorenz_system

% Maybe Mandelbrot set...but no: that arises from a discrete chaotic system




%===================================================================================================
\subsection{Solution Methods}

\subsubsection{Analytical Methods}

%https://tutorial.math.lamar.edu/classes/de/IntroHigherOrder.aspx

\paragraph{Method of Undetermined Coefficients}
% https://tutorial.math.lamar.edu/classes/de/HOUndeterminedCoeff.aspx

% Applicable, if we have some idea about the general form of the solution. We make an ansatz

\paragraph{Separation of Variables}
Let us assume that the ODE is given in (or can be transformed into) the special form $y' = f(x) g(y)$, i.e. the right hand side is a product of two functions $f,g$ where $f$ only depends on $x$ and $g$ only depends on $y$. In this case, we can rewrite $y'$ in the Leibniz notation as $dy / dx$ and treat these $dx, dy$ differentials formally like variables. We bring everything with $y$ to one side of the equation and everything with $x$ to the other side:
\begin{equation}
\frac{dy}{dx} = f(x) g(y) 
\quad \Rightarrow \quad
\frac{1}{g(y)} dy = f(x) dx
\quad \Rightarrow \quad
\int \frac{1}{g(y)} dy = \int f(x) dx
\end{equation}
Now we integrate both sides separately with respect to their respective integration variable, i.e. we evaluate the integrals. This will give us an expression of the form $H(y) = F(x)$ where $F(x)$ is an antiderivative of $f(x)$ and $H(y)$ is an antiderivative of $1/g(y)$. If we are lucky, we may be able to transform the resulting equation into an explicit expression for $y$. Let's see this method in action on an example. Let the ODE be given as:
\begin{equation}
y' = \frac{dy}{dx} = \sin(x) \sqrt{y} 
\; \Rightarrow \; 
\frac{1}{\sqrt{y}} dy = \sin(x) dx
\; \Rightarrow \;
\int \frac{1}{\sqrt{y}} dy = \int \sin(x) dx
\; \Rightarrow \;
2 \sqrt{y} + c_1 = - \cos(x) + c_2
\end{equation}
Combining the two integration constants $c_1, c_2$ into a single constant $c$ and solving for $y$ yields:
\begin{equation}
y = \left( \frac{c - \cos(x)}{2} \right)^2,
\qquad \qquad
y' = \frac{\sin(x) (c-\cos(x))}{2},
\quad
\sqrt{y} = \frac{c - \cos(x)}{2}
\end{equation}
where our result is the leftmost formula $y = \ldots$ and the $y' = \ldots$ and $\sqrt{y} = \ldots$ I have only given to make it easier for you to verify that $y = y(x)$ is indeed a solution. Just plug the formula into the original ODE and observe how it becomes a true statement.

\medskip
[Q: Can it happen that the so found equation for $y$ cannot be solved explicitly for $y$, i.e. can we arrive at an implicit equation for $y$? I think so. ToDo: Try to find an example, where this happens. Maybe something where we get an expression $H(y) = y + \sin(y)$. I think, the function $y(x)$ that we are interested in will be the inverse of $H$. I think, the solution is $y(x) = H^{-1}(F(x))$]
%then we can 

% Other examples (from Cycon, pg 218 ff)
% y' = 1 - y^2   ->   y = tanh(x)
% y' + x y = 0   ->   y = c * exp(-x^2 / 2)


% https://www.wolframalpha.com/input?i=integral+of+y%5E%28-1%2F2%29
% https://www.wolframalpha.com/input?i=solve+dy%2Fdx+%3D+sin%28x%29+sqrt%28y%29

\paragraph{Variation of Constants}
% aka variation of parameters - but I like the formulation variation of constants more
% https://tutorial.math.lamar.edu/classes/de/HOVariationOfParam.aspx

% https://en.wikipedia.org/wiki/Variation_of_parameters

\paragraph{Series Solutions}
% https://tutorial.math.lamar.edu/classes/de/HOSeries.aspx

\paragraph{Laplace Transforms}

\paragraph{Integrating Factors}




%\paragraph{Making an Ansatz}
% Method of? Undetermined coefficients

% Variation of parameters

%% 


% https://www.wolframalpha.com/input?i=derivative+of+e%5E%28a+x%29+sin%28w+x%29
% https://www.wolframalpha.com/input?i=second+derivative+of+e%5E%28a+x%29+sin%28w+x%29

% Maybe make a subsection "Determining the Parameters" with subsubsections "Initial Value Probelms" and "Boundary Value Probelms"

\subsubsection{Numerical Methods}

\paragraph{Euler's Method}

\paragraph{Runge-Kutta Methods}

\paragraph{Implicit Methods}




%===================================================================================================
\subsection{Theory of Linear ODEs}
For the important subset of linear ODEs, we actually do have a full blown analytic solution theory. Much of this theory will closely parallel and build upon the solution theory of linear systems of equations that we know from linear algebra. ...TBC...





% https://en.wikipedia.org/wiki/Linear_differential_equation

\subsubsection{Linear ODEs with Constant Coefficients}
We have seen how a second order linear ODE with constant coefficients could model decaying (or damped) oscillations and the range of possible behaviors also included pure oscillations and pure decay processes as special edge cases as well. What sort of behavior we will see depends on the coefficients. Exponential growth (with or without oscillation) is actually a third kind of behavior that can be produced. We will now look into what happens when we allow higher order ODEs or, equivalently, systems of more than two first order ODEs. What we will discover may be on the one hand disappointing: We will not see any fundamentally new sorts of behavior. These higher order systems will exhibit behaviors that can be expressed as superpositions (i.e. weighted sums) of the behavior that we already saw. But that may be - on the other hand - also satisfying: we already have the mathematical machinery to fully understand that kind of behavior. For linear ODEs with constant coefficients, there is a full blown analytic theory about their solutions. That theory actually extends even a bit further to non-constant coefficients, i.e. "coefficients" that can be functions of the independent variable. But let's treat the simpler case of constant coefficients first. ...TBC...

...TBC...

% The resulting sortd of behavior is actually not something fundamentally new - we'll just see superpositions of oscillations


%\subsubsection{Linear ODEs with Polynomial Coefficients}


% https://en.wikipedia.org/wiki/Sturm%E2%80%93Liouville_theory
% https://de.wikipedia.org/wiki/Sturm-Liouville-Problem

\begin{comment}
-solving an ODE can be seen as a certain generalization of solving an integral - explain how
-give ODE of damped oscillator: f + a_1 f' + a_2 f'' = 0. For $a_1 = 0, a_2 = 1$ we get the ODE for exp, for $a_1 = -1, a_2 = 0$ we get the ODE for sin.

Talks about self-adjoint ODEs (what is this?)
https://huichawaii.org/wp-content/uploads/2019/06/Beccar-Varela-Maria-2019-STEM-HUIC.pdf

Differential transform:
https://core.ac.uk/download/pdf/32226457.pdf
-Takes a continuous function and transforms it into a series of cofficients defined by its
 MacLaurin expansion coeffs?
 
https://en.wikipedia.org/wiki/Homogeneous_differential_equation 



\end{comment}