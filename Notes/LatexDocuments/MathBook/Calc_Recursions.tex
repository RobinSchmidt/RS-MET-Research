\section{Infinite Anythings}
After having seen infinite sums and infinite products, we'll now do a thing that mathematicians love to do: Generalize. We have defined an infinite sum via a limit over the partial sums $s_n$. The computation of such an $s_n$ was a finite sum which we already knew how to do. There is, however, another way to look at it. The computation of $s_n$ can also be seen as a formula involving the previous sum $s_{n-1}$ plus some new "update" or "correction" term $a_n$, i.e. $s_n = s_{n-1} + a_n$. For this $a_n$ term, we typically had an explicit formula involving $n$, i.e. $a_n = f(n)$ for a given function $f(n)$. With infinite products, the situation was similar with the only difference that we had a sequence of (finite) products $p_n$ and the update rule was $p_{n+1} = p_n \cdot a_n$. The general pattern here is that we compute the next term in our sequence (of partial sums or products) via some rule that takes in one (or, more generally, more) previous values of the sequence and possibly the index $n$ and computes the next value in this sequence. That is: we are dealing with sequences that can be produced via some recursive rule of the general form:
\begin{equation}
a_{n+1} = f(n, a_n, a_{n-1}, a_{n-2}, \ldots, a_0)
\end{equation}
This form is very general and the actual rule will often be simpler than that in practical cases. Often, the dependency will only have a finite window of memory. For example, $a_{n+1}$ may depend only on $a_n, a_{n-1}$ but not on even "older" values of the $a$ sequence. But for greatest generality, we allow it to depend on all previous values of the sequence and we also allow an explicit dependency on the index $n$ to be able to include the infinite sums and products into this more general framework. In infinite sums and products, we usually do have such an explicit dependency on $n$. ...TBC...

% ToDo: 
%
% - Maybe in the explanation of the sums, don't introduce this intermediate sequence a_n as in
%   the formula $s_n = s_{n-1} + a_n$. Instead frame it as $s_n = s_{n-1} + f(n)$ directly. Give
%   an example for how an original sum can be transformed into such a recursive representation. We
%   just need to extract the formula f(n) and the initial value. Maybe take the example of the
%   sum $s = \sum_{n=1}^{\infty} \frac{1}{n^2}$. Here, we have $f(n) = \frac{1}{n^2}$ and the
%   initial value $s_1$ for $n=1$ is $a_1 = 1$
%
% - Or, maybe make a subsubsection in the "Recursively Defined Sequences" subsection for how to
%   transform infinite sums and products into this recursive form
%
% - Maybe, for consistency with previous secstions, use k instead of n for the index


%===================================================================================================
\subsection{Recursively Defined Sequences}

% Maybe start with the Babylonian algorithm applied to 2 as motivating example - without yet mentioning the name.

\paragraph{Computing Square Roots}
As a motivating example, consider the following rule to compute a new term in a sequence from a previous one:
\begin{equation}
a_{n+1} = \frac{1}{2} \left( a_n + \frac{c}{a_n}  \right)
\end{equation}
For some given constant number $c$ and some given initial value $a_0$. Here, $a_{n+1}$ only depends on the sequence value immediately before it, i.e. $a_n$, and there is no explicit dependency on $n$ in the formula. Let's see what this rule produces for $c = 2, a_0 = 3$ with SageMath:
\begin{center}
\begin{tabular}{ ccccc } 
\begin{lstlisting}
c = 2.0
a = 3.0
for i in range(1,8):
    a = (a + c/a) / 2
    print(a)
\end{lstlisting}
& & & &
\begin{lstlisting}
1.83333333333333
1.46212121212121
1.41499842989480
1.41421378004720
1.41421356237311
1.41421356237309
1.41421356237309
\end{lstlisting}
\end{tabular}
\end{center}
% See:
% https://stackoverflow.com/questions/3220121/verbatim-environment-inside-latex-cell
After the 6th iteration, the sequence has converged to a value that will thereafter remain the same in the first 15 decimal digits. The value that this sequence converges to is none other than the square root of two. More generally, for any given positive real number $c$, the sequence of numbers defined by this algorithm will converge to $\sqrt{c}$ regardless of the initial value $a_0$ as long as it is positive [VERIFY!]. Let's try to figure out why ...TBC...

% ToDo:

% The explanation why should be deferred to the discussion of Newton-Raphson iteration - or maybe provide a geometric explanation here. Start with a rectangle of sidelengths 1 and c...see Weitz' video

% Maybe mention that this can indeed be a practically useful algorithm for evaluating square roots - or for refining an existing estimate of the square root. Maybe mention the famous fast inverse square root algorithm from the Quake codebase. Maybe mention startegies to come up with a good starting value. Maybe this may involve bit-twiddling trickery with IEEE-754 floating point numbers. For arbitrary pecision arithemtic, we may use initial values computed from converting results in machine precision and then improve the precision on them.



%---------------------------------------------------------------------------------------------------
\subsubsection{Convergence}
...TBC...

%---------------------------------------------------------------------------------------------------
\subsubsection{Fibonacci Numbers}
...TBC...

% the sequence itself diverges but the sequence of ratios of successive terms converges to the golden ratio

% Maybe mention also Lucas numbers - they use the same ruel with different initial values.

\paragraph{The Golden Ratio}
Although the sequence of Fibonacci numbers itself diverges to infinity, we can construct a new sequence from it that does converge - and it does so to a rather interesting number. ...TBC...


%---------------------------------------------------------------------------------------------------
\subsubsection{Newton-Raphson Iteration}
...TBC...

%\paragraph{The Babylonian Algorithm}

% https://en.wikipedia.org/wiki/Square_root_algorithms#Heron's_method

% Use it to compute the Lambert-W function. Apply it to y = x e^x  ->  x e^x - y = 0 where y is
% just a constant in this context


%---------------------------------------------------------------------------------------------------
\subsubsection{The Arithmetic-Geometric Mean}
Consider the following recursive rule for a pair of numbers:
\begin{equation}
\begin{pmatrix} 
a_{n+1} \\ 
b_{n+1}
\end{pmatrix} 
= 
\begin{pmatrix} 
(a_n + b_n)/2 \\ 
\sqrt{a_n \cdot b_n}
\end{pmatrix} 
\end{equation}
That is, we have a recursive rule that produces a vector from a previous vector. The first component of the next vector is the arithmetic mean of the components of the current vector. The second component is the geometric mean. Let's see what this rule produces for the initial values $a_0 = 2, b_0 = 8$:
\begin{center}
\begin{tabular}{ ccccc } 
\begin{lstlisting}
a = 2.0
b = 8.0
for i in range(1,7):
    t = (a+b)/2
    b = sqrt(a*b)
    a = t
    print(a, b)
\end{lstlisting}
& & & &
\begin{lstlisting}
5.00000000000000 4.00000000000000
4.50000000000000 4.47213595499958
4.48606797749979 4.48604634366366
4.48605716058173 4.48605716056869
4.48605716057521 4.48605716057521
4.48605716057521 4.48605716057521
\end{lstlisting}
\end{tabular}
\end{center}
Apparently both vector components converge to the same value of around $4.486$. A value that is somewhere in between the arithmetic mean $5$ and geometric mean $4$ of the initial values $2$ and $8$. This common limit of both vector components is called the arithmetic-geometric mean. ...TBC...ToDo: explain applications, explain how the vector iteration can be embedded into the current framework of sequences by "zipping"

% Maybe move the print call into the 1st line of the loop such that we also print out the initial
% values. Do this also for the sqrt-algorithm

% Frame this in the following way: we start with a vector-valued recursion formula:
% (a_{n+1}, b_{n+1}) = ((a_n + b_n)/2, sqrt{a_n b_n})
% but this can be reformulated within a framework of scalar-valued sequences by just "zipping" the a_n, b_n sequences into a single sequence. The update rule may then be of the form a_{n+1} = ...for n even, ....for n odd - or something like that. It may be inconvenient but it can be done.

% What happens if we apply the same idea to other pairs of means? Try the arithmetic-harmonic mean.

% https://en.wikipedia.org/wiki/Geometric%E2%80%93harmonic_mean
% 


% https://en.wikipedia.org/wiki/Arithmetic%E2%80%93geometric_mean
% "The arithmeticâ€“harmonic mean is equivalent to the geometric mean." (section of "Realted Concepts")
% ...is it generally true that for the generalized means G_m(x,y) and G_n(x,y), the mutual limit of
% the recursion is G_{(m+n)/2}(x,y). By generalized mean, I mean ((x^n + y^n)/2)^{1/n}. Try it numerically! The fact that the arithmetic-harmonic mean is equal to the geometric mean seems to suggest that. The arithmetic mean would be G_1(x,y), the harmonic mean G_{-1}(x,y) and the geometric mean G_0(x,y). If so, that would mean that the arithmetic-geometric mean would be G_{1.5}(x,y) = ((x^{3/2} + y^{3/2})/2)^{2/3}. That would be a closed form solution for the AGM which may be useful sometimes.

% https://mathworld.wolfram.com/Arithmetic-GeometricMean.html





%===================================================================================================
\subsection{Infinitely Nested Radicals}
Let's assume that we have some given constants $a,b$ and consider the following expression to compute a value $x$:
\begin{equation}
 x = a + b \cdot \sqrt{a + b \cdot \sqrt{a + b \cdot \sqrt{\ldots}}}
\end{equation}
Can we make sense of such an expression involving infinitely nested radicals (aka roots) and maybe even find a value of $x$ that satisfies this equation? By inspection, we may realize that the expression inside of the outermost square root is actually the same as the whole right hand side expression which, by virtue of the equation, is supposed to be equal to the left hand side $x$. So we may rewrite the equation as:
\begin{equation}
 x = a + b \cdot \sqrt{x}
\end{equation}
That looks a lot simpler! With a little bit of algebra we can bring it into the standard form of a reduced quadratic equation:
\begin{equation}
x^2 - (2a + b^2) x + a^2 = 0
\end{equation}
which we can solve for $x$ via the $pq$-formula to obtain:
\begin{equation}
x_{1,2} = k \pm s 
\quad \text{where} \quad
k = a + \frac{b^2}{2}, \;
s = \sqrt{k^2 - a^2}, 
\end{equation}
But we need to be a bit careful with this result. The "little bit of algebra" involved a step where both sides of an equation had to be squared. Such steps may produce extraneous solutions. We need to try both signs for $s$ and pick the one that produces a value $x$ that does indeed solve the equation. For some combinations of $a,b$ we may not get any real solution at all. For $a = 3, b = 2$, the formula produces the solutions $x = 1, x = 9$ and of these, only the $x=9$ solution actually works. With $a = -2, b = 3$, we get $x = 1, x = 4$ as solutions and they are both true solutions. But be that as it may. It's interesting to note here that no actual calculus tools were needed to get to the formula solution(s). We don't evaluate any limit here. What we did instead was to observe that in an infinite nesting of functions, we are allowed to peel off the outermost layer of function evaluation, if we see by inspection that the argument of the function is the same as the function's output. In fact, we can peel off any finite number of layers but doing it only for the outermost one was the simplest choice here. This act of peeling off was legal only due to the \emph{infinite} nesting. For a finite nesting, we would not have been able to do that. We recognized that one part of an infinite expression was equal to the expression as a whole and took advantage of this self-similarity to simplify the expression. This is a general technique that we should add to our problem solving toolkit. It feels like lying somewhere in between an algebraic and a calculusly technique. It has to do with taking a process to infinity (which is what we do in calculus) but also with replacing a subexpression with some other subexpression which we know to be equivalent (which is what we do in algebra).

%...TBC...Mention other kinds of infinite expressions involving roots


% which one that is may depend on the signs of a and b? Or maybe on the sign of k? Figure out! Give an example for a triple a,b,x that works. a = 3, b = 2, x = 9 works. Ah - with a = -2, b = 3, we actually get two real solutions at x = 1 and x = 4. So maybe different cases are possible? Sometimes the + solution, sometimes the - solution sometimes both and sometimes none? Figure out! With a = -1, b = 1, we get no (real) solution. With a = -1.6, b = 2.5, we get close to a tangency solution. I think, when the - solution works, the + solution will also work but not the other way around. The - solution produces the smaller value - but if the sqrt-like graph crosses the diagonal once, it must cross it again due to the slope for sqrt going down when we move rightward. So, I think, the case that the - solution works but the + solution doesn't cannot occurr.


% https://en.wikipedia.org/wiki/Nested_radical#Infinitely_nested_radicals

% https://mathworld.wolfram.com/NestedRadical.html

% https://math.stackexchange.com/questions/4049946/the-infinitely-nested-radicals-problem-and-ramanujans-wondrous-formula

% https://en.wikipedia.org/wiki/Quadratic_equation#Reduced_quadratic_equation

\begin{comment}

To solve it, use the Sage code:

a = 3
b = 2
k = a + b^2/2
s = sqrt(k^2 - a^2)
x1 = k + s
x2 = k - s
y1 = a + b * sqrt(x1)
y2 = a + b * sqrt(x2)
N(x1),N(y1), N(x2),N(y2)

which gives:

(9.00000000000000, 9.00000000000000, 1.00000000000000, 5.00000000000000)


Check the formula against Mathematica's result:

https://www.wolframalpha.com/input?i=solve+x+%3D+a+%2B+b+sqrt%28x%29+for+x

Here is a plot of  f(x) = x  and  f(x) = a + b sqrt(x)  with adjustable a,b:
  https://www.desmos.com/calculator/epu6yonem9
The intersection of the graphs should be our solution

\end{comment}




%===================================================================================================
\subsection{Infinite Power Towers}
Let's next consider a power tower of infinite height, aka infinite tetration. We want to make sense of an expression of the form:

\begin{equation}
f(x) = x^{x^{\cdot^{\cdot^{\cdot^x}}}}
%x^{x^{\cdot^{\cdot^{\cdot^{x^{x^x}}}}}}
% x^{x^{x^{x^{x^{x^{x^{x^{x^x}}}}}}}}
\end{equation}
% https://tex.stackexchange.com/questions/144490/how-do-i-typeset-a-tenfold-powering-a-tower-of-powers-with-latex
...TBC...


%===================================================================================================
\subsection{Continued Fractions}

% ...hmm...I'm not so sure if continued fractions and radicals fit in here because the upate rule actually is not so simple. We have to evaluate the whole expression from the inside out implying that we must do a complete re-evalutation of everything for each new $n$. However, in the beginning we said that the update rule may involve more than one previous values, so maybe we can us that as justification? ..But hey! We are actually "outside" the recursion subsection here - so we do not need the content to fit in!





%===================================================================================================
%\subsection{Iterated Function Systems}
\subsection{Fixed Point Iteration}
Consider the following situation: We are given a function $f(x)$ and an initial value $x_0$ and now we apply the function again and again to produce the next value. That means $x_1 = f(x_0)$, $x_2 = f(x_1) = f(f(x_0))$ and so on. As you may have guessed, we intend to repeat the application of $f$ to our previous result ad infinitum. With some functions $f$ and initial values $x_0$, this process may actually converge to a definite number. Let's call that number $x$, so we have by definition:
\begin{equation}
 x = f(f(f( \ldots f(x_0))))
\end{equation}
where the dots intend to mean that we apply $f$ infinitely often to $x_0$. Now remember what we did with the infinitely nested radical. We have a similar situation here, just more general. In our previous infinite radical situation, we had this same situation for the special case $f(x) = a + b \sqrt{x}$. When we peel off the outermost function application on the right hand side, what remains is \emph{still} an infinite number of function applications to $x_0$ and it is therefore still equal to $x$. That means $x$ must satisfy $x = f(x)$. A point $x$ with that property is called a \emph{fixed point} of the given function $f$. Geometrically, it is a point of intersection between the graph of $f(x)$ and the graph of the identity function. Not all functions have such intersection points but some do. Some functions $f$ even have multiple such intersection points. For example, the graph of the function $f(x) = x^3$ intersects with the graph of the identity function $g(x) = x$ in the points $(-1,-1)$ and $(1,1)$ and it is indeed true that $(-1)^3 = -1$ and $1^3 = 1$.

% Maybe take 8 cos(x) as another example. It has 5 fixed points
%
%   https://www.desmos.com/calculator/di4hxswknp
%
% Maybe create an example where one of the fixed points is not an intersection point but a point
% of tangency. Maybe use k cos(x) and adjust k accordingly. k = 6.202 seems to work:
%
%   https://www.desmos.com/calculator/wzjyzxnp1r
%
% How would we find a formula for k? Tangency means that the derivative of f must be 1 at the
% fixed point.


%If it does, it means that this number $x$ must satisfy the condition $f(x) = x$. This can be seen by looking at the infinitely nested expression $x = f(f(f(\ldots f(x_0))))$. We may again peel off the outermost function application and recognize that what remains is equal to the whole right hand side. This is, of course, equal to the left hand side $x$, so we may conclude that the infinite expression is equivalent to the simple finite expression $x = f(x)$ ...TBC...


% https://en.wikipedia.org/wiki/Iterated_function_system

% Iterated Function Systems: A Comprehensive Survey
% https://arxiv.org/abs/2211.14661

% IMPLEMENTING ITERATED FUNCTION SYSTEMS IN PYTHON
% https://www.math.uni-rostock.de/~wj/projects/Fractals/160355141_Habib_Wahab_MTH6138_IFS.pdf

% Consider the equation  x = f(f(f(...f(x_0)))). Peeling off the outermost application of f in the 
% RHS, we note that, by the equation, we must have x = f(x). That means that the equation requires
% that x must be a fixed point of the function f. The is equivalent to requiring x - f(x) = 0 which
% is in the form suitable to hand over to a root finder ...tbc...

% That is a general pattern: when the function is iterated infinitely many times, we are allowed to
% "peel off" the outer layer of the evaluation - the next inner layer will still have infinitely 
% many nested layers of evaluation. This would not be possible with only finitely many layers so the
% qualitytive step to go to infinity actually gives us an opportunity for simplification. That's
% pretty interesting because normally, going to infinity makes things more complicated. Well - not
% always but often.


% https://www.youtube.com/watch?v=bEZ6JLLjM3Y  The beauty of Fixed Points

% https://en.wikipedia.org/wiki/Contraction_mapping
% Contraction: d(f(x), f(y)) < k d(x,y)  where k is a constant with 0 <= k < 1. Why is it not enough
% to require d(f(x), f(y)) < d(x,y)? Maybe consider a function from R to R with some point x_0 and 
% an f that lets the points approach x_0 - eps_L  when x comes in from the left and to x_0 - eps_R 
% when y comes in from the right. Then, the distance between x and y in repeated iteration will
% indeed shrink in every iteration, i.e. d(f(x), f(y)) < d(x,y)  will be satisfied. But I think,
% we cannot find a constant k such that  d(f(x), f(y)) < k d(x,y) will always be satisfied. Not sure
% if that makes sense though. What if our inital values x,y are inside x-eps_L ... x+eps_R? Then
% d(f(x), f(y)) < d(x,y) will have to be violated as well ...or maybe not? Try to create a 
% counter example for f such that only the stricter definition achieves what we want but the looser
% definition doesn't. Maybe f need to be piecewise defined for inside and outside the interval
% [x-eps_L, x+eps_R]




%===================================================================================================
%\subsection{Sinkhorn Limits}
\subsection{Sinkhorn Limits and Kruithof's Method}


% Google AI overview for "sinkhorn limit"

% The Sinkhorn limit, also known as the Sinkhorn-Knopp limit, is the result of iteratively scaling the rows and columns of a positive square matrix until it becomes doubly stochastic (where row and column sums are all equal to one). This iterative process, often called the Sinkhorn-Knopp algorithm, is known to converge to a unique doubly stochastic matrix, which is the Sinkhorn limit. The Sinkhorn limit has applications in various fields like transportation, economics, image processing, and machine learning. 

% Iterative Scaling: The Sinkhorn-Knopp algorithm starts with a matrix where all entries are positive. In each iteration, it scales the rows of the matrix by a factor such that the row sums become equal to 1. Then, it scales the columns of the resulting matrix by a factor such that the column sums become equal to 1. 

% The process of iteratively scaling rows and columns continues until the matrix becomes doubly stochastic. The resulting matrix is the Sinkhorn limit. 

% For any positive square matrix, there is a unique Sinkhorn limit. 



% My text:
% For the Kruithof method, we do not necessarily require the row-sums and column-sums to be all one. Instead, we prescribe an arbitrary desired sum for each row and each column - so it's more general. The generalized Sinkhorn limits can also be called Kruithof limits.

% https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem



%===================================================================================================
%\subsection{Infinite Convolutions}

% Researchers thought this was a bug (Borwein integrals)
% https://www.youtube.com/watch?v=851U557j6HE


%===================================================================================================
%\subsection{Picard Iteration}

% https://de.wikipedia.org/wiki/Picard-Iteration
% https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem


\begin{comment}

ToDo:
-Infinite tetration
-Function sequences that are defined by infinite processes
 Examples: Bolzano function, Weierstrass function, ...
-Infinite convolutions (central limit theorem) - can also be framed as recursively defined sequence
 but this time, the sequence arguments are functions rather than numbers and the operations that we
 do on them may involve operators such as integration.
-Infinitely repeated integration
-Integrals over infinite sequences of functions:
 See https://www.youtube.com/watch?v=851U557j6HE  
     Researchers thought this was a bug (Borwein integrals)
-Mention Picard iteration: 
 https://de.wikipedia.org/wiki/Picard-Iteration
 https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem
 https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method
 ...but it actually can only be properly understood after the section about ODEs. Maybe change the 
 order of the sections of make the Picard iteration note one with a star (i.e. can be skipped
 on first read)
-Space filling curves

Maybe make a section about continued fractions. See:

Die geheime Macht der KettenbrÃ¼che fÃ¼r unmÃ¶gliche Gleichungen
https://www.youtube.com/watch?v=2qTgi4nq9TA


...what about infinte radicals? Or generally infinitely nested functions? Maybe take the 
arithmetic/geometric mean as example. The general theme is to have some (possibly recursive) rule to
create an infinite sequence of numbers that converges to some specific number. In series and
products, the rule is defined via a summation or product respectively. Maybe make a section "Other Infinite Sequences" or "...Computations". Maybe explain also the Newton iteration there. Maybe first
frame sums and product in terms of recursion, i.e. S_{n+1} = S_n + f(n) or something like that and products similarly. Another example is the Fibonacci sequence - and the quotient of successive terms (which converges to the golden ratio) - it also domenostrates something lese: take an existing sequence (here the Fibonacci numbers) and create a new sequence from it (here the ratios of successive terms)

-we will come full circle back to sequences - an earlier section of calculus

But maybe that's a topic for discrete calc - in the theory of recurrence relations



These Limits Are Too Complicated for Calculus
https://www.youtube.com/watch?v=-uIwboK4nwE
-It's about a certain operation on matrices that is iterated infinitely many times and converges to
 a particular matrix
-It's called Kruithof's Double Factor (KDF) method or iterative proportional fitting (IFP) procedure 
 https://en.wikipedia.org/wiki/Iterative_proportional_fitting
 https://ieeexplore.ieee.org/document/6366034
-This paper belongs to the video:
 "The entries of the Sinkhorn limit of an m x n matrix"  https://arxiv.org/abs/2409.02789


https://en.wikipedia.org/wiki/Bailey%E2%80%93Borwein%E2%80%93Plouffe_formula
https://en.wikipedia.org/wiki/Spigot_algorithm


https://en.wikipedia.org/wiki/Vi%C3%A8te%27s_formula
-Infinite product formula for pi where each factor contains an increasingly deeply nested 
 square-root expression.


\end{comment}