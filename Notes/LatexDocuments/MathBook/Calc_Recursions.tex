\section{Infinite Anythings}
After having seen infinite sums and infinite products, we'll now do a thing that mathematicians love to do: Generalize. We have defined an infinite sum via a limit over the partial sums $s_n$. The computation of such an $s_n$ was a finite sum which we already knew how to do. There is, however, another way to look at it. The computation of $s_n$ can also be seen as a formula involving the previous sum $s_{n-1}$ plus some new "update" or "correction" term $a_n$, i.e. $s_n = s_{n-1} + a_n$. For this $a_n$ term, we typically had an explicit formula involving $n$, i.e. $a_n = f(n)$ for a given function $f(n)$. With infinite products, the situation was similar with the only difference that we had a sequence of (finite) products $p_n$ and the update rule was $p_{n+1} = p_n \cdot a_n$. The general pattern here is that we compute the next term in our sequence (of partial sums or products) via some rule that takes in one (or, more generally, more) previous values of the sequence and possibly the index $n$ and computes the next value in this sequence. That is: we are dealing with sequences that can be produced via some recursive rule of the general form:
\begin{equation}
a_{n+1} = f(n, a_n, a_{n-1}, a_{n-2}, \ldots, a_0)
\end{equation}
This form is very general and the actual rule will often be simpler than that in practical cases. Often, the dependency will only have a finite window of memory. For example, $a_{n+1}$ may depend only on $a_n, a_{n-1}$ but not on even "older" values of the $a$ sequence. But for greatest generality, we allow it to depend on all previous values of the sequence and we also allow an explicit dependency on the index $n$ to be able to include the infinite sums and products into this more general framework. In infinite sums and products, we usually do have such an explicit dependency on $n$. ...TBC...

% ToDo: 
%
% - Maybe in the explanation of the sums, don't introduce this intermediate sequence a_n as in
%   the formula $s_n = s_{n-1} + a_n$. Instead frame it as $s_n = s_{n-1} + f(n)$ directly. Give
%   an example for how an original sum can be transformed into such a recursive representation. We
%   just need to extract the formula f(n) and the initial value. Maybe take the example of the
%   sum $s = \sum_{n=1}^{\infty} \frac{1}{n^2}$. Here, we have $f(n) = \frac{1}{n^2}$ and the
%   initial value $s_1$ for $n=1$ is $a_1 = 1$
%
% - Or, maybe make a subsubsection in the "Recursively Defined Sequences" subsection for how to
%   transform infinite sums and products into this recursive form
%
% - Maybe, for consistency with previous secstions, use k instead of n for the index


%===================================================================================================
\subsection{Recursively Defined Sequences}

% Maybe start with the Babylonian algorithm applied to 2 as motivating example - without yet mentioning the name.

\paragraph{Computing Square Roots}
As a motivating example, consider the following rule to compute a new term in a sequence from a previous one:
\begin{equation}
a_{n+1} = \frac{1}{2} \left( a_n + \frac{c}{a_n}  \right)
\end{equation}
For some given constant number $c$ and some given initial value $a_0$. Here, $a_{n+1}$ only depends on the sequence value immediately before it, i.e. $a_n$, and there is no explicit dependency on $n$ in the formula. Let's see what this rule produces for $c = 2, a_0 = 3$ with SageMath:
\begin{center}
\begin{tabular}{ ccccc } 
\begin{lstlisting}
c = 2.0
a = 3.0
for i in range(1,8):
    a = (a + c/a) / 2
    print(a)
\end{lstlisting}
& & & &
\begin{lstlisting}
1.83333333333333
1.46212121212121
1.41499842989480
1.41421378004720
1.41421356237311
1.41421356237309
1.41421356237309
\end{lstlisting}
\end{tabular}
\end{center}
% See:
% https://stackoverflow.com/questions/3220121/verbatim-environment-inside-latex-cell
After the 6th iteration, the sequence has converged to a value that will thereafter remain the same in the first 15 decimal digits. The value that this sequence converges to is none other than the square root of two. More generally, for any given positive real number $c$, the sequence of numbers defined by this algorithm will converge to $\sqrt{c}$ regardless of the initial value $a_0$ as long as it is positive [VERIFY!]. Let's try to figure out why ...TBC...

% ToDo:

% The explanation why should be deferred to the discussion of Newton-Raphson iteration - or maybe provide a geometric explanation here. Start with a rectangle of sidelengths 1 and c...see Weitz' video

% Maybe mention that this can indeed be a practically useful algorithm for evaluating square roots - or for refining an existing estimate of the square root. Maybe mention the famous fast inverse square root algorithm from the Quake codebase. Maybe mention startegies to come up with a good starting value. Maybe this may involve bit-twiddling trickery with IEEE-754 floating point numbers. For arbitrary pecision arithemtic, we may use initial values computed from converting results in machine precision and then improve the precision on them.



%---------------------------------------------------------------------------------------------------
\subsubsection{Convergence}
...TBC...

%---------------------------------------------------------------------------------------------------
\subsubsection{Fibonacci Numbers}
...TBC...

% the sequence itself diverges but the sequence of ratios of successive terms converges to the golden ratio

% Maybe mention also Lucas numbers - they use the same ruel with different initial values.

\paragraph{The Golden Ratio}
Although the sequence of Fibonacci numbers itself diverges to infinity, we can construct a new sequence from it that does converge - and it does so to a rather interesting number. ...TBC...


%---------------------------------------------------------------------------------------------------
\subsubsection{Newton-Raphson Iteration}
...TBC...

%\paragraph{The Babylonian Algorithm}

% https://en.wikipedia.org/wiki/Square_root_algorithms#Heron's_method

% Use it to compute the Lambert-W function. Apply it to y = x e^x  ->  x e^x - y = 0 where y is
% just a constant in this context

% Rigging Newton's Method | #SoME4
% https://www.youtube.com/watch?v=I2sjchgXsmk


%---------------------------------------------------------------------------------------------------
\subsubsection{The Arithmetic-Geometric Mean}
Consider the following recursive rule for a pair of numbers:
\begin{equation}
\begin{pmatrix} 
a_{n+1} \\ 
b_{n+1}
\end{pmatrix} 
= 
\begin{pmatrix} 
(a_n + b_n)/2 \\ 
\sqrt{a_n \cdot b_n}
\end{pmatrix} 
\end{equation}
That is, we have a recursive rule that produces a vector from a previous vector. The first component of the next vector is the arithmetic mean of the components of the current vector. The second component is the geometric mean. Let's see what this rule produces for the initial values $a_0 = 2, b_0 = 8$:
\begin{center}
\begin{tabular}{ ccccc } 
\begin{lstlisting}
a = 2.0
b = 8.0
for i in range(1,7):
    t = (a+b)/2
    b = sqrt(a*b)
    a = t
    print(a, b)
\end{lstlisting}
& & & &
\begin{lstlisting}
5.00000000000000 4.00000000000000
4.50000000000000 4.47213595499958
4.48606797749979 4.48604634366366
4.48605716058173 4.48605716056869
4.48605716057521 4.48605716057521
4.48605716057521 4.48605716057521
\end{lstlisting}
\end{tabular}
\end{center}
Apparently both vector components converge to the same value of around $4.486$. A value that is somewhere in between the arithmetic mean $5$ and geometric mean $4$ of the initial values $2$ and $8$. This common limit of both vector components is called the arithmetic-geometric mean. ...TBC...ToDo: explain applications, explain how the vector iteration can be embedded into the current framework of sequences by "zipping"

% Maybe move the print call into the 1st line of the loop such that we also print out the initial
% values. Do this also for the sqrt-algorithm

% Frame this in the following way: we start with a vector-valued recursion formula:
% (a_{n+1}, b_{n+1}) = ((a_n + b_n)/2, sqrt{a_n b_n})
% but this can be reformulated within a framework of scalar-valued sequences by just "zipping" the a_n, b_n sequences into a single sequence. The update rule may then be of the form a_{n+1} = ...for n even, ....for n odd - or something like that. It may be inconvenient but it can be done.

% What happens if we apply the same idea to other pairs of means? Try the arithmetic-harmonic mean.

% https://en.wikipedia.org/wiki/Geometric%E2%80%93harmonic_mean
% 


% https://en.wikipedia.org/wiki/Arithmetic%E2%80%93geometric_mean
% "The arithmetic–harmonic mean is equivalent to the geometric mean." (section of "Realted Concepts")
% ...is it generally true that for the generalized means G_m(x,y) and G_n(x,y), the mutual limit of
% the recursion is G_{(m+n)/2}(x,y). By generalized mean, I mean ((x^n + y^n)/2)^{1/n}. Try it numerically! The fact that the arithmetic-harmonic mean is equal to the geometric mean seems to suggest that. The arithmetic mean would be G_1(x,y), the harmonic mean G_{-1}(x,y) and the geometric mean G_0(x,y). If so, that would mean that the arithmetic-geometric mean would be G_{1.5}(x,y) = ((x^{3/2} + y^{3/2})/2)^{2/3}. That would be a closed form solution for the AGM which may be useful sometimes.

% https://mathworld.wolfram.com/Arithmetic-GeometricMean.html





%===================================================================================================
\subsection{Infinitely Nested Radicals}
Let's assume that we have some given constants $a,b$ and consider the following expression to compute a value $x$:
\begin{equation}
 x = a + b \cdot \sqrt{a + b \cdot \sqrt{a + b \cdot \sqrt{\ldots}}}
\end{equation}
Can we make sense of such an expression involving infinitely nested radicals (aka roots) and maybe even find a value of $x$ that satisfies this equation? By inspection, we may realize that the expression inside of the outermost square root is actually the same as the whole right hand side expression which, by virtue of the equation, is supposed to be equal to the left hand side $x$. So we may rewrite the equation as:
\begin{equation}
 x = a + b \cdot \sqrt{x}
\end{equation}
That looks a lot simpler! With a little bit of algebra we can bring it into the standard form of a reduced quadratic equation:
\begin{equation}
x^2 - (2a + b^2) x + a^2 = 0
\end{equation}
which we can solve for $x$ via the $pq$-formula to obtain:
\begin{equation}
x_{1,2} = k \pm s 
\quad \text{where} \quad
k = a + \frac{b^2}{2}, \;
s = \sqrt{k^2 - a^2}, 
\end{equation}
But we need to be a bit careful with this result. The "little bit of algebra" involved a step where both sides of an equation had to be squared. Such steps may produce extraneous solutions. We need to try both signs for $s$ and pick the one that produces a value $x$ that does indeed solve the equation. For some combinations of $a,b$ we may not get any real solution at all. For $a = 3, b = 2$, the formula produces the solutions $x = 1, x = 9$ and of these, only the $x=9$ solution actually works. With $a = -2, b = 3$, we get $x = 1, x = 4$ as solutions and they are both true solutions. But be that as it may. It's interesting to note here that no actual calculus tools were needed to find the solution(s). We don't evaluate any limit here. What we did instead was to observe that in an infinite nesting of functions, we are allowed to "peel off" the outermost layer of function evaluation, if we see by inspection that the argument of the function is the same as the function's output. In fact, we can peel off any finite number of layers but doing it only for the outermost one was the simplest choice here. This act of peeling off was legal only due to the \emph{infinite} nesting. For a finite nesting, we would not have been able to do that. We recognized that one part of an infinite expression was equal to the expression as a whole and took advantage of this self-similarity to simplify the expression. This is a general technique that we should add to our problem solving toolkit. It feels like lying somewhere in between an algebraic and a calculusly technique. It has to do with taking a process to infinity (which is what we do in calculus) but also with replacing a subexpression with some other subexpression which we know to be equivalent (which is what we do in algebra).

%...TBC...Mention other kinds of infinite expressions involving roots


% which one that is may depend on the signs of a and b? Or maybe on the sign of k? Figure out! Give an example for a triple a,b,x that works. a = 3, b = 2, x = 9 works. Ah - with a = -2, b = 3, we actually get two real solutions at x = 1 and x = 4. So maybe different cases are possible? Sometimes the + solution, sometimes the - solution sometimes both and sometimes none? Figure out! With a = -1, b = 1, we get no (real) solution. With a = -1.6, b = 2.5, we get close to a tangency solution. I think, when the - solution works, the + solution will also work but not the other way around. The - solution produces the smaller value - but if the sqrt-like graph crosses the diagonal once, it must cross it again due to the slope for sqrt going down when we move rightward. So, I think, the case that the - solution works but the + solution doesn't cannot occurr.


% https://en.wikipedia.org/wiki/Nested_radical#Infinitely_nested_radicals

% https://mathworld.wolfram.com/NestedRadical.html

% https://math.stackexchange.com/questions/4049946/the-infinitely-nested-radicals-problem-and-ramanujans-wondrous-formula

% https://en.wikipedia.org/wiki/Quadratic_equation#Reduced_quadratic_equation

\begin{comment}

To solve it, use the Sage code:

a = 3
b = 2
k = a + b^2/2
s = sqrt(k^2 - a^2)
x1 = k + s
x2 = k - s
y1 = a + b * sqrt(x1)
y2 = a + b * sqrt(x2)
N(x1),N(y1), N(x2),N(y2)

which gives:

(9.00000000000000, 9.00000000000000, 1.00000000000000, 5.00000000000000)


Check the formula against Mathematica's result:

https://www.wolframalpha.com/input?i=solve+x+%3D+a+%2B+b+sqrt%28x%29+for+x

Here is a plot of  f(x) = x  and  f(x) = a + b sqrt(x)  with adjustable a,b:
  https://www.desmos.com/calculator/epu6yonem9
The intersection of the graphs should be our solution

\end{comment}




%===================================================================================================
\subsection{Infinite Power Towers}
Let's next consider a power tower of infinite height, aka infinite tetration. We want to make sense of an expression of the form:

\begin{equation}
f(x) = x^{x^{\cdot^{\cdot^{\cdot^x}}}}
%x^{x^{\cdot^{\cdot^{\cdot^{x^{x^x}}}}}}
% x^{x^{x^{x^{x^{x^{x^{x^{x^x}}}}}}}}
\end{equation}
% https://tex.stackexchange.com/questions/144490/how-do-i-typeset-a-tenfold-powering-a-tower-of-powers-with-latex
...TBC...


%===================================================================================================
\subsection{Continued Fractions}

% This LaTeX operator definition code has bee adapted from the last answer (Dec 2, 2014 at 15:45 by dremondaris) here (A replaced by K):

% https://tex.stackexchange.com/questions/23432/how-to-create-my-own-math-operator-with-limits/23436#23436

% ToDo: move this to Setup.tex

% This allows us to turn arbitrary letters into math-symbols with lower and upper limits similar to the Sigma and Pi notaion for sums and products:
\newcommand{\operator}[1]{\mathop{\vphantom{\sum}\mathchoice
{\vcenter{\hbox{\huge $#1$}}}
{\vcenter{\hbox{\Large $#1$}}}{#1}{#1}}\displaylimits}

% This uses the above code to define a big K operator form continued fractions:
\newcommand{\opK}{\operator{\mathrm{K}}}
% It can be used like \opK_{i=1}^{\infty} \frac{a_i}{b_i}



Consider defining a nonegative real number $x \in \mathbb{R}_0^+$ by an expression of the form:
\begin{equation}
\label{Eq:ContinuedFraction}
x = b_0 + \cfrac{a_1}{b_1 + \cfrac{a_2}{b_2 + \cfrac{a_3}{b_3 + \ddots }}}
\end{equation}
where the dots mean, as usual, that we imagine the pattern to go on forever. Such an expression is called a \emph{continued fraction} expansion (henceforth abbreviated as CFE) of the number $x$. The numbers $b_i, i = 0,1,2,3,\ldots$ and $a_j, j = 1,2,3,\ldots$ are given sequences of coefficients. These numbers are typically assumed to be integers. The $a_j$ are called \emph{partial numerators} and the $b_i$ are called \emph{partial denominators}. Because the notation above is rather wasteful with precious space, there are alternative notations for writing down a continued fraction. Among them are:
\begin{equation}
x = b_0 + \frac{a_1}{b_1 +} \, \frac{a_2}{b_2 +} \, \frac{a_3}{b_3 +} \cdots, \quad
x = b_0 + \frac{|a_1}{b_1|} + \frac{|a_2}{b_2|} + \frac{|a_3}{b_3|} + \cdots, \quad
x = b_0 + \opK_{i=1}^{\infty} \frac{a_i}{b_i}
\end{equation}
The last one was introduced by Gauss and is analogous to the $\sum$ notation for sums and the $\prod$ notation for products\footnote{which both were introduced by Euler. Somehow it's always Euler or Gauss.}. The big $\opK$ stands for "Kettenbruch" which is the German translation for continued fraction (literally: chain fraction). The first coefficient $b_0$ has to be outside the $\opK$ because it doesn't really fit nicely into the general pattern. It has no $a_0$ to pair with. It is typically taken to be the integer part of the number $x$, i.e. $b_0 =\floor{x}$ and we usually also assume that the whole $\opK$ expression is less than $1$ such that it represents the fractional part of $x$, i.e. $\opK_{i=1}^{\infty} \frac{b_i}{a_i} = x - \floor{x} < 1$ [VERIFY!]. In an environment where no mathematical typesetting is available, I like to write continued fractions as:
\begin{equation}
x = b_0 + a_1 / (b_1 + a_2 / (b_2 + (a_3 /  (b_3 + (\cdots))))) 
\end{equation}
which bears some similarity to the nested form of writing down a polynomial using Horner's rule. 

...TBC... ToDo: explain terminology: generalized vs simple (used differently in calculus and number theory), explain conventions to ensure that only proper fractions occur, explain how to deal with negative $x$, give $(b_0;b_1,b_2,b_3,\ldots)$ notation for simple CFEs (i.e. with all $a_j = 1$), mention Hirzebruch-Jung CFEs (where all $a_j = -1$).


\subsubsection{Naive Evaluation}
To understand how we would go about evaluating such an infinite continued fraction, let's first consider a finite version of this situation. Let's assume that in the expression in equation (\ref{Eq:ContinuedFraction}), the partial numerator $a_4$ (i.e. the first one that would occur in the "dotdotdot" subexpression) would be zero. That would mean that the whole dots subexpression would also evaluate zero. Then we could evaluate the expression from the "bottom up" or from the "inside out" or "backwards". When simplifying the resulting mess, we will end up with a regular old fraction aka a rational number in standard form. ...TBC...ToDo: give naive (backward) evaluation algorithm as SageMath/Python code, expand the first few finite cases (up to $n=3$) into regular fractions


% Give also the notation:
% x = (a_0; a_1, a_2, \ldots, a_n)
% and Gauss's big-K notation similar to Sigma amd Pi for sums and products, see:
% https://tex.stackexchange.com/questions/540735/how-to-format-k-notation-for-continued-fractions
% https://math.stackexchange.com/questions/2179834/weird-large-k-symbol
% https://tex.stackexchange.com/questions/73195/how-to-typeset-a-continued-fraction-in-the-following-format
% https://tex.stackexchange.com/questions/23432/how-to-create-my-own-math-operator-with-limits/23436#23436
% The K here stands for Kettenbruch (the German translation of continued fraction, literally chain fraction)


\paragraph{Convergents and Continuants}
Let's write down the first couple of truncated CFEs in simplified form, i.e. in the form of a simple fraction with a single numerator and single denominator and assign the names $A_n, B_n$ to the resulting numerators and denominators and the name $x_n$ to the finite fractions that we get:
\begin{equation}
\label{Eq:ContinuantsAndConvergents}
x_0 = \frac{A_0}{B_0} = b_0, \quad
x_1 = \frac{A_1}{B_1} = \frac{b_1 b_0 + a_1}{b_1}, \quad
x_2 = \frac{A_2}{B_2} = \frac{b_2(b_1 b_0 + a_1) + a_2 b_0}{b_2 b_1 + a_2}, \quad
\ldots \quad
x_n = \frac{A_n}{B_n}
\end{equation}
The numbers $A_n, B_n$ that we can create in this way are called the $n$-th order \emph{continuants} and the numbers $x_n$ are called the $n$-th order \emph{convergents} of the CFE. The latter name rightfully suggests that the sequence of $x_n$ converges - namely to $x$ - which is the whole point of all of this business. [Q: Are there any conditions that need to be satisfied for convergence?] ...TBC... ToDo: Explain the sense in which the convergents are the best rational approximations to $x$, mention the $22/7 = 3 + 
1/7$ approximation for $\pi$

% Google AI says:
% "Continuants also have combinatorial interpretations, such as being the sum of monomials corresponding to configurations on a frieze,"
% when being prompted with "continued fraction continuants"

% It also says:

%Recursive Relation:
%For a simple continued fraction [a₀; a₁, a₂, ...] (where aᵢ are coefficients), the numerators Aᵢ and denominators Bᵢ of the convergents are defined by the following recurrence relations:
%Aₙ = aₙ Aₙ₋₁ + Aₙ₋₂
%Bₙ = aₙ Bₙ₋₁ + Bₙ₋₂
%With initial values A₋₂ = 0, A₋₁ = 1 and B₋₂ = 1, B₋₁ = 0.

% This formula is different from what Wikipedia says. I think Wikipedia may be wrong? Check that! Compare it to other sources and try to re-derive the recursion. On page 11 in Jones/Thron "Continued Fractions..." is the same formula as in Wikipedia. They are probably taken from there

% See also:
% https://en.wikipedia.org/wiki/Continuant_(mathematics)

\subsubsection{Forward Evaluation}
While it is totally possible to evaluate a finite continued fraction with the naive backward evaluation algorithm given above, there is actually also a way that avoids starting at the innermost subexpression and working our way back to the outermost. I will refer to this alternative evaluation algorithm as forward evaluation. Such an algorithm will turn out to particularly convenient when we want to evaluate infinite CFEs because in this case, there isn't any innermost subexpression. ...TBC...



\paragraph{Fundamental Recurrence Formulas}
Let's take a closer look at equation (\ref{Eq:ContinuantsAndConvergents}). It may not be totally obvious, but it turns out that the continuants satisfy the following 3-term recurrence relations:
\begin{equation}
A_n = b_n A_{n-1} + a_n A_{n-2}, \quad
B_n = b_n B_{n-1} + a_n B_{n-2}  \quad
\text{for }  n \geq 1
\end{equation}
These formulas are rather important and are therefore called the \emph{fundamental recurrence formulas}.
To get the recursion started, we will also need the initial values. These are given by:
\begin{equation}
A_{-1} = 1, A_0 = b_0, B_{-1} = 0, B_0 = 1
\end{equation}
The recurrence formulas are the key for an alternative evaluation algorithm that allows us to evaluate the CFE in a forward manner, i.e. in a way that allows use to produce finer and finer approximations of $x$ without knowing all coeffs up to $n$ upfront. We just need to keep track of the continuants $A_{n-2}, A_{n-1}, B_{n-2}, B_{n-1}$ and when a new pair of partial numerators and denominators $a_n, b_n$ "arrives", we can use these coefficients together with the stored continuants to "update" or refine our approximation of $x$ from the previous convergent $x_{n-1}$ to the new convergent $x_n$ by using these recurrence formulas for the continuants and then just compute $x_n = A_n/B_n$. ...TBC...ToDo: Give SageMath/Python code for forward evaluation


% ToDo: Give all the different notations, explain different terminologies in number theory and analysis for the qualifiers "simple" and "generalized", Should be impose the constraint that $a_i < b_i$ to ensure each partial fraction is a proper fraction?

% Motivate CFEs as a bridge between the rational and the real numbers. It uses potentially infinite sequences of integer numbers to approximate real numbers with increasing accuracy. Actually, decimal or binary or hexadecimal expansions do the same thing but in a different way. There, we represent real numbers as  sum_k d_k b^k  where the d_k are the digits and b is the base. Wit a CFE, the representation is given by a somewhat more complicated conversion algorithm. It may be more natural (because it doesn't require us to introduce an arbitrary base) and maybe more efficient (in the sense of having a better accuracy for a given number of coeffs? I'm not sure about that, though - it may depend on the number - maybe try it for the golden ratio which is the worst case for a CFE). Also, a CFE does not have the problem of becoming potentially infinite when the number x is rational. In any given base, some rational numbers will have infinite expansions (they will be periodic, though). Maybe give 1/7 as example ...TBC...

% See: 
% Warum ist 1/7 interessanter als 1/13? (Woher kommen die Nachkommastellen?)
% https://www.youtube.com/watch?v=ThIlhNKzBks
%

% Give the algorithm to produce a CFE as Sage code. See the C++ implementation if the research repo. Maybe start with a numerical approximation of pi using x = N(pi, 50) or something. Do it also for the golden ratio and maybe some other interesting irrational numbers

% See: https://oeis.org/A001203


% ...hmm...I'm not so sure if continued fractions and radicals fit in here because the upate rule actually is not so simple. We have to evaluate the whole expression from the inside out implying that we must do a complete re-evalutation of everything for each new $n$. However, in the beginning we said that the update rule may involve more than one previous values, so maybe we can us that as justification? ..But hey! We are actually "outside" the recursion subsection here - so we do not need the content to fit in!


% https://en.wikipedia.org/wiki/Continued_fraction#Formulation
% fundamental recurrence formulas

%\begin{equation}
%x = b_0 + \frac{a_1}{b_1 + \frac{a_2}{b_2 + \frac{a_3}{b_3 + \ddots }}}
%\end{equation}
%% That looks ugly!

% https://en.wikipedia.org/wiki/Continued_fraction#The_determinant_formula

\subsubsection{Obtaining the CFE}
Let's now look at the problem to obtain a continued fraction expansion for a given number $x$. The first $b$-coeff is easy: it's just $b_0 = \floor{x}$. ...TBC...ToDo: Give Python code to produce a CFE for a given $x$, give a couple of interesting/useful CFEs for important mathematical constants such as $\e, \pi, \varphi, \ldots$, explain how the size of the coeffs determines the error of the approximation, explain why the golden ratio $\varphi$ is the worst case for a CFE

% When we include a big coeff, the error decreases a lot. That is also the reason why the golden ratio is the hardest number to approximate by a CFE because it has all coeffs equal to 1, i.e. it doesn't have any larger coeffs.

% Explain the rate of convergence in the worst case, i.e. in the case of the golden ratio $\varphi$. Maybe give a plot of the approximation error

% Maybe put this section before the section about evaluation.



%\subsubsection{Composition of Linear Fractional Transformations}

% https://de.wikipedia.org/wiki/Kettenbruch#Darstellung_als_Komposition_von_Abbildungen


%\subsubsection{Orthogonal Polynomials}


%\subsubsection{Function Approximation}
% Explain hwo CFEs can be used to approximate functions


%\subsubsection{The Conservative Matrix Field}
% In a recent discovery, mathematicians have found a systematic way to produce CFEs for many interesting numbers. ...TBC...


% Arithmetic With... Continued Fractions?? #SoME2
% https://www.youtube.com/watch?v=O6Atodhpr1M

% Continued Fraction Arithmetic
% https://www.youtube.com/watch?v=tBc_xcRzMxk


% https://hsinhaoyu.github.io/cont_frac/
% https://arxiv.org/html/2412.19929v1#S3

% C-Library for continued fraction arithmetic:
% https://github.com/mjdominus/cf

% Another one:
% https://github.com/blynn/frac
% https://crypto.stanford.edu/pbc/notes/contfrac/   Infors about the code(?)

% Another one in Python:
% https://github.com/hsinhaoyu/cont_frac
% https://github.com/hsinhaoyu/cont_frac/blob/main/org/cont_frac.org  Article that explains it

% https://perl.plover.com/classes/cftalk/INFO/gosper.html  Gosper's original paper(?)
% I think, when transforming a rational number into a CFE, we can use the Euclidean algorithm for the gcd just that here, we are not only interested in the last result but in the whole sequence of intermediate results that is produced. Verify this!

% The same as .txt file:
% https://perl.plover.com/yak/cftalk/INFO/gosper.txt

% Another text by Gosper:
% https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/cont-frac-gosper-1.pdf

% https://arxiv.org/pdf/2412.19929
% https://rosettacode.org/wiki/Continued_fraction/Arithmetic
% https://medium.com/@omer.kasdarma/the-curious-world-of-simple-continued-fractions-part-v-the-arithmetics-b749a215383d

% Websites with infos:
% https://srossd.com/blog/2020/gosper-1/
% https://compasstech.com.au/gxwgosper/#top
% https://crypto.stanford.edu/pbc/notes/contfrac/

% Transcendental Functions on Continued Fractions
% https://arxiv.org/abs/2412.19929
% -Paper about improvements to Gosper's algorithm

%===================================================================================================
%\subsection{Iterated Function Systems}
\subsection{Fixed Point Iteration}
Consider the following situation: We are given a function $f(x)$ and an initial value $x_0$ and now we apply the function again and again to produce the next value. That means $x_1 = f(x_0)$, $x_2 = f(x_1) = f(f(x_0))$ and so on. As you may have guessed, we intend to repeat the application of $f$ to our previous result ad infinitum. With some functions $f$ and initial values $x_0$, this process may actually converge to a definite number. Let's call that number $x$, so we have by definition:
\begin{equation}
 x = f(f(f( \ldots f(x_0))))
\end{equation}
where the dots intend to mean that we apply $f$ infinitely often to $x_0$. Now remember what we did with the infinitely nested radical. We have a similar situation here, just more general. In our previous infinite radical situation, we had this same situation for the special case $f(x) = a + b \sqrt{x}$. When we peel off the outermost function application on the right hand side, what remains is \emph{still} an infinite number of function applications to $x_0$ and it is therefore still equal to $x$. That means $x$ must satisfy $x = f(x)$. A point $x$ with that property is called a \emph{fixed point} of the given function $f$. The name reflects the fact that applying $f$ to $x$ changes nothing. At the value $x$, the output of $f$ is equal to the input. Geometrically, it is a point of intersection between the graph of $f(x)$ and the graph of the identity function. Not all functions have such intersection points but some do. Some functions $f$ even have multiple such intersection points. For example, the graph of the function $f(x) = x^3$ intersects with the graph of the identity function $g(x) = x$ in the points $(-1,-1)$ and $(1,1)$ and it is indeed true that $(-1)^3 = -1$ and $1^3 = 1$.

% Maybe take 8 cos(x) as another example. It has 5 fixed points
%
%   https://www.desmos.com/calculator/di4hxswknp
%
% Maybe create an example where one of the fixed points is not an intersection point but a point
% of tangency. Maybe use k cos(x) and adjust k accordingly. k = 6.202 seems to work:
%
%   https://www.desmos.com/calculator/wzjyzxnp1r
%
% How would we find a formula for k? Tangency means that the derivative of f must be 1 at the
% fixed point.


%If it does, it means that this number $x$ must satisfy the condition $f(x) = x$. This can be seen by looking at the infinitely nested expression $x = f(f(f(\ldots f(x_0))))$. We may again peel off the outermost function application and recognize that what remains is equal to the whole right hand side. This is, of course, equal to the left hand side $x$, so we may conclude that the infinite expression is equivalent to the simple finite expression $x = f(x)$ ...TBC...


% https://en.wikipedia.org/wiki/Iterated_function_system

% Iterated Function Systems: A Comprehensive Survey
% https://arxiv.org/abs/2211.14661

% IMPLEMENTING ITERATED FUNCTION SYSTEMS IN PYTHON
% https://www.math.uni-rostock.de/~wj/projects/Fractals/160355141_Habib_Wahab_MTH6138_IFS.pdf

% Consider the equation  x = f(f(f(...f(x_0)))). Peeling off the outermost application of f in the 
% RHS, we note that, by the equation, we must have x = f(x). That means that the equation requires
% that x must be a fixed point of the function f. The is equivalent to requiring x - f(x) = 0 which
% is in the form suitable to hand over to a root finder ...tbc...

% That is a general pattern: when the function is iterated infinitely many times, we are allowed to
% "peel off" the outer layer of the evaluation - the next inner layer will still have infinitely 
% many nested layers of evaluation. This would not be possible with only finitely many layers so the
% qualitytive step to go to infinity actually gives us an opportunity for simplification. That's
% pretty interesting because normally, going to infinity makes things more complicated. Well - not
% always but often.


% https://www.youtube.com/watch?v=bEZ6JLLjM3Y  The beauty of Fixed Points

% https://en.wikipedia.org/wiki/Contraction_mapping
% Contraction: d(f(x), f(y)) < k d(x,y)  where k is a constant with 0 <= k < 1. Why is it not enough
% to require d(f(x), f(y)) < d(x,y)? Maybe consider a function from R to R with some point x_0 and 
% an f that lets the points approach x_0 - eps_L  when x comes in from the left and to x_0 - eps_R 
% when y comes in from the right. Then, the distance between x and y in repeated iteration will
% indeed shrink in every iteration, i.e. d(f(x), f(y)) < d(x,y)  will be satisfied. But I think,
% we cannot find a constant k such that  d(f(x), f(y)) < k d(x,y) will always be satisfied. Not sure
% if that makes sense though. What if our inital values x,y are inside x-eps_L ... x+eps_R? Then
% d(f(x), f(y)) < d(x,y) will have to be violated as well ...or maybe not? Try to create a 
% counter example for f such that only the stricter definition achieves what we want but the looser
% definition doesn't. Maybe f need to be piecewise defined for inside and outside the interval
% [x-eps_L, x+eps_R]

% What is cos( cos( cos( cos( cos( cos( cos( cos( cos( cos( cos( cos(…?? // Banach Fixed Point Theorem
% https://www.youtube.com/watch?v=qHnXE_h5c2M


%===================================================================================================
%\subsection{Sinkhorn Limits}
\subsection{Sinkhorn Limits and Kruithof's Method}


% Google AI overview for "sinkhorn limit"

% The Sinkhorn limit, also known as the Sinkhorn-Knopp limit, is the result of iteratively scaling the rows and columns of a positive square matrix until it becomes doubly stochastic (where row and column sums are all equal to one). This iterative process, often called the Sinkhorn-Knopp algorithm, is known to converge to a unique doubly stochastic matrix, which is the Sinkhorn limit. The Sinkhorn limit has applications in various fields like transportation, economics, image processing, and machine learning. 

% Iterative Scaling: The Sinkhorn-Knopp algorithm starts with a matrix where all entries are positive. In each iteration, it scales the rows of the matrix by a factor such that the row sums become equal to 1. Then, it scales the columns of the resulting matrix by a factor such that the column sums become equal to 1. 

% The process of iteratively scaling rows and columns continues until the matrix becomes doubly stochastic. The resulting matrix is the Sinkhorn limit. 

% For any positive square matrix, there is a unique Sinkhorn limit. 



% My text:
% For the Kruithof method, we do not necessarily require the row-sums and column-sums to be all one. Instead, we prescribe an arbitrary desired sum for each row and each column - so it's more general. The generalized Sinkhorn limits can also be called Kruithof limits.

% https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem



%===================================================================================================
%\subsection{Infinite Convolutions}

% Researchers thought this was a bug (Borwein integrals)
% https://www.youtube.com/watch?v=851U557j6HE


%===================================================================================================
%\subsection{Picard Iteration}

% https://de.wikipedia.org/wiki/Picard-Iteration
% https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem


\begin{comment}

ToDo:
-Infinite tetration
-Function sequences that are defined by infinite processes
 Examples: Bolzano function, Weierstrass function, ...
-Infinite convolutions (central limit theorem) - can also be framed as recursively defined sequence
 but this time, the sequence arguments are functions rather than numbers and the operations that we
 do on them may involve operators such as integration.
-Infinitely repeated integration
-Integrals over infinite sequences of functions:
 See https://www.youtube.com/watch?v=851U557j6HE  
     Researchers thought this was a bug (Borwein integrals)
-Mention Picard iteration: 
 https://de.wikipedia.org/wiki/Picard-Iteration
 https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem
 https://adamdjellouli.com/articles/numerical_methods/7_ordinary_differential_equations/picards_method
 ...but it actually can only be properly understood after the section about ODEs. Maybe change the 
 order of the sections of make the Picard iteration note one with a star (i.e. can be skipped
 on first read)
-Space filling curves

Maybe make a section about continued fractions. See:

Die geheime Macht der Kettenbrüche für unmögliche Gleichungen
https://www.youtube.com/watch?v=2qTgi4nq9TA


...what about infinte radicals? Or generally infinitely nested functions? Maybe take the 
arithmetic/geometric mean as example. The general theme is to have some (possibly recursive) rule to
create an infinite sequence of numbers that converges to some specific number. In series and
products, the rule is defined via a summation or product respectively. Maybe make a section "Other Infinite Sequences" or "...Computations". Maybe explain also the Newton iteration there. Maybe first
frame sums and product in terms of recursion, i.e. S_{n+1} = S_n + f(n) or something like that and products similarly. Another example is the Fibonacci sequence - and the quotient of successive terms (which converges to the golden ratio) - it also domenostrates something lese: take an existing sequence (here the Fibonacci numbers) and create a new sequence from it (here the ratios of successive terms)

-we will come full circle back to sequences - an earlier section of calculus

But maybe that's a topic for discrete calc - in the theory of recurrence relations



These Limits Are Too Complicated for Calculus
https://www.youtube.com/watch?v=-uIwboK4nwE
-It's about a certain operation on matrices that is iterated infinitely many times and converges to
 a particular matrix
-It's called Kruithof's Double Factor (KDF) method or iterative proportional fitting (IFP) procedure 
 https://en.wikipedia.org/wiki/Iterative_proportional_fitting
 https://ieeexplore.ieee.org/document/6366034
-This paper belongs to the video:
 "The entries of the Sinkhorn limit of an m x n matrix"  https://arxiv.org/abs/2409.02789


https://en.wikipedia.org/wiki/Bailey%E2%80%93Borwein%E2%80%93Plouffe_formula
https://en.wikipedia.org/wiki/Spigot_algorithm


https://en.wikipedia.org/wiki/Vi%C3%A8te%27s_formula
-Infinite product formula for pi where each factor contains an increasingly deeply nested 
 square-root expression.


\end{comment}