\section{Series}

\subsection{Sequences}
A \emph{sequence} can be defined as a function from the natural into the real numbers: $f: \mathbb{N} \rightarrow \mathbb{R}$. Sometimes the codomain can also be the complex numbers. The input can be seen as an index for an element of the sequence. Sometimes it is more convenient to restrict the domain to the positive natural numbers, i.e. consider functions $f: \mathbb{N}^+ \rightarrow \mathbb{R}$. You will find both definitions for sequences in the literature. Here, we will mostly use the former and I will give a special disclaimer when the latter is used. Some example sequences are:
\begin{equation}
 a_n = (-1)^n,           \quad 
 a_n = \frac{1}{n},      \quad
 a_n = \frac{(-1)^n}{n}, \quad 
 a_n = \frac{1}{n^2},    \quad
 a_n = \frac{1}{2^n},    \quad  
 a_n = \sqrt{n}
\end{equation}
The examples with an $n$ or $n^2$ in the denominator can only be meaningfully defined as functions from the positive naturals into the reals because otherwise we would have a division by zero for the first (actually zero-th) element, i.e. when $n=0$. For the last two, plugging in $n=0$ poses no problem because $2^0=1$ and $\sqrt{0} = 0$. So, for those which would have a division by zero for $n=0$, we would chose a domain of $\mathbb{N}^+$ while for the others, we could choose $\mathbb{N}$. We could also say that they are all defined on $\mathbb{N}$ and make a special case definition $a_0 = 0$ for the problematic ones. The function definition would then look a bit ugly but is entirely legitimate. When we deal with sequences, we will be mostly concerned with the question of \emph{convergence} for which it doesn't really matter, if finitely many elements in an initial section are ill defined, so we will mostly ignore this detail. Sequences themselves are a sort of preliminary to what we actually want to build up to: namely series. These are infinite sums of sequence elements.

\medskip
The notation for the whole sequence is $(a_n)$. That is: By $a_n$ we mean a specific element whereas by $(a_n)$ we mean the whole sequence of elements. In a sense, a sequence can be seen as an infinite tuple so using the parentheses notation known from tuples seems like an appropriate reminder.

% https://en.wikipedia.org/wiki/Sequence

\subsection{Operations}

\subsubsection{Pointwise Operations}
Just like we can do with any functions $f: \mathbb{R} \rightarrow \mathbb{R}$, we can of course also add, subtract, multiply and divide two functions $f: \mathbb{N} \rightarrow \mathbb{R}$ pointwise. The fact that the domain is restricted to the naturals does not change anything about that. That means: we can just perform our usual arithmetic operations on sequences element-wise.

\subsubsection{Convolution aka Cauchy Product}
Another binary operation that we can perform on two sequences $(a_n),(b_n)$ is \emph{convolution}, also known as the \emph{Cauchy product} between the sequences. We will denote the operation symbol for convolution by an asterisk $\ast$ and define the operation as:
\begin{equation}
 (c_n) = (a_n) \ast (b_n) = (b_n) \ast (a_n) \quad \text{with} \quad
 c_n = \sum_{k=0}^n a_k b_{n-k} 
     = \sum_{k=0}^n b_k a_{n-k}
     = \sum_{i+j=n} b_i a_j
\end{equation}
Each coefficient $c_n$ of the resulting sequence is given by a sum of products of coefficients from the original sequences and in each term of this sum, the indices of the two factors must sum up to $n$, the index in the $c_n$ sequence of the coefficient that is being computed. From this fact and the formula, you may already correctly infer that the operation is commutative.
[VERIFY! What about complex sequences?]. It is also associative and distributive over element-wise addition and subtraction (but not over element-wise multiplication and division - VERIFY). This distributivity, together with the fact that scaling one of the inputs by a factor yields an output scaled by the same factor constitutes the important property of \emph{bilinearity} [VERIFY! might be only sesquilinear in complex case?]. That means, the operation is linear in both of its input arguments. In the literature, especially the DSP literature, they will often say that convolution is \emph{linear} rather than \emph{bilinear}. I guess, this is because in this context, they view it as a single input operation because one of the two sequences is assumed to be fixed once and for all. In this context, the fixed sequence is often called the \emph{impulse response} and sometimes also as \emph{kernel}. But be careful: the term kernel has another meaning in math, too - namely the set of inputs to a function that get mapped to zero - but that other meaning has nothing to do with the meaning here\footnote{Yeah - I know - it's a mess!}. TODO: explain relation to multiplication of polynomials
% https://mathworld.wolfram.com/CauchyProduct.html

% https://en.wikipedia.org/wiki/Convolution#Discrete_convolution

% generalization:
% https://math.stackexchange.com/questions/4036715/definition-of-complex-convolution

% What about complex sequences? Do we need a conjugation on one of the terms?  Maybe convolution is only sesquilinear in this case? In Simon Haykin's "Adaptive Filter Theory", page 7, the convolution action of a transversal FIR filter has complex conjugation on the filter coeffs (and the lags on the input samples), i.e. y[n] = sum_k conj(w_k) x[n-k]

% In complex analysis, there doesn't seem to be a conjugation:
% https://math.stackexchange.com/questions/4653227/laurent-series-cauchy-product

% For infinite sequences, the Cauchy product converges if both factors converge absolutely (verify!). see Mertens theorem
% https://www.youtube.com/watch?v=zWBQXzjj5fY
% https://en.wikipedia.org/wiki/Cauchy_product#Convergence_and_Mertens'_theorem

\paragraph{Deconvolution}
With some caveats, it is actually possible to undo a convolution. We will call this operation \emph{deconvolution}. ...TBC...

\paragraph{Continuous Convolution}
As a side note, an analoguos operation of convolution can also be defined between two functions $f,g: \mathbb{R} \rightarrow \mathbb{R}$. In this case, the convolution is defined via an integral instead of a sum:
\begin{equation}
 h(t) = f(t) \ast g(t) = g(t) \ast f(t) \quad \text{with} \quad
 h(t) = \int_{-\infty}^{\infty} f(\tau) g(t-\tau) \, d \tau 
      = \int_{-\infty}^{\infty} g(\tau) f(t-\tau) \, d \tau
\end{equation}
This is not relevant in the context of series but it's sometimes good to point out the connections to other topics. I have used $t$ for the input and $\tau$ for the dummy integration variable here because this is the way, you often find it stated. This operation occurs a lot in (continuous time) signal processing where $t$ stands for time and $\tau$ for a time-lag or delay.

\paragraph{Dirichlet Convolution}
An operation that is of interest in the context of number theory and related to our regular old standard convolution is called Dirichlet convolution. It is defined as follows: 
\begin{equation}
 (c_n) = (a_n) \ast_D (b_n) 
       = \sum_{k | n} a_k b_{n/k} 
       = \sum_{ij = n} a_i b_j 
\end{equation}
The difference to regular convolution is that now the product of the indices $i$ and $j$ of the two factors must be equal to $n$. In regular convolution, it was the sum. The notation $\sum_{k | n}$ means that the index $k$ runs over all divisors of $n$ and $\sum_{ij = n}$ means that the sum runs of all pairs $i,j$ whose product $ij$ equals $n$. ...TBC...Is it also commutative? I think so. Explain how it arises from multiplying Dirichlet series in a similar way how normal convolution arises from multiplying polynomials. 

% Instead of considering a function $\sum_n a_n x^n$, we consider $\sum_n a_n n^x$...or actually  $\sum_n a_n n^{-x}$ but the minus does not matter for what happens to the coeffs in a multiplication of two such functions. In both types of convolutions, we want to collect the coeffs that multiply the same term, i.e. x^n or n^x respectively.





% https://en.wikipedia.org/wiki/Dirichlet_convolution

% https://www.youtube.com/watch?v=fGbJrY75LU8
% What is the Moebius function? #SoME4 #SomePi

% https://en.wikipedia.org/wiki/Dirichlet_series

% 

\subsection{Convergence}
When faced with a sequence of numbers, an important question is its behavior when the index $n$ approaches infinity. Specifically, we are interested, if the output of the sequence approaches some finite number or not. There are a couple of things that could happen when $n$ approaches infinity: (1) the numbers converge to some finite number, (2) the numbers diverge to infinity, (3) the numbers stay finite but jump around and approach nothing. In the third case, we will mostly find situations where the sequence alternates between positive and negative values. The example sequence $(-1)^n$ is of that kind because $(-1)^n$ will be $+1$ for even $n$ and $-1$ for odd $n$ and thereby induce this alternating behavior. The sequence $1/n$ approaches zero, so that would count as convergent. The sequence $(-1)^n / n$ is an alternating version of the former. It alternates between positive and negative values but the absolute values become smaller and smaller. It also converges to zero but in an alternating way. The sequence $1/n^2$ also converges to zero and does so even faster than $1/n$ and $1/2^n$ converges to zero yet faster. If a sequence $(a_n)$ converges to some value $a$, we will denote this as $a_n \rightarrow a$.

% ToDo: Bring the formal definition of convergence. Point out that the limit must be an 
% element of the original set of numbers (that's the subtle difference between convergence 
% and Cauchy sequences, I think)

% See also the paragraph about padic numbers - use a definition here that is consistent with the one there

%This sequence has a name: it's called the \emph{alternating harmonic series}.

%Theorems: pointwise sum of sequences converges to sum of limits, etc.

% https://www.youtube.com/watch?v=epejTvlWOXA
% -8:33: Merkregel: Bei Grenzwertprozessen kann aus "kleiner" "kleiner-gleich" werden. Bsp: Nimm
%  zwei relle konvergente Folgen a_n, b_n mit Grenzwerten a, b. Wenn a_n < b_n für fast alle n (also
%  alle bis auf endlich viele) dann ist a <= b. Das (a <= b) gilt auch immer noch, wenn wir nur
%  a_n <= b_n für fast alle n voraussetzen.

% Notation: a_n converges to a: a_n \rightarrow a

\paragraph{Convergence to Zero} The examples of convergent sequences that we have seen so far converge to zero. They are somewhat special in that regard because convergence to zero is a practically relevant special case of more general convergence. Consider $a_n = n/(n+1) = (0,1/2,2/3,3/4,4/5,\ldots)$. This sequence converges to $1$. The takeaway is that the sequences that converge to zero are an important subset of all the convergent sequences because convergence to zero of a sequence often appears as a necessary condition for certain other properties of interest to hold. We'll see this later in the context of series.


\subsubsection{Absolute Convergence}
An even stronger type of convergence is called \emph{absolute convergence}. If a sequence converges absolutely, it means that we can form a new series by taking the absolute values of terms of the original series and that new series still converges. We can intuitively see that this is a stronger requirement by noticing that when a series has positive and negative terms, they may partially cancel out thereby making the partial sums smaller compared to the case when all terms would have the same sign. ...TBC...
[WAIT: this belongs into the series section - not into the sequences section! For sequences, absolute convergence is not a meaningful concept. Or is it?]

\subsubsection{Rate of Convergence}
We already informally used statements like "converges faster". Now we'll make more explicit what we mean by that...TBC...

\subsubsection{Cauchy Sequences and Real Numbers}
A concept related to convergence is that of Cauchy sequences. When saying that a sequence of numbers converges to some limit, there is usually an implicit assumption that the limit is also an element of the same set of numbers as the individual sequence elements. But that isn't always the case. Consider the following recursive definition of a sequence: $a_0 = 2, a_{n+1} = (a_n^2 + 2) / (2 a_n)$ for $n > 0$. It converges to the irrational number $\sqrt{2}$ but every element of the sequence is a rational number. We may therefore consider it as a sequence of rational numbers - but one that doesn't converge \emph{within} the rational numbers. To express the limit, we need to extend our number system to include $\sqrt{2}$. We could form the set $\mathbb{Q} \cup \sqrt{2}$ and say that our sequence converges within that set. However, we are of course interested in a more general extension - one that works for every imaginable sequence and not just that particular one. That's how we arrive at the set of real numbers $\mathbb{R}$. ...TBC...

% No - that's not what "adjoin" means - adjoining sqrt froms the set a + b sqrt(2) for a,b, in Q

%https://www.youtube.com/watch?v=E-Tquvais4w
% Examples for non-convergent Cauchy sequences:
% Babylonian algorithm for sqrt - limit is outside Q, therefore formally not convergent
% in R and C, every Cauchy sequence is convergent (R and C are complete metric spaces)
% ...
% This may seem like nitpicking but it was important for the historical development. Imagine living in the 1800s - a time at which the real numbers were not yet defined satifactorily. It was known that irrational numbers exist - the irrationality of sqrt(2) was already known to the ancient greeks - but there wasn't yet an airtight definition for what the larger number space actually was. ...

% Häufungspunkt (accumulation point?)
% https://en.wikipedia.org/wiki/Accumulation_point

\subsection{Divergence}
By definition, any sequence that is not convergent is called \emph{divergent}. Perhaps counterintuitively, that includes bounded series that bounce around between two or more values but always stay within a finite interval. ...TBC...

\subsubsection{Divergence to Infinity}
If a sequence really eventually blows up to either positive or negative infinity, we say that this sequence \emph{diverges to infinity}. We may further specify the behavior by saying that it diverges to positive or negative infinity. Stated formally, convergence to positive infinity means that for any number $a$ that we may choose, no matter how large, we may find some index $N$ such that all $a_n > a$ for $n > N$ [VERIFY]. This means that the elements $a_n$ of the sequence become arbitrarily large as $n$ increases. Divergence to negative infinity is defined analogously: the elements become arbitrarily "small" - in the sense of arbitrarily negatively large. This notion "divergence to infinity" may be closer to our intuitive sense about what "divergence" should mean. But don't forget that the actual definition of divergence does not imply that kind of behavior. In a sense, this notion "divergence to infinity" might be seen as the counterpart to "convergence to zero": If $a_n$ diverges to (plus or minus) infinity, then $1/a_n$ converges to zero. For the converse, we must be a bit more careful - consider the alternating harmonic series as counterexample. ...TBC...

% Can this be leaborated? Maybe iff a_n converges to zero then b_n = 1/a_n diverges to infinity?

% "...ergibt nur Sinn für relle Folgen..." at 18:22:
% https://www.youtube.com/watch?v=fKI_e7KMj5Q

% What about "bestimmte Divergenz"
% https://de.wikibooks.org/wiki/Mathe_f%C3%BCr_Nicht-Freaks:_Bestimmte_Divergenz,_uneigentliche_Konvergenz

% https://math.libretexts.org/Bookshelves/Analysis/Real_Analysis_(Boman_and_Rogers)/04%3A_Convergence_of_Sequences_and_Series/4.03%3A_Divergence_of_a_Series

% I think, it might be called "divergence to infinity" - see:
% https://math.libretexts.org/Bookshelves/Analysis/Real_Analysis_(Boman_and_Rogers)/04%3A_Convergence_of_Sequences_and_Series/4.03%3A_Divergence_of_a_Series

% https://math.stackexchange.com/questions/52077/types-of-divergence


%\subsubsection{Examples}
%ToDo: give some more examples of absolute convergent, conditionally convergent and different kinds of divergent series (alternating but finite, to plus infinity, to minus infinity, altenating with absolute value going to infinity) - maybe also give complex examples. Maybe take ((2/3)(1+i))^n and
%((3/4)(1+i))^n. The former should converge, the latter diverge. Maybe argue via the geometric series and absolute value. Important in practice: a_n = n^k / b^n converges to zero for k in N, b > 1. Maybe |b| > 1 is enough? n-th root of a, a > 0 -> converges to 1.


% https://en.wikipedia.org/wiki/Antilimit


\subsubsection{Rules for Convergence and Divergence} 

\paragraph{Sum Rule}
Suppose we have two sequences $(a_n), (b_n)$ and form a third sequence by taking their pointwise sum: $c_n = a_n + b_n$. Then, if $(a_n)$ converges to $a$ and $(b_n)$ converges to $b$, then $(c_n)$ will converge to $a+b$. In math notation: $(a_n \rightarrow a, b_n \rightarrow b) \Rightarrow ((a_n + b_n) \rightarrow (a + b))$.

\paragraph{Comparison Test}
If we have a nonnegative sequence $(a_n)$ and want to know whether it converges or diverges, we can apply the following test: Suppose we have another nonnegative sequence $(b_n)$ which we already know to be convergent. Then, if we can find any index $N$ such that $a_n \leq b_n$ for all $n \geq N$, then $(a_n)$ is also convergent. Likewise, if we know that $(b_n)$ diverges to (positive) infinity and we can find an index $N$ such that $a_n \geq b_n$ for all $n \geq N$, then $(a_n)$ also diverges to (positive) infinity. You may sometimes find the latter statement stated like "if $(b_n)$ diverges, then $(a_n)$ also diverges" without the qualifier "to infinity". By our definition of divergence as mere absence of convergence without necessarily implying explosion to infinity, such a statement would be false: Consider as counterexample the divergent sequence $b_n = (-1)^n + 1$ that alternates between $0$ and $2$ and the sequence $a_n = 3n / (n+1)$ that converges to $3$. 

% This:
% https://www.sfu.ca/math-coursenotes/Math%20158%20Course%20Notes/sec_ComparisonTests.html
% actually says nothing about diverging "to positive infinity" but speaks about divergence in general. But intuitively, it is not so clear to me why that more general statement should hold. ...verify! If it does indeed hold for the more general notion of convergence, explain why
%
% https://en.wikipedia.org/wiki/Direct_comparison_test
%
% Try constructing a counterexample: take a series b_n that diverges but is bounded:
%   b_n = (-1)^n
% Now take a series that converges to 2:
%   a_n = (2 n) / (n+1)
% We actually do have a_n >= b_n but a_n does *not* diverge
% 


% Majorante/Minorante
% sum rule, product rule?



\subsection{Sequence Transformations}
A transformation of a sequence is, very generally speaking, a process that we apply to it to obtain a new sequence. For such a transformation to be interesting, we usually want it to satisfy some additional constraints. Most notably among them is the desire to not change the limit of a sequence in the case that such a limit exists, i.e. when the sequence is convergent. If our transformation has this desirable property of not affecting limits of convergent sequences, then we call the transformation \emph{regular} (VERIFY). Such regular transformations can help to speed up the convergence of a sequence which can be very helpful in practice when our task is to (approximately) evaluate the limit of a given sequence. They can even help to turn divergent sequences into convergent ones and thereby give us a potential way to "evaluate" limits of divergent sequences - although we need to be very careful with that. We will have to ask ourselves questions like whether or not this new "limit" is well defined in the sense that all different possible regular transformations will produce the same value. This is a tall order if we don't even know (yet) how the set of all possible regular transformations looks like (VERIFY)...TBC...

% https://en.wikipedia.org/wiki/Sequence_transformation
% binomial transform, Möbius transform, Stirling transform 
% https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process
% https://en.wikipedia.org/wiki/Series_acceleration

% Give examples for irregular transformations - taking the absolute value of the terms could be seen as such an example

% https://math.stackexchange.com/questions/1085570/when-do-regularization-methods-for-divergent-series-disagree

% using such techniques is sometimes calle "regularization"

% A Visual Attempt at 1 + 2 + 3 + 4 + 5 + ... = -1/12
% https://www.youtube.com/watch?v=hB2F9lyr2_k

\subsubsection{Convolution as Transformation}
One way to transform a sequence into a new one is to convolve it with some fixed other sequence. For a simple example, let this other sequence be given by $(\frac{1}{2}, \frac{1}{2}, 0, 0, 0, \ldots)$. In effect, the convolution with the given kernel will just let our new transformed sequence consist of 2-point averages of the old sequence. This will clearly not change the limit of a given sequence if it exists - because if the limit exists, it means that the sequence converges to some fixed value which also implies that two neighboring points will tend to have the same value - and taking the average of two values that are actually one and the same value will not change anything. So, we have our first example of a regular transformation. It's very simple and not very powerful. It will not help much in accelerating convergence of sequences - let alone turning divergent into convergent ones\footnote{Well - maybe it could actually do some good with alternating sequences. Figure out!}. But at least, we get a sense of how such regular transformations may look like. ...TBC... explain the general condition for a convolution kernel to be "regular" in the sense of not affecting the limit. I guess it must go down to zero (fast enough?) and sum up to one? I think, these two conditions should be sufficient for regularity - but are they necessary? 

\subsubsection{Transformation by Matrices}

% Series to Series Transformations and Analytic Continuation by Matrix Methods
% https://www.jstor.org/stable/2372347


\subsection{Infinite Sums aka Series}
When we have a given sequence $(a_k)$, we can use it to define a new sequence, namely the sequence of its partial sums. Let's call that sequence $(s_n)$. Its $n$th element is given by the sum over all elements up to $n$ of our input sequence $(a_n)$:
\begin{equation}
 s_n = \sum_{k=0}^n a_k
\end{equation}
It's not often framed like this but if you want, you may see the production of the sequence of partial sums as a sequence transformation in the sense above - albeit not a regular one. Just like with any sequence, we can ask whether this sequence of partial sums converges or diverges when we let $n$ approach infinity. What we have then created is an \emph{infinite sum}. Such infinite sums have a special name: they are called \emph{series}. A necessary condition for a series $(s_n)$ to converge is that the sequence $(a_n)$ of its terms converges to zero. But that is not a sufficient condition. The terms must also converge to zero "fast enough". For example, if the terms follow a power rule like $a_n = 1 / n^c$ for some constant $c > 0$, then the $a_n$ will converge to zero. But they will only converge to zero "fast enough" if $c > 1$. If the sequence of partial sums $s_n$ converges, then we can define its value as:
\begin{equation}
 S = \lim_{n \rightarrow \infty} s_n = \lim_{n \rightarrow \infty} \sum_{k=0}^n a_k
\end{equation}
This limit, if it exists and is finite, is called the \emph{sum} of the series. 

% https://en.wikipedia.org/wiki/Series_(mathematics)
% For a_n = 1/n^c, it will be fast enough if |c| > 1.

% ToDo: frame the production of a series from a sequence in terms of a transformation - it is one - albeit not a regular one, in general.

\subsubsection{Some Sums}
ToDo: give formulas for finite and infinite geometric series, infinite alternating harmonic series, inverse-squares ("Basel-problem"), explain divergence of harmonic series

% ToDo: explain some special important series such as geometric, harmonic, alternating harmnonic, inverse-squares, etc. - give formulas for the finite and infinite sums
% https://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/SandS/SeriesTests/series_list.html
% geometric series, p-series, telecsoping series


Series that converge but do not converge absolutely, have a rather peculiar property which is encapsulated in...

% explain term "conditionally convergent"

% Give rules for convergent series formed by our operations above - sum, product, Cauchy product. I think, in the case of the Cauchy product, we knoe that the it converges, if the two factors converge. But what else can be said? Porbably not much about the number it converges to. But maybe if we convolve two divergent series, we may end up with a convergent one? Maybe we can make them "interfere" destructively or something? What about the harmonic series 1/n and the alternating series (-1)^n. They are both individually divergent but their pointwise product is convergent, I think - it's the alternating harmonic series. What about the Cauchy product?




\subsubsection{Riemann's Reordering Theorem} There is a pretty interesting theorem that applies to conditionally convergent series, i.e. series that do converge but fail to converge absolutely. Such series can be made to converge to any value or even to diverge just by reordering the terms. This theorem is called \emph{Riemann reordering theorem}, \emph{Riemann rearrangement theorem} or \emph{Riemann series theorem}. That result is kinda weird because in finite sums, we can reorder the terms in any way we like without changing the final sum. That is a consequence of the associativity of addition. Infinite sums may break the associativity of addition in certain cases. [VERIFY!] ...TBC....Give example - maybe use the alternating harmonic series and show how to make it approach any value

% Can you change a sum by rearranging its numbers? --- The Riemann Series Theorem
% https://www.youtube.com/watch?v=U0w0f0PDdPA  by Morphocular

% ...TBC...series that converge but fail to converge absolutely can be made to converge to any value just by reordering the terms

% https://en.wikipedia.org/wiki/Riemann_series_theorem
% Riemann rearrangement theorem, Riemann series theorem,

\subsubsection{Summation Methods} Certain divergent series can be transformed into other series that are convergent. Of course, if we would allow any transformation, that would be trivially true and an uninteresting statement - we could just transform our divergent series into a series of all zeros, for example. We want to allow only those kinds of transformations that, when being applied to a convergent series, do not change their value. So, the name of the game is to transform the series in a certain way and then trying to figure out the value of the transformed series. This is what summation methods are about. A summation method that has our desired the property of not changing the value of any convergent series is called regular (VERIFY!). If we have found such a regular transformation and then apply it to a divergent series, it may happen that the transformed series becomes convergent. If that works out, we will have a way to assign numerical values to divergent series. If it furthermore turns out that different regular transformations will always lead to the same values, then we may be on to something - maybe some mathematically well defined way to actually evaluate divergent series ...TBC..

% To be honest, I have seen the term "regular" only be used in the context of series but I boldly generalized its application to transformations of sequences, too

% Series to Series Transformations and Analytic Continuation by Matrix Methods
% https://www.jstor.org/stable/2372347
% T-matrix:      sequence to sequence
% gamma-matrix:  series to sequence
% alpha-matrix:  series to series

%Of course, to make any sense, our transformation process

%by processes that have the property that, when being applied to a convergent series, the resulting series will converge to the same value.

% 

% https://encyclopediaofmath.org/wiki/Regular_summation_methods
% https://en.wikipedia.org/wiki/Divergent_series#Properties_of_summation_methods

...ToDo: Cesaro summation, Hölder summation, Abel Summation ...
% -Can manke certain divergent series convergent
% -Can improve convergence properties of already convergent series - for example, get rid
%  of Gibbs ripple. Maybe it can also increase rate of convergence
% https://www.youtube.com/watch?v=LDuD_ClkkG8

% https://en.wikipedia.org/wiki/Ces%C3%A0ro_summation
% https://en.wikipedia.org/wiki/Divergent_series#Abel_summation
% https://en.wikipedia.org/wiki/Divergent_series#Lindel%C3%B6f_summation
% https://en.wikipedia.org/wiki/Euler_summation
% https://en.wikipedia.org/wiki/Borel_summation
% https://en.wikipedia.org/wiki/Mittag-Leffler_summation
% https://en.wikipedia.org/wiki/Lambert_summation
% https://en.wikipedia.org/wiki/Euler%E2%80%93Boole_summation
% https://en.wikipedia.org/wiki/Van_Wijngaarden_transformation

% https://en.wikipedia.org/wiki/Antilimit

% One minus one plus one minus one - Numberphile
% https://www.youtube.com/watch?v=PCu_BNNI5x4

\subsection{Power Series}
A power series is a series that involves a variable $x$. We imagine that we have a given sequence $(a_n)$ and we will intepret this as a sequence of coefficients of a series of powers of $x$, i.e. a sort of infinite polynomial. We will consider the series:
\begin{equation}
\label{Eq:PowerSeries}
 f(x) = \sum_{n=0}^\infty a_n (x-c)^n
\end{equation}
where $c$ is some fixed constant. It's often zero but for generality, I've already included it. On the left hand side, I already suggestively have written $f(x)$ to indicate that this "infinite polynomial" may be used to define a function - at least for those values of $x$, for which the infinite sum converges to a finite value. Note that (finite) polynomials are included as the special case in which all the coefficients $a_n$ are equal to zero for $n > k$ for some $k$ ($k$ is actually the degree of the polynomial, in this case). If the series converges for any $x$ at all, it will do so inside a disc centered at $c$ (this is not supposed to be obvious) when we assume that $c$ and $x$ can be complex numbers. The radius of this disc is called the radius of convergence. On the boundary of this disc, the series may or may not converge and the convergence property may depend on the actual point on the boundary - so this needs to be investigated more closely, if this information is needed. The series will actually always converge for $x = c$ (we'll see later why) but what we are usually really interested in is whether or not it also converges inside come neighborhood of the expansion point $c$, i.e. whether or not the radius of convergence is nonzero. For some particularly well behaved functions, the so called \emph{entire functions}, it may even be infinite. In general, we could also allow complex coefficients $a_n$. For the time being, we'll focus our attention to real $x,c,a_n$ all being real numbers, though. A formula for the radius of convergence is given by the Cauchy-Hadamard theorem. ..TBC...


% https://en.wikipedia.org/wiki/Radius_of_convergence

% https://en.wikipedia.org/wiki/Cauchy%E2%80%93Hadamard_theorem
% -gives formula for the radius of convergence

% Maybe include this into the section about limits:
% https://en.wikipedia.org/wiki/Limit_inferior_and_limit_superior#limit_superior
% but it applies to sequences, so maybe include it here.

% https://en.wikipedia.org/wiki/Entire_function

\paragraph{Convolution Revisited}
When we defined the convolution operation between sequences, it may have seemed somewhat arbitrary and unmotivated. In light of power series, we may give it some interpretation that also justifies why it is also called the Cauchy "product". Recall that to multiply two (finite) polynomials, we have to convolve their lists of coefficients. The Cauchy product is basically the infinite version of that. It's a generalization in the sense that it includes the finite case as special case when we assume that the sequences just become identically zero after some $n$. The product sequence is the sequence of coefficients of an infinite product polynomial, so to speak...TBC...that can perhaps be explained better

\medskip
...TBC..ToDo: explain how to compute radius of convergence - distance to nearest singularity

%explain 

\subsubsection{Taylor Series Expansion}
One truly remarkable result of calculus is that a lot of functions can be expressed as such a power series. For many important functions (including our favorites $\exp, \sin, \cos$), we will even find an infinite radius of convergence. Taylor's theorem tells us how to find the coefficients and the formula is actually pretty simple. First, we pick an expansion point, i.e. the center of our expansion. That's the $c$ in the formula given above but here, we'll call it $x_0$. The $k$th coefficient is given the $k$th derivative of $f$ at our expansion point $x_0$ divided by $k!$. So the formula for the Taylor series is:
\begin{equation}
\label{Eq:TaylorSeries}
f(x) = \sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k
%     = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k + \mathcal{O}(x-x_0)^n
\end{equation}
For this formula to work, we require a few things. First, $f$ must be smooth at $x_0$ such that we actually can take all the derivatives. Second, we require $x$ to be within the radius of convergence of the series, i.e. close enough to $x_0$. There are some rather special functions that are indeed smooth but have a radius of convergence of zero. But we'll forget about that complication right now. For most functions, it works just fine. By inspecting this formula, we can see the reason why (\ref{Eq:PowerSeries}) always converges for $x=c$ or $x=x_0$ in the new notation here: the factor $(x-x_0)^k$ will be zero for any $k > 0$. For $x = x_0$ and $k = 0$, the factor $(x-x_0)^k$ actually evaluates to $0^0$. For the formula to still work at $x=x_0$, we are forced to define $0^0 = 1$ which is a quite common definition in algebra, calculus and combinatorics.

% toDo: explain how the formula arises from approximating a function with polynomials at a point by mathcing more and more derivatives. Explain hwo the power series for e^x arises from that 

% which means that for $x=x_0$, 

%only the first term in the whole series is actually nonzero such that in this case, the series converges trivially.
% maybe give a formula that splits it into a finite sum and a remainder

% (at least, if we define $0^0 = 1$, which is common but not universal, so beware! VERIFY!)

% https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero

\medskip
The practical usefulness of this Taylor expansion lies in the fact that when we just take a finite sum instead of an infinite one, we obtain an approximation to our function. The more terms we use, the better our approximation becomes (provided that the argument is within the radius of convergence - of course). Due to that feature, Taylor expansions are used a lot in numerical computations to approximate all kinds of functions. They are also used a lot in physics to simplify complicated problems by making approximations. There, often just first or second order approximations are used, i.e. approximations up to the linear or quadratic term. Let's see what happens, when we only use a single term, i.e. only the $k=0$ term. The formula tells us to evaluate the $0$th derivative of $f$, which is $f$ itself, at $x_0$, divide by $0!$, i.e. by one and multiply that by $(x-x_0)^0 = 1$. So what we get is just $f(x_0)$. The $0$th order Taylor polynomial around $x_0$ approximates $f$ as the constant function with value $f(x_0)$. Not a very impressive approximation but at least it has the correct function value at $x_0$ and that's the best we could possibly hope for, when we try to approximate a function $f$ by a constant function around some $x_0$. If we take one term more, we obtain a linear approximation of $f$ around $x_0$. It will have a matching function value and a matching first derivative. The approximant will be a tangent to our actual function at $(x_0, f(x_0))$. The next approximation will additionally match the second derivative and create a parabolic approximation. And so on.

ToDo: maybe move that into an "Applications" section, explain how the factorial factors arise from matching the derivatives of a Taylor-polynomial

\paragraph{Example} Let's consider the function $f(x) = 1 / (1 + x^2)$ and let's pick $x_0 = 0$ as expansion point. ToDo: draw plot of the function and a couple of approximants - show how convergence breaks down at +-1 because of the poles at +-i

\medskip
...TBC... explain aproximation properties and error term

% https://en.wikipedia.org/wiki/Taylor%27s_theorem

% https://www.youtube.com/watch?v=0HaBNdmUWXY
% How We Compute sin, cos, and e^x Shouldn't Work At All | Smooth vs. Analytic Functions
% -explains hwo the remainder/error in Taylor's theorem determines the convergence of the series, 
%  i.e. the circumstances under which the error goes to zero as the order of the series 
%  approximation grows


% This Function Breaks Taylor Series!
% https://www.youtube.com/watch?v=01h743ZBcjo
% Give examples of functions that have all derivatives defined but are not analytic
% consider e^(-1/x) on the positive real number line

\subsubsection{Formal Power Series}
A formal power series is an expression of the form (\ref{Eq:PowerSeries}) without any considerations for the convergence of the sum. The series are just treated as formal expressions that can be formally manipulated using algebraic operations (like sum, product, etc.) without any intention to ever evaluate the expression. It's the (infinite) sequence of coefficients itself that is of interest, not the number that would result when evaluating the sum. ...TBC...explain use cases

%https://en.wikipedia.org/wiki/Formal_power_series
% "the method of generating functions uses formal power series to represent numerical sequences and multisets"

\subsubsection{Laurent Series Expansion}
The Taylor series is a power series expansion of a function $f(x)$ around a point $x_0$. The coefficients can be computed by the Taylor series formula. For this to work, a necessary (but not sufficient) condition is that the function must be smooth at the point. Otherwise, we wouldn't even be able to evaluate all the derivatives that occur in the formula for the coefficients. A Laurent expansion is a generalization that can sometimes be used to expand a function around a pole, i.e. around a point where not even the function value is well defined (let alone derivatives). The formula is [VERIFY!]:
\begin{equation}
\label{Eq:LaurentSeries}
f(x) = \sum_{n=-\infty}^\infty a_n  (x-x_0)^n
\end{equation}
It looks similar to the power series expansion in (\ref{Eq:PowerSeries}) but it allows also for negative exponents - the index $n$ now starts at $-\infty$. That negative exponents are able to model the behavior of poles is apparent when we consider the function $(x-x_0)^{-1}$. ...TBC...ToDo: give formula for the coeffs $a_n$, Q: Does the expansion point have to be a pole or can it also be a jump or corner discontinuity? Explain convergence properties - converges in an annulus

% Can we expand the abs or sign function into a Laurent series? ...I don't think so...hmm...if we differentiate abs, we get sign, if we diff again, we get a Dirac spike at zero...yeah...no..I don't think, we can model that with a Laurent series. If we can, we could integrate the result twice

% Maybe explain difference between poles of even and odd order - odd order poles go to plus and minus inf at x_0 whereas even order poles go only to plus or to minsu inf. But that holds for the real number line - what about the complex plane?


% https://en.wikipedia.org/wiki/Laurent_series
% -"one often pieces together the Laurent series by combining known Taylor expansions." ...HOW?
% https://mathworld.wolfram.com/LaurentSeries.html


\subsubsection{Pusieux Series}
A Pusieux series is another generalization of the idea of power series that allows for rational exponents. There are some restrictions, though: the denominators of the exponents must be bounded and the negative exponents cannot go all the way down to minus infinity - so it's not a proper generalization of the Laurent series. ...TBC...

%https://en.wikipedia.org/wiki/Puiseux_series
% it's not a proper generalization of a Laurent series because it introduces the restriction that the negative exponents cannot go down to minus infinity

% by reducing exponents to a common denominator n, a Puiseux series becomes a Laurent series in an nth root of the indeterminate.

\subsubsection{Hahn Series}
A Hahn series generalizes the idea of power series even further by allowing the exponents to be arbitrary values from... ...TBC...

% ...from an ordered subset of the value group ...but what does that mean?

%https://en.wikipedia.org/wiki/Puiseux_series 
%https://en.wikipedia.org/wiki/Hahn_series

\subsection{Trigonometric Series}
We now want to look at a completely different kind of series. It will again involve a variable $x$ but this time, we will not use powers of $x$ but instead trigonometric functions, specifically sines and cosines. We will look at series of the form:
\begin{equation}
\label{Eq:FourierSeries}
 f(x) = \frac{a_0}{2} + \sum_{k=1}^\infty \left(  a_k \cos(k x) + b_k \sin(k x) \right)
\end{equation}
As with power series, the $f(x)$ on the left hand side indicates that we intend to use such a series to define a function of $x$. By construction, our so defined function will be periodic with a period of $2\pi$. This can easily be seen as follows. The $a_0/2$ term will just give us a constant offset and that's periodic with any period. The $k=1$ term gives a sine and cosine of $x$ which are the prototypical functions with period $2\pi$. For higher $k$, the argument of the sin/cos pair will be multiplied by $k$ so the (co)sines will oscillate $k$ times faster and will have a period of $2\pi/k$. But of course, something that is periodic with period $2\pi/k$ is also periodic with period $2\pi$. So, all the higher terms will also be $2\pi$-periodic. They will just undergo $k$ cycles when our slowest sinusoid (for $k=1$) undergoes one cycle.

\medskip
There are different conventions for dealing with the $a_0$ coefficient which represents the constant part of the function, in electrical engineering also known as the DC (directed current) term. ...TBC..

% https://en.wikipedia.org/wiki/Fourier_series
% Leupold, Vol2, pg 50 ff
% maybe use \tau instead of 2 \pi 
% maybe assume a period of 2 pi - having to carry aorund the p and the omega makes things unnecessarily complicated. Yes, it's more general but the application to functions with arbitrary period can better be done on a higher level by stretching the functions appropriately. we don't want to deal with that inside the nitty gritty of Fourier analysis and it's more consistent with most books
% maybe use the convention that pulls the a0/2 out of the sum and starts the sum at 1. this relieves us from special casing the analysis equation for a_0 and also simplifies the explanation of the periodicity

%...TBC...

%explain different conventions - some use $2 \pi x$ as argument, some pull out $a_0/2$, etc.

\subsubsection{Fourier Expansion}
The Taylor expansion expressed arbitrary functions as (potentially) infinite weighted sums of powers of the variable $x$. The coefficients were computed using derivatives of the function at one single point. Fourier expansions, on the other hand, express arbitrary periodic functions as (potentially) infinite sums of sines and cosines of $x$. The coefficients are calculated by a definite integral over one period. The formulas are:
\begin{equation}
\label{Eq:FourierCoeffs}	
 a_k = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos(k x) \, dx, \quad
 b_k = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin(k x) \, dx 
\end{equation}
We can intepret (\ref{Eq:FourierSeries}) as a synthesis formula and (\ref{Eq:FourierCoeffs}) as an analysis formula in the sense that the former is used to synthesize (i.e. generate, create) a periodic function $f(x)$ whereas the latter takes a periodic function $f(x)$ as input, analyzes it, and spits out the synthesis coefficients. In signal processing terms,  the analysis formula actually computes a cross-correlation between the input function $f(x)$ and the respective sine or cosine function. The integration can also be performed over some other interval of length $2\pi$ like $[a,a+2\pi]$ for some $a$. [VERIFY! Wouldn't that change the phase of the coeffs? If not, say so and explain why not. I think, it's because our time axis anchor point $t=0$ is not really affected by that shift] 

\medskip
The requirements on $f$ are quite different from those for Taylor series. First of all and most obviously, we require $f$ to be periodic. Here, we restricted our attention to $2 \pi$-periodic functions, but the theory can easily be applied to functions with other periods by stretching the function appropriately along the $x$-axis.  For a Taylor series, we required $f$ to be smooth because we had to take (infinitely many) derivatives to compute the coeffs. For a Fourier series, $f$ does not even need to be continuous. We only require that the integrals in the analysis equation must exist and be finite. This is a much less strict requirement [WAIT: I think, it's actually an independent requirement - but maybe "less strict" in some "practical" sense]. If the function has jump discontinuities, it will converge to average of the values on the left and right at the jump. If we use only finitely many terms, there will be some overshoot and wiggle around the jump. This is called Gibbs's phenomenon. Taking more terms will make these wiggles narrower in width but not smaller in height. Their height will settle to something like $9\%$ of the jump height when taking ever more but still finitely many terms. The exact amount of overshoot defines the so called Wilbraham–Gibbs constant. The overshoots will disappear completely only due to their vanishing width in the limit of infinitely many terms.

% https://en.wikipedia.org/wiki/Gibbs_phenomenon
% https://mathworld.wolfram.com/Wilbraham-GibbsConstant.html


\subsubsection{Magnitude and Phase}
It is well known that a weighted sum of a sine and a cosine of the same frequency can also be expressed as just a sine or just a cosine. For this, we need to give the argument an appropriate phase shift $\varphi_k$ and scale the result by an appropriate amplitude factor $A_k$. The phase shift and the scale factor depend on the relative and total amounts of the sine and cosine components in the following way.:
\begin{equation}
 A_0 = \frac{a_0}{2},           \;\;
 A_k = \sqrt{a_k^2 + b_k^2},    \;\;
 \varphi_k = \atan2(b_k, a_k),  \qquad
 a_k = A_k \sin(\varphi_k),     \;\;
 b_k = A_k \cos(\varphi_k)   
\end{equation}
where the $A_k = \ldots$ formula applies only to $k \neq 0$. I have also given the conversion formula for the other direction. With these new coeffs, we can express the series as:
\begin{equation}
f(x) = A_0 + \sum_{k=1}^\infty A_k \sin(k x +\varphi_k) 
     = A_0 + \sum_{k=1}^\infty A_k \cos(k x +\varphi_k - \frac{\pi}{2})
\end{equation}
[VERIFY!] 
% Seems like we can't get rid of $A_0$ when we start the sum at $k=0$. phi_0 = 0 because atan2(0,x) is 0° for any x and sin(0) = 0 and cos(-pi/2) = 0. So the DC would be missing in the resythesis. But wouldn't it be nicer if the formula would cover the edge case also? can that be made to work? We would just need to ensure that phi_0 = pi/2, such that sin(phi_0) = cos(0) = 1, I think. 
% https://en.wikipedia.org/wiki/Sine_and_cosine#Identities
\newline



\medskip
...TBC... interpretation as amplitude and phase, symmetries (odd - no cosines, even - no sines)
%Leup2, pg 65

\subsubsection{Complex Formulation}
The theory of Fourier series if often being presented in its complex form. Doing so hides some of its complexity in the complex numbers. Depending on your fluency with complex numbers, this may feel like a simplification or like a mystification. The equations are shorter, yes. But they are complex - pun intended. Instead of using real valued sine and cosine functions for analysis and synthesis, one uses a single complex exponential function. This is equivalent due to Euler's formula. By doing so, one reduces the apparent complexity of having to deal with two sets of coefficients, namely the $a_n$ and $b_n$, to a single set of $c_n$ coefficients - but these coeffs are then complex valued. Going to the complex domain actually also generalizes the applicability to functions $f: \mathbb{R} \rightarrow \mathbb{C}$ as well, so we get a little bit more than just a simpli/mystification. In the complex formulation, the synthesis and analysis equations look like:
\begin{equation}
\label{Eq:FourierSeriesComplex}
 f(x) = \sum_{k=-\infty}^\infty c_k  e^{\i k x}, \qquad
 c_k = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) e^{-\i k x}  \, dx
\end{equation}
We have now written down the synthesis and analysis equations together into one line and it's still shorter than the real valued formulation of the analysis equation alone. Notice also how there's no special case for $c_0$ like there was for $a_0$. It may seem a little bit strange that the summation in the synthesis equation now starts at minus infinity. Shouldn't the real and imaginary parts of the non-negatively indexed coeffs $c_0, c_1, \ldots$ already contain the same amount of information as the pairs $(a_0,b_0),(a_1,b_1), \ldots$ from the real formulation? Why do we need the additional coeffs with negative indices? Indeed, when the input function  $f$ is purely real valued, the negatively indexed coeffs will be redundant, but remember that we now allow, in general, complex valued functions $f(x)$ functions, too (the input $x$ is still real, though). For the special case of real valued functions $f$, the $c_k$ will be conjugate symmetric, i.e. $c_{-k} = \overline{c_k}$. [VERIFY]. A $c_k$ with a negative index $k$ formally represents the coefficient for a negative frequency. That just means the complex phasor rotates clockwise rather than counterclockwise in the corresponding $c_k  e^{\i k x}$ term in the synthesis equation. This flips the sign of the sine part, the cosine part stays the same [VERIFY]

...TBC...

\subsubsection{Theorems}
...TBC...
% f even -> only cosines
% f odd  -> only sines
% f real -> c_k conjugate symmetric, A_k even, phi_k odd, a_k even, b_k odd
% energy -> Pasreval's theorem
% multiplciation -> convolution
% only positive freqs -> f has real and imag part 90° out of phase at each frequency

% https://en.wikipedia.org/wiki/Fourier_series#Table_of_basic_properties
% taking derivatives and intergrals multiplies fourier coeffs k or 1/k

\subsubsection{Aperiodic Functions}
So far, we assumed $f$ to be periodic. If you apply the analysis equation as is to an aperiodic input function or signal $f$ and then try to resynthesize your signal via the synthesis equation, the resynthesized signal will periodically repeat the chunk of the function that you have analyzed, i.e. here the portion of the function between $0$ and $2 \pi$. You will have pretended to the analysis equation that this chunk was a period from a periodic signal when in fact it wasn't, so to speak. The theory can actually be generalized to deal properly with (some) aperiodic functions, too. This leads to the idea of the Fourier transform which will be treated in more detail in the section about integral transforms. At this point, I'll just say that the sum in the synthesis equation will be replaced by an integral, our discrete Fourier coefficients $a_k, b_k$ will be replaced by a continuous function of frequency and in the analysis equations, we will have to perform an integration from minus to plus infinity instead of over one period. Within this wider framework, the periodic functions will be found as a special case, if we also allow the spectra (i.e. the continuous functions of frequency) to be composed from (shifted and scaled) Dirac delta "functions" [VERIFY!]. Defining those rigorously requires some more advanced mathematical machinery that we will treat later but informally, they can be thought of as infinitely narrow spikes with infinite height and a finite nonzero area. If that doesn't seem to make any sense, then yeah - that's why this is only an informal view.



\subsubsection{Fourier Polynomials}
If we truncate a Fourier series at a given index, say $n$, then we obtain what is called the \emph{Fourier polynomial} of degree $n$. ...ToDo: explain Gibbs ripples and how they can be avoided by Cesaro summation, spectral windowing, etc.


\subsubsection{Connection to Power Series}
Taylor and Fourier series seem to be totally disconnected things. However, when we widen our perspective to the complex domain, a beautiful connection between these two emerges...

\medskip
ToDo: bring stuff from Weitz video "Taylor trifft Fourier"
% https://www.youtube.com/watch?v=cXxsSdbfEEg
%\hyperlink{https://www.youtube.com/watch?v=cXxsSdbfEEg}{youtube.com/watch?v=cXxsSdbfEEg}

% Another beautiful example for how math is full of unexpected connections. It's like a huge interconnected network of bits of knowledge that we can look at and stand in awe.

\begin{comment}
ToDo:
-Mention other types of series
 -Dirichlet series: sum_n a_n / n^s, relation to Euler products. See "Elliptic Tales" 
  pg 168, 174. Is there an Euler product for each Dirichlet series, i.e. a way to convert
  back and forth between infinite sums and infinite products? Yes, seems so:
  https://en.wikipedia.org/wiki/Euler_product
  https://de.wikipedia.org/wiki/Euler-Produkt
  https://en.wikipedia.org/wiki/Dirichlet_series
  https://en.wikipedia.org/wiki/General_Dirichlet_series
  https://de.wikipedia.org/wiki/Dirichletreihe
  https://en.wikipedia.org/wiki/Dirichlet_L-function
  https://en.wikipedia.org/wiki/Dirichlet_convolution
  https://fractional-calculus.com/dirichlet_series_taylor_series.pdf
  ...but wait: Does an Euler product actually have coefficients? It has this P(p,s) function.
 -Bürmann series: 
  Bürmann vs Fourier: Einführung zum Thema Bürmann-Reihen:
  https://www.youtube.com/watch?v=WjCRdPBgSZg
  Besser als TAYLOR-REIHEN: BÜRMANN-REIHEN EXPLODIEREN NICHT!:
  https://www.youtube.com/watch?v=VcXl1oqQJOA
 -Sine and cosine series. This video at around 22:00
  https://www.youtube.com/watch?v=lt_QHey_yIQ
  has an interesting segment about these. They use either sines or cosines but not both but they allow for multiples of half-cycles (not only full cycles as in Fourier series). Any function can be decomposed in both of these ways, but, depending on the nature of the function (in particular, its values at the boundaries), one or the other may converge more quickly.
 -Laurent series. They are actually Taylor series for points with finite y-value but they can also
  model the infinitey y-value at the pole ...what about using a Taylor series for 1/f(x) at a 
  pole? Maybe try it with log(x) - find a Taylor series for 1 / log(x) - it is super steep 
  at x = 0: https://www.desmos.com/calculator/qblvhbkkgu  
  interesting: 1/log_2(x) https://www.desmos.com/calculator/1ceyqmelry
  goes through (-0.05,-1) - what's so special about 0.05? ...It's the base of the log divided by
  10...why 10? That seems arbitrary. I mean, it's the base of the number system but what has the number system to do with anything here?


Maybe make a subsection about asymptotic series:
https://en.wikipedia.org/wiki/Asymptotic_analysis
https://en.wikipedia.org/wiki/Asymptotic_expansion
https://en.wikipedia.org/wiki/Borel_summation
https://en.wikipedia.org/wiki/Singular_perturbation
but maybe is should go into a section "Asymptotic Analysis" in its own right after the 
Series section. Or maybe in some section Mathematical Modeling in an Applications part.
Maybe in a section about "Approximation Theory" together with subsctiosn about "Curve Fitting",
"Polynomial Approximation" (Chebychev, Lest Squares, MiniMax), etc.
-How about a Taylor-Series around "infinity"? Maybe derive expressions for a Taylor series
 around a general x_0 and then let x_0 approach infinity for each term separately?
-Stirling formula for factorial

https://www.youtube.com/watch?v=G-29AZf-NkU
-12:16 The limit of a sequence of continuous functions is not necessaricly continuous. Example:
 f_n(x) = x^n on [0,1]  ->  converges to 0 for x < 1 and to 1 for x = 1 and diverges for x > 1
 we need a stronger condition "gleichmäßige Konvergenz" ..dunno what that is in english

Make a section about divergent series and how we may deal with them:
-Generalized summation methods (e.g. Cesaro summation)
-Riemann reordering theorem
-Analytic Continuation
See:
https://www.youtube.com/watch?v=FmLIGN8ZGdw  The Return of -1/12 - Numberphile
https://www.youtube.com/watch?v=YuIIjLr6vUA  Numberphile v. Math: the truth about 1+2+3+...=-1/12
https://www.youtube.com/watch?v=jcKRGpMiVTw  Ramanujan: Making sense of 1+2+3+... = -1/12 and Co.
https://www.youtube.com/watch?v=U0w0f0PDdPA  Can you change a sum by rearranging its numbers? --- The Riemann Series Theorem

Trevor Bazett on how to use Cesaro summation to counteract the gibbs Phenomenon in Foruier 
series:
https://www.youtube.com/watch?v=AkPZcS8eqmA  Could 1-1+1-1+1-1+1-1+... actually converge?

https://en.wikipedia.org/wiki/Divergent_series

Has also Cesaro summationa and Abel Summation
https://www.youtube.com/watch?v=LDuD_ClkkG8
 

 
\end{comment}
