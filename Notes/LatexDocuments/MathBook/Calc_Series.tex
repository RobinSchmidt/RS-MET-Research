\section{Infinite Sums (aka Series)}

\subsection{Infinite Sums}
When we have a given sequence $(a_k)$, we can use it to define a new sequence, namely the sequence of its partial sums. Let's call that sequence $(s_n)$. Its $n$th element is given by the sum over all elements\footnote{It's not often framed like this but if you want, you may see the production of the sequence of partial sums as a sequence transformation in the sense described in section about sequences (albeit not a regular one).} up to $n$ of our input sequence $(a_n)$:
\begin{equation}
 s_n = \sum_{k=0}^n a_k
\end{equation}
Just like with any sequence, we can ask whether this sequence of partial sums converges or diverges when we let $n$ approach infinity. What we have then created is an \emph{infinite sum}. Such infinite sums have a special name: they are called \emph{series}. A necessary condition for a series $(s_n)$ to converge is that the sequence $(a_n)$ of its terms converges to zero. But that is not a sufficient condition. The terms must also converge to zero "fast enough". For example, if the terms follow a power rule like $a_n = 1 / n^c$ for some constant $c > 0$, then the $a_n$ will converge to zero. But they will only converge to zero "fast enough" if $c > 1$. If the sequence of partial sums $s_n$ converges, then we can define its value as:
\begin{equation}
\label{Eq:InfiniteSum}
S = \lim_{n \rightarrow \infty} s_n 
  = \lim_{n \rightarrow \infty} \sum_{k=0}^n a_k
  = \sum_{k=0}^{\infty} a_k
\end{equation}
This limit, if it exists and is finite, is called the \emph{sum} of the series. The rightmost expression in the equation is just a definition of notation. That is: we define a sum that has $\infty$ as its upper summation limit via replacing $\infty$ by $n$ and then take the limit as $n$ approaches infinity. That naturally extends our sigma notation for summation to the case of infinite upper limits. By the way, an analogous step can be taken to extend the notation further to allow $-\infty$ as lower summation limit. [Q: Is there a way to extend the notation to allow real numbers for the summation limits? And no, I don't mean integrals.]

% Is there a way to extend the notation to allow real numbers for the limits? I mean, we have integrals - but can we do it with sums, too?

% Here is a way:
% How to Extend the Sum of Any* Function
% https://www.youtube.com/watch?v=hkn9zeRuzHs

% https://en.wikipedia.org/wiki/Series_(mathematics)
% For a_n = 1/n^c, it will be fast enough if |c| > 1.

% ToDo: frame the production of a series from a sequence in terms of a transformation - it is one - albeit not a regular one, in general.

\subsubsection{Some Important Sums}
ToDo: give formulas for finite and infinite geometric series, infinite alternating harmonic series, inverse-squares ("Basel-problem"), explain divergence of harmonic series

% r^0 + r^1 + r^2 + ... + r^n = (1 - r^(n+1)) / (1 - r)
% ...this goes to 1 / (1-r)  as n -> inf

% ToDo: explain some special important series such as geometric, harmonic, alternating harmnonic, inverse-squares, etc. - give formulas for the finite and infinite sums
% https://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/SandS/SeriesTests/series_list.html
% geometric series, p-series, telecsoping series, binomial series (generalization of geometric 
% series and of binomial theorem),

% JOS Filters book has a formula for the matrix geometric series (page 347, footnote). For a
% square matrix R:  S = \sum_{n=0}^{\infty} R^n = (I - R)^{-1}  where I is the identity matrix
% that has the same shape as R


Series that converge but do not converge absolutely, have a rather peculiar property which is encapsulated in...

% explain term "conditionally convergent", and "uniformly convergent" (means that the series converges to a function f in the sense that we can put a "tube" of fixed width (i.e. fixed y-width) around f and for some value of n, the series trucated at some m >= n will be completely inside the tube

% Give rules for convergent series formed by our operations above - sum, product, Cauchy product. I think, in the case of the Cauchy product, we knoe that the it converges, if the two factors converge. But what else can be said? Porbably not much about the number it converges to. But maybe if we convolve two divergent series, we may end up with a convergent one? Maybe we can make them "interfere" destructively or something? What about the harmonic series 1/n and the alternating series (-1)^n. They are both individually divergent but their pointwise product is convergent, I think - it's the alternating harmonic series. What about the Cauchy product?


% An Infinite Sum from the Berkeley Math Tournament
% https://www.youtube.com/watch?v=fuUBMFYDZNg
% -Derives sum_{n=0}^{\infty}  n^3 / n! = 5 e
% -There's a comment (by @HL-iw1du) that says something like  
%  sum_{n=0}^{\infty}  n^k / n! = B(k) e   where B(k) is the k-th Bell number, see
%  https://oeis.org/A000110
%  https://www.youtube.com/watch?v=fuUBMFYDZNg&lc=Ugz1gQWImgQnUtVJuV14AaABAg
% -There's another comment (by @DanGRV) that explains how to further generalize the idea to
%  sum_{n=0}^{\infty}  p(n) / n!  where p(n) is a polynomial
%  https://www.youtube.com/watch?v=fuUBMFYDZNg&lc=UgyS38H5Wb2P1gJM6nF4AaABAg


\subsubsection{Riemann's Reordering Theorem} There is a pretty interesting theorem that applies to conditionally convergent series, i.e. series that do converge but fail to converge absolutely. Such series can be made to converge to any value or even to diverge just by reordering the terms. This theorem is called \emph{Riemann reordering theorem}, \emph{Riemann rearrangement theorem} or \emph{Riemann series theorem}. That result is kinda weird because in finite sums, we can reorder the terms in any way we like without changing the final sum. That is a consequence of the associativity of addition. Infinite sums may break the associativity of addition in certain cases. [VERIFY!] ...TBC....Give example - maybe use the alternating harmonic series and show how to make it approach any value

% Can you change a sum by rearranging its numbers? --- The Riemann Series Theorem
% https://www.youtube.com/watch?v=U0w0f0PDdPA  by Morphocular

% ...TBC...series that converge but fail to converge absolutely can be made to converge to any value just by reordering the terms

% https://en.wikipedia.org/wiki/Riemann_series_theorem
% Riemann rearrangement theorem, Riemann series theorem,

% https://www.youtube.com/watch?v=3aCM5KoV9zY  Rearrange and Transform Zero into Infinity

\subsubsection{Summation Methods} Certain divergent series can be transformed into other series that are convergent. Of course, if we would allow any transformation, that would be trivially true and an uninteresting statement - we could just transform our divergent series into a series of all zeros, for example. We want to allow only those kinds of transformations that, when being applied to a convergent series, do not change their value. So, the name of the game is to transform the series in a certain way and then trying to figure out the value of the transformed series. This is what summation methods are about. A summation method that has our desired the property of not changing the value of any convergent series is called regular (VERIFY!). If we have found such a regular transformation and then apply it to a divergent series, it may happen that the transformed series becomes convergent. If that works out, we will have a way to assign numerical values to divergent series. If it furthermore turns out that different regular transformations will always lead to the same values, then we may be on to something - maybe some mathematically well defined way to actually evaluate divergent series ...TBC..

% To be honest, I have seen the term "regular" only be used in the context of series but I boldly generalized its application to transformations of sequences, too

% Series to Series Transformations and Analytic Continuation by Matrix Methods
% https://www.jstor.org/stable/2372347
% T-matrix:      sequence to sequence
% gamma-matrix:  series to sequence
% alpha-matrix:  series to series

%Of course, to make any sense, our transformation process

%by processes that have the property that, when being applied to a convergent series, the resulting series will converge to the same value.

% 

% https://encyclopediaofmath.org/wiki/Regular_summation_methods
% https://en.wikipedia.org/wiki/Divergent_series#Properties_of_summation_methods

...ToDo: Cesaro summation, HÃ¶lder summation, Abel Summation ...
% -Can manke certain divergent series convergent
% -Can improve convergence properties of already convergent series - for example, get rid
%  of Gibbs ripple. Maybe it can also increase rate of convergence
% https://www.youtube.com/watch?v=LDuD_ClkkG8

% https://en.wikipedia.org/wiki/Ces%C3%A0ro_summation
% https://en.wikipedia.org/wiki/Divergent_series#Abel_summation
% https://en.wikipedia.org/wiki/Divergent_series#Lindel%C3%B6f_summation
% https://en.wikipedia.org/wiki/Euler_summation
% https://en.wikipedia.org/wiki/Borel_summation
% https://en.wikipedia.org/wiki/Mittag-Leffler_summation
% https://en.wikipedia.org/wiki/Lambert_summation
% https://en.wikipedia.org/wiki/Euler%E2%80%93Boole_summation
% https://en.wikipedia.org/wiki/Van_Wijngaarden_transformation

% https://en.wikipedia.org/wiki/Antilimit

% One minus one plus one minus one - Numberphile
% https://www.youtube.com/watch?v=PCu_BNNI5x4

% on the product of all natural numbers...
% https://www.youtube.com/watch?v=VTX1QddnCFE
% -about "Regularization"


\subsection{Convergence}

\subsubsection{Acceleration of Convergence} Many mathematical problems have solutions that can be expressed in the form of convergent series. The practical relevance of such series solutions is that a truncation of such an infinite series can provide a good approximation to the desired solution. A truncated series is just a finite sum and can therefore be readily evaluated. The accuracy of the so obtained solution will depend on the number of terms after which we truncate and on the rate of convergence of the series. So, a faster rate of convergence is usually desirable if we want to use a series solution in practice. There are methods by which we can transform a given series with sum $S$ into another related series with sum $T$ such that we can express $S$ as some closed form formula of $T$, i.e. $S = f(T)$ with a series expression for $T$ that converges faster than the series expression for $S$. ...TBC... ToDo: explain Euler transformation, Kummer's method, etc.

%\paragraph{Kummer's Comparison Method}

% See Hamming - NumericalMethods for Scientiests and Engineers, pg 195

%\paragraph{Euler Transform}

% https://mathworld.wolfram.com/EulerTransform.html
% https://en.wikipedia.org/wiki/Euler_summation

\subsection{Power Series}
A power series is a series that involves a variable $x$. We imagine that we have a given sequence $(a_n)$ and we will intepret this as a sequence of coefficients of a series of powers of $x$, i.e. a sort of infinite polynomial. We will consider the series:
\begin{equation}
\label{Eq:PowerSeries}
 f(x) = \sum_{n=0}^\infty a_n (x-c)^n
\end{equation}
where $c$ is some fixed constant. It's often zero but for generality, I've already included it. On the left hand side, I already suggestively have written $f(x)$ to indicate that this "infinite polynomial" may be used to define a function - at least for those values of $x$, for which the infinite sum converges to a finite value. Note that (finite) polynomials are included as the special case in which all the coefficients $a_n$ are equal to zero for $n > k$ for some $k$ ($k$ is actually the degree of the polynomial, in this case). If the series converges for any $x$ at all, it will do so inside a disc centered at $c$ (this is not supposed to be obvious) when we assume that $c$ and $x$ can be complex numbers. The radius of this disc is called the radius of convergence. On the boundary of this disc, the series may or may not converge and the convergence property may depend on the actual point on the boundary - so this needs to be investigated more closely, if this information is needed. The series will actually always converge for $x = c$ (we'll see later why) but what we are usually really interested in is whether or not it also converges inside come neighborhood of the expansion point $c$, i.e. whether or not the radius of convergence is nonzero. For some particularly well behaved functions, the so called \emph{entire functions}, it may even be infinite. In general, we could also allow complex coefficients $a_n$. For the time being, we'll focus our attention to real $x,c,a_n$ all being real numbers, though. A formula for the radius of convergence is given by the Cauchy-Hadamard theorem. ..TBC...


% https://en.wikipedia.org/wiki/Radius_of_convergence

% https://en.wikipedia.org/wiki/Cauchy%E2%80%93Hadamard_theorem
% -gives formula for the radius of convergence

% Maybe include this into the section about limits:
% https://en.wikipedia.org/wiki/Limit_inferior_and_limit_superior#limit_superior
% but it applies to sequences, so maybe include it here.

% https://en.wikipedia.org/wiki/Entire_function




\paragraph{Convolution Revisited}
When we defined the convolution operation between sequences, it may have seemed somewhat arbitrary and unmotivated. In light of power series, we may give it some interpretation that also justifies why it is also called the Cauchy "product". Recall that to multiply two (finite) polynomials, we have to convolve their lists of coefficients. The Cauchy product is basically the infinite version of that. It's a generalization in the sense that it includes the finite case as special case when we assume that the sequences just become identically zero after some $n$. The product sequence is the sequence of coefficients of an infinite product polynomial, so to speak...TBC...that can perhaps be explained better

\medskip
...TBC..ToDo: explain how to compute radius of convergence - distance to nearest singularity

%explain 



\subsubsection{Taylor Series Expansion}
\label{Sec:TaylorSeries}
One truly remarkable result of calculus is that a lot of functions can be expressed as such a power series. For many important functions (including our favorites $\exp, \sin, \cos$), we will even find an infinite radius of convergence. Taylor's theorem tells us how to find the coefficients and the formula is actually pretty simple. First, we pick an expansion point, i.e. the center of our expansion. That's the $c$ in the formula given above but here, we'll call it $x_0$. The $k$th coefficient is given the $k$th derivative of $f$ evaluated at our expansion point $x_0$ divided by $k!$. So the formula for the \emph{Taylor series} is:
\begin{equation}
\label{Eq:TaylorSeries}
f(x) = \sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k
%     = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k + \mathcal{O}(x-x_0)^n
\end{equation}
In the context of the more general power series formula given by (\ref{Eq:PowerSeries}), we recognize the Taylor series as the case where $c = x_0$ and $a_n =  \frac{f^{(k)}(x_0)}{k!}$ can be interpreted as a coefficient calculation formula for the $a_n$ of a general power series. For this formula to work, we require a few things. First, $f$ must be smooth at $x_0$ such that we actually can take all the derivatives. Second, we require $x$ to be within the radius of convergence of the series, i.e. close enough to $x_0$. There are some rather special functions that are indeed smooth but have a radius of convergence of zero. But we'll forget about that complication right now. For most functions, it works just fine. By inspecting this formula, we can see the reason why (\ref{Eq:PowerSeries}) always converges for $x=c$ or $x=x_0$ in the new notation here: the factor $(x-x_0)^k$ will be zero for any $k > 0$. For $x = x_0$ and $k = 0$, the factor $(x-x_0)^k$ actually evaluates to $0^0$. For the formula to still work at $x=x_0$, we are forced to define $0^0 = 1$ which is a quite common definition in algebra, calculus and combinatorics. Defining the \emph{indeterminate form} of $0^0$ to be equal to one just makes the most sense in these contexts because lets the formulas used in these fields to remain valid even in such edge cases. If the expansion point $x_0$ is chosen to be $x_0 = 0$, the Taylor series is also called \emph{Maclaurin series}. That is: a Maclaurin series is a special case of a Taylor series with $x_0 = 0$. 

%In the context of the more general power series formula given by (\ref{Eq:PowerSeries}), we recognize the Taylor series as the case where $c = x_0$ and $a_n =  \frac{f^{(k)}(x_0)}{k!}$ can be interpreted as a coefficient calculation formula for the $a_n$ of a general power series.

% ToDo: explain how the formula arises from approximating a function with polynomials at a point by matching more and more derivatives. Explain how the power series for e^x arises from that. We can 
% intepret  \frac{f^{(k)}(x_0)}{k!}  as a computation formula for a coefficient a_k for a more general
% power seris

% ToDo: mention some examples of formulas from algebra and combinatorics where the definition of 0^0 = 1 also plays nicely with edge cases. Maybe with forward references to the actual discussions of such formulas in the appropriate sections about algebra and combinatorics

% which means that for $x=x_0$, 

%only the first term in the whole series is actually nonzero such that in this case, the series converges trivially.
% maybe give a formula that splits it into a finite sum and a remainder

% (at least, if we define $0^0 = 1$, which is common but not universal, so beware! VERIFY!)

% https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero

\medskip
The practical usefulness of this Taylor expansion lies in the fact that when we just take a finite sum instead of an infinite one, we obtain an approximation to our function. The more terms we use, the better our approximation becomes (provided that the argument $x$ is within the radius of convergence - of course). The approximation also tends to be more accurate, the closer our input value $x$ is to the expansion center $x_0$. Due to the approximation feature, Taylor expansions are used a lot in numerical computations to approximate all kinds of functions. They are also used a lot in physics to simplify complicated problems by making approximations. There, often just first or second order approximations are used, i.e. approximations up to the linear or quadratic term. Let's see what happens, when we only use a single term, i.e. only the $k=0$ term. The formula tells us to evaluate the $0$th derivative of $f$, which is $f$ itself, at $x_0$, divide by $0!$, i.e. by one and multiply that by $(x-x_0)^0 = 1$. So what we get is just $f(x_0)$. The $0$th order Taylor polynomial around $x_0$ approximates $f$ as the constant function with value $f(x_0)$. Not a very impressive approximation but at least it has the correct function value at $x_0$ and that's the best we could possibly hope for, when we try to approximate a function $f$ by a constant function around some $x_0$. If we take one term more, we obtain a linear approximation of $f$ around $x_0$. It will have a matching function value and a matching first derivative. The approximant will be a tangent to our actual function at $(x_0, f(x_0))$. The next approximation will additionally match the second derivative and create a parabolic approximation. And so on. The more terms we use, the more derivatives our approximant will match to the target function. An $n$th order Taylor approximation matches our function $f$ at the point $x_0$ up to the $n$th derivative.

% Taylor approximations are good to approximate functions around a point. They get increaingly bad, the further we move away from the point. In some situations, we may rather want to approximate the function withing an interval and perhaps minimize the maximum deviation of the approximant from the target function withing that interval. For that purpose, a Taylor expansion is not optimal. Instead, other polynomial approximations can be used (minimax, Chebychev, least-squares, Hermite etc.). They different in the way in which the polynomial coefficients are computed. ...maybe move this to Approximation Theory in the Applications part


\paragraph{Example} Let's consider the function $f(x) = 1 / (1 + x^2)$ and let's pick $x_0 = 0$ as expansion point. ToDo: draw plot of the function and a couple of approximants - show how convergence breaks down at +-1 because of the poles at +-i

\medskip
...TBC... explain approximation properties and error term, explain radius of convergence, maybe also show Taylor expansions of exp, sin, cos, $1 / (1 - x^2)$, explain how the factorial factors arise from matching the derivatives of a Taylor-polynomial, explain how the power series expansions of exp, sin etc. that were just handed down in previous chapters can now be justified

% If we pick some expansion point $z_0$ in the complex plane and investigate the region in which such a Taylor series around the given point $z_0$ will converge, we will always find that this so called region of convergence forms a circle centered at $z_0$. The radius of this circle is given by the distance to the nearest singularity of $f$ - nearest to $z_0$, of course.

% I think, this implies that if for some $z_0$ we find an infinite radius of convergence, this implies that the radius of convergence is infinite for all expansion centers.

% Can we say something more about the speed of convergence? Could it be related to distances to the singularities? Maybe if there are nearby singularities, it converges more slowly - or something?

% https://en.wikipedia.org/wiki/Taylor%27s_theorem

% https://www.youtube.com/watch?v=0HaBNdmUWXY
% How We Compute sin, cos, and e^x Shouldn't Work At All | Smooth vs. Analytic Functions
% -explains hwo the remainder/error in Taylor's theorem determines the convergence of the series, 
%  i.e. the circumstances under which the error goes to zero as the order of the series 
%  approximation grows


% This Function Breaks Taylor Series!
% https://www.youtube.com/watch?v=01h743ZBcjo
% Give examples of functions that have all derivatives defined but are not analytic
% consider e^(-1/x) on the positive real number line


% The Subtle Reason Taylor Series Work | Smooth vs. Analytic Functions
% https://www.youtube.com/watch?v=0HaBNdmUWXY
% -Has nice presentation of the formula for the remainder term in the Taylor series
% -explains difference between holomorphic and analytic

\paragraph{Lagrange Inversion Theorem}
Given a functional dependency $x = f(y)$ such that $y = f^{-1}(x)$, there is also a formula to produce a Taylor series expansion of the inverse of $f$. ...TBC...

%

% Taylor series of inverse functions:
% https://en.wikipedia.org/wiki/Lagrange_inversion_theorem
% https://randorithms.com/2021/08/31/Taylor-Series-Inverse.html
% 

% The Impossible Equation at the Heart of Astronomy [Keplerâs Equation]
% https://www.youtube.com/watch?v=hBkmyJ3TE0g
% -discusses the history of trying to solve Kepler's equation including numerical methods and
%  series approaches - the latter includes Lagrange's inversion theorem

% See also:
% https://en.wikipedia.org/wiki/Lagrange_reversion_theorem

\subsubsection{Formal Power Series}
A formal power series is an expression of the form (\ref{Eq:PowerSeries}) without any considerations for the convergence of the sum. The series are just treated as formal expressions that can be formally manipulated using algebraic operations (like sum, product, etc.) without any intention to ever evaluate the expression. It's the (infinite) sequence of coefficients itself that is of interest, not the number that would result when evaluating the sum. Such a number may or may not exist and in the context of formal power series, we don't care about that question. ...TBC...explain use cases

%https://en.wikipedia.org/wiki/Formal_power_series
% "the method of generating functions uses formal power series to represent numerical sequences and multisets"

\subsubsection{Laurent Series Expansion}
The Taylor series is a power series expansion of a function $f(x)$ around a point $x_0$. The coefficients can be computed by the Taylor series formula. For this to work, a necessary (but not sufficient) condition is that the function must be smooth at the point. Otherwise, we wouldn't even be able to evaluate all the derivatives that occur in the formula for the coefficients. A Laurent expansion is a generalization that can sometimes be used to expand a function around a pole, i.e. around a point where not even the function value is well defined (let alone derivatives). The formula is [VERIFY!]:
\begin{equation}
\label{Eq:LaurentSeries}
f(x) = \sum_{n=-\infty}^\infty a_n  (x-x_0)^n
\end{equation}
It looks similar to the power series expansion in (\ref{Eq:PowerSeries}) but it allows also for negative exponents - the index $n$ now starts at $-\infty$ rather than $0$. We now also get negative exponents which allow the series to model a qualitatively different behavior - namely the behavior of \emph{poles}. This is apparent when we consider the function $(x-x_0)^{-1}$. It clearly features a pole at $x = x_0$ because we would have a division by zero there. More specifically, it's a pole of first order. Generally, a term of the form $r (x-x_0)^{-n}$ with $n > 0$ would produce a pole of $n$th order at $x = x_0$ and the scaling factor $r$ in front of it is called the \emph{residue} of the pole. Such Laurent expansions are important in complex analysis. We'll revisit them there later. ...TBC...ToDo: give formula for the coeffs $a_n$, Q: Does the expansion point have to be a pole or can it also be a jump or corner discontinuity (I don't think so)? Explain convergence properties - converges in an annulus

% Verify the state ment that $r$ is called the residue. Maybe that term applies only to 1st order poles and poles of order $n$ are expressed as $n$ factors of the form $r^{1/n} (x-x_0)^{-1}$?

% Can we expand the abs or sign function into a Laurent series? ...I don't think so...hmm...if we differentiate abs, we get sign, if we diff again, we get a Dirac spike at zero...yeah...no..I don't think, we can model that with a Laurent series. If we can, we could integrate the result twice

% Maybe explain difference between poles of even and odd order - odd order poles go to plus and minus inf at x_0 whereas even order poles go only to plus or to minsu inf. But that holds for the real number line - what about the complex plane?


% https://en.wikipedia.org/wiki/Laurent_series
% -"one often pieces together the Laurent series by combining known Taylor expansions." ...HOW?
% https://mathworld.wolfram.com/LaurentSeries.html


\subsubsection{Pusieux Series}
A Pusieux series is another generalization of the idea of power series that allows for rational exponents. There are some restrictions, though: the denominators of the exponents must be bounded and the negative exponents cannot go all the way down to minus infinity - so it's not a proper generalization of the Laurent series. ...TBC...

%https://en.wikipedia.org/wiki/Puiseux_series
% it's not a proper generalization of a Laurent series because it introduces the restriction that the negative exponents cannot go down to minus infinity

% by reducing exponents to a common denominator n, a Puiseux series becomes a Laurent series in an nth root of the indeterminate.

\subsubsection{Hahn Series}
A Hahn series generalizes the idea of power series even further by allowing the exponents to be arbitrary values from... ...TBC...

% ...from an ordered subset of the value group ...but what does that mean?

%https://en.wikipedia.org/wiki/Puiseux_series 
%https://en.wikipedia.org/wiki/Hahn_series

% p-series:  sum_{k=1}^{\infty} 1 / k^p
% ..it's actually the Riemann zeta function just with p isntead of s?

\subsection{Trigonometric Series}
We now want to look at a completely different kind of series. It will again involve a variable $x$ but this time, we will not use powers of $x$ but instead trigonometric functions, specifically sines and cosines. We will look at series of the form:
\begin{equation}
\label{Eq:FourierSeries}
 f(x) = \frac{a_0}{2} + \sum_{k=1}^\infty \left(  a_k \cos(k x) + b_k \sin(k x) \right)
\end{equation}
As with power series, the $f(x)$ on the left hand side indicates that we intend to use such a series to define a function of $x$. By construction, our so defined function will be periodic with a period of $2\pi$. This can easily be seen as follows. The $a_0/2$ term will just give us a constant offset and that's periodic with any period. The $k=1$ term gives a sine and cosine of $x$ which are the prototypical functions with period $2\pi$. For higher $k$, the argument of the sin/cos pair will be multiplied by $k$ so the (co)sines will oscillate $k$ times faster and will have a period of $2\pi/k$. But of course, something that is periodic with period $2\pi/k$ is also periodic with period $2\pi$. So, all the higher terms will also be $2\pi$-periodic. They will just undergo $k$ cycles when our slowest sinusoid (for $k=1$) undergoes one cycle.

\medskip
There are different conventions for dealing with the $a_0$ coefficient which represents the constant part of the function, in electrical engineering also known as the DC (directed current) term. ...TBC..

% https://en.wikipedia.org/wiki/Fourier_series
% Leupold, Vol2, pg 50 ff
% maybe use \tau instead of 2 \pi 
% maybe assume a period of 2 pi - having to carry aorund the p and the omega makes things unnecessarily complicated. Yes, it's more general but the application to functions with arbitrary period can better be done on a higher level by stretching the functions appropriately. we don't want to deal with that inside the nitty gritty of Fourier analysis and it's more consistent with most books
% maybe use the convention that pulls the a0/2 out of the sum and starts the sum at 1. this relieves us from special casing the analysis equation for a_0 and also simplifies the explanation of the periodicity

%...TBC...

%explain different conventions - some use $2 \pi x$ as argument, some pull out $a_0/2$, etc.

\subsubsection{Fourier Expansion}
Recall that the Taylor expansion expressed arbitrary functions as (potentially) infinite weighted sums of powers of the variable $x$. The coefficients were computed using derivatives of the function at one single point. Fourier expansions, on the other hand, express arbitrary periodic functions as (potentially) infinite sums of sines and cosines of $x$. The coefficients are calculated by a definite integral over one period. The formulas are:
\begin{equation}
\label{Eq:FourierCoeffs}	
 a_k = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos(k x) \, dx, \quad
 b_k = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin(k x) \, dx 
\end{equation}
We can intepret (\ref{Eq:FourierSeries}) as a synthesis formula and (\ref{Eq:FourierCoeffs}) as an analysis formula in the sense that the former is used to synthesize (i.e. generate, create) a periodic function $f(x)$ whereas the latter takes a periodic function $f(x)$ as input, analyzes it, and spits out the synthesis coefficients. In signal processing terms,  the analysis formula actually computes a cross-correlation between the input function $f(x)$ and the respective sine or cosine function. The integration can also be performed over some other interval of length $2\pi$ like $[a,a+2\pi]$ for some $a$. [VERIFY! Wouldn't that change the phase of the coeffs? If not, say so and explain why not. I think, it's because our time axis anchor point $t=0$ is not really affected by that shift] 

\medskip
The requirements on $f$ are quite different from those for Taylor series. First of all and most obviously, we require $f$ to be periodic. Here, we restricted our attention to $2 \pi$-periodic functions, but the theory can easily be applied to functions with other periods by stretching the function appropriately along the $x$-axis.  For a Taylor series, we required $f$ to be smooth because we had to take (infinitely many) derivatives to compute the coeffs. For a Fourier series, $f$ does not even need to be continuous. We only require that the integrals in the analysis equation must exist and be finite. This is a much less strict requirement [WAIT: I think, it's actually an independent requirement - but maybe "less strict" in some "practical" sense]. If the function has jump discontinuities, it will converge to the average of the values on the left and right at the jump. If we use only finitely many terms, there will be some overshoot and wiggle around the jump. This is called Gibbs's phenomenon. Taking more terms will make these wiggles narrower in width but not smaller in height. Their height will settle to something like $9\%$ of the jump height when taking ever more but still finitely many terms. The exact amount of overshoot defines the so called WilbrahamâGibbs constant. The overshoots will disappear completely only due to their vanishing width in the limit of infinitely many terms.

% https://en.wikipedia.org/wiki/Gibbs_phenomenon
% https://mathworld.wolfram.com/Wilbraham-GibbsConstant.html


\subsubsection{Magnitude and Phase}
It is well known that a weighted sum of a sine and a cosine of the same frequency can also be expressed as just a sine or just a cosine. For this, we need to give the argument an appropriate phase shift $\varphi_k$ and scale the result by an appropriate amplitude factor $A_k$. The phase shift and the scale factor depend on the relative and total amounts of the sine and cosine components in the following way.:
\begin{equation}
 A_0 = \frac{a_0}{2},           \;\;
 A_k = \sqrt{a_k^2 + b_k^2},    \;\;
 \varphi_k = \atan2(b_k, a_k),  \qquad
 a_k = A_k \sin(\varphi_k),     \;\;
 b_k = A_k \cos(\varphi_k)   
\end{equation}
where the $A_k = \ldots$ formula applies only to $k \neq 0$. I have also given the conversion formula for the other direction. With these new coeffs, we can express the series as:
\begin{equation}
f(x) = A_0 + \sum_{k=1}^\infty A_k \sin(k x +\varphi_k) 
     = A_0 + \sum_{k=1}^\infty A_k \cos(k x +\varphi_k - \frac{\pi}{2})
\end{equation}
[VERIFY!] 
% Seems like we can't get rid of $A_0$ when we start the sum at $k=0$. phi_0 = 0 because atan2(0,x) is 0Â° for any x and sin(0) = 0 and cos(-pi/2) = 0. So the DC would be missing in the resythesis. But wouldn't it be nicer if the formula would cover the edge case also? can that be made to work? We would just need to ensure that phi_0 = pi/2, such that sin(phi_0) = cos(0) = 1, I think. 
% https://en.wikipedia.org/wiki/Sine_and_cosine#Identities
\newline



\medskip
...TBC... interpretation as amplitude and phase, symmetries (odd - no cosines, even - no sines)
%Leup2, pg 65

\subsubsection{Complex Formulation}
The theory of Fourier series if often being presented in its complex form. Doing so hides some of its complexity in the complex numbers. Depending on your fluency with complex numbers, this may feel like a simplification or like a mystification. The equations are shorter, yes. But they are complex - pun intended. Instead of using real valued sine and cosine functions for analysis and synthesis, one uses a single complex exponential function. This is equivalent due to Euler's formula. By doing so, one reduces the apparent complexity of having to deal with two sets of coefficients, namely the $a_n$ and $b_n$, to a single set of $c_n$ coefficients - but these coeffs are then complex valued. Going to the complex domain actually also generalizes the applicability to functions $f: \mathbb{R} \rightarrow \mathbb{C}$ as well, so we get a little bit more than just a simpli/mystification. Formerly, we were looking at functions $f: \mathbb{R} \rightarrow \mathbb{R}$. In the complex formulation, the synthesis and analysis equations look like:
\begin{equation}
\label{Eq:FourierSeriesComplex}
 f(x) = \sum_{k=-\infty}^\infty c_k  e^{\i k x}, \qquad
 c_k = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) e^{-\i k x}  \, dx
\end{equation}
We have now written down the synthesis and analysis equations together into one line and it's still shorter than the real valued formulation of the analysis equation alone. Notice also how there's no special case for $c_0$ like there was for $a_0$. It may seem a little bit strange that the summation in the synthesis equation now starts at minus infinity. Shouldn't the real and imaginary parts of the non-negatively indexed coeffs $c_0, c_1, \ldots$ already contain the same amount of information as the pairs $(a_0,b_0),(a_1,b_1), \ldots$ from the real formulation? Why do we need the additional coeffs with negative indices? Indeed, when the input function  $f$ is purely real valued, the negatively indexed coeffs will be redundant, but remember that we now allow, in general, complex valued functions $f(x)$, too (the input $x$ is still real, though). For the special case of real valued functions $f$, the $c_k$ will be conjugate symmetric, i.e. $c_{-k} = \overline{c_k}$. [VERIFY]. A $c_k$ with a negative index $k$ formally represents the coefficient for a negative frequency. That just means the complex phasor rotates clockwise rather than counterclockwise in the corresponding $c_k  e^{\i k x}$ term in the synthesis equation. This flips the sign of the sine part, the cosine part stays the same [VERIFY]

...TBC...

\subsubsection{Theorems}
...TBC...
% f even -> only cosines
% f odd  -> only sines
% f real -> c_k conjugate symmetric, A_k even, phi_k odd, a_k even, b_k odd
% energy -> Pasreval's theorem
% multiplciation -> convolution
% only positive freqs -> f has real and imag part 90Â° out of phase at each frequency

% https://en.wikipedia.org/wiki/Fourier_series#Table_of_basic_properties
% taking derivatives and intergrals multiplies fourier coeffs k or 1/k

\subsubsection{Aperiodic Functions}
So far, we assumed $f$ to be periodic. If you apply the analysis equation as is to an aperiodic input function or signal $f$ and then try to resynthesize your signal via the synthesis equation, the resynthesized signal will periodically repeat the chunk of the function that you have analyzed, i.e. here the portion of the function between $-\pi$ and $\pi$. You will have pretended to the analysis equation that this chunk was a period from a periodic signal when in fact it wasn't, so to speak. The analysis equation still interprets it as such, though. The theory can actually be generalized to deal properly with (some) aperiodic functions, too. This leads to the idea of the Fourier transform which will be treated in more detail in the section about integral transforms. At this point, I'll just say that the sum in the synthesis equation will be replaced by an integral, our discrete Fourier coefficients $a_k, b_k$ will be replaced by a continuous function of frequency and in the analysis equations, we will have to perform an integration from minus to plus infinity instead of over one period. Within this wider framework, the periodic functions will be found as a special case, if we also allow the spectra (i.e. the continuous functions of frequency) to be composed from (shifted and scaled) Dirac delta "functions" [VERIFY!]. Defining those rigorously requires some more advanced mathematical machinery that we will treat later but informally, they can be thought of as infinitely narrow spikes with infinite height and a finite nonzero area. If that doesn't seem to make any sense, then yeah - that's why this is only an informal view.


% These is also some process called "periodization" of a a function f. It involves taking an infinite
% sum from -inf to +inf over f(x + n * p) where p is the period, i.e. 
% f_p(x) = \sum_{n=\-infty}^{\infty} f(x + n p)
% https://en.wikipedia.org/wiki/Poisson_summation_formula



\subsubsection{Fourier Polynomials}
If we truncate a Fourier series at a given index, say $n$, then we obtain what is called the \emph{Fourier polynomial} of degree $n$. ...ToDo: explain Gibbs ripples and how they can be avoided by Cesaro summation, spectral windowing, etc.


\subsubsection{Connection to Power Series}
Taylor and Fourier series seem to be totally disconnected things. However, when we widen our perspective to the complex domain, a beautiful connection between these two emerges...

\medskip
ToDo: bring stuff from Weitz video "Taylor trifft Fourier"
% https://www.youtube.com/watch?v=cXxsSdbfEEg
%\hyperlink{https://www.youtube.com/watch?v=cXxsSdbfEEg}{youtube.com/watch?v=cXxsSdbfEEg}


% https://www.youtube.com/watch?v=zXd743X6I0w  Fourier-Analysis in 100 Minuten


% Another beautiful example for how math is full of unexpected connections. It's like a huge interconnected network of bits of knowledge that we can look at and stand in awe.


%\subsection{Dirichlet Series}


\begin{comment}
ToDo:
-Mention other types of series
 -Dirichlet series: sum_n a_n / n^s, relation to Euler products. See "Elliptic Tales" 
  pg 168, 174. Is there an Euler product for each Dirichlet series, i.e. a way to convert
  back and forth between infinite sums and infinite products? Yes, seems so:
  https://en.wikipedia.org/wiki/Euler_product
  https://de.wikipedia.org/wiki/Euler-Produkt
  https://en.wikipedia.org/wiki/Dirichlet_series
  https://en.wikipedia.org/wiki/General_Dirichlet_series
  https://de.wikipedia.org/wiki/Dirichletreihe
  https://en.wikipedia.org/wiki/Dirichlet_L-function
  https://en.wikipedia.org/wiki/Dirichlet_convolution
  https://fractional-calculus.com/dirichlet_series_taylor_series.pdf
  ...but wait: Does an Euler product actually have coefficients? It has this P(p,s) function.
 -BÃ¼rmann series: 
  BÃ¼rmann vs Fourier: EinfÃ¼hrung zum Thema BÃ¼rmann-Reihen:
  https://www.youtube.com/watch?v=WjCRdPBgSZg
  Besser als TAYLOR-REIHEN: BÃRMANN-REIHEN EXPLODIEREN NICHT!:
  https://www.youtube.com/watch?v=VcXl1oqQJOA
 -Sine and cosine series. This video at around 22:00
  https://www.youtube.com/watch?v=lt_QHey_yIQ
  has an interesting segment about these. They use either sines or cosines but not both but they allow for multiples of half-cycles (not only full cycles as in Fourier series). Any function can be decomposed in both of these ways, but, depending on the nature of the function (in particular, its values at the boundaries), one or the other may converge more quickly.
 -Laurent series. They are actually Taylor series for points with finite y-value but they can also
  model the infinitey y-value at the pole ...what about using a Taylor series for 1/f(x) at a 
  pole? Maybe try it with log(x) - find a Taylor series for 1 / log(x) - it is super steep 
  at x = 0: https://www.desmos.com/calculator/qblvhbkkgu  
  interesting: 1/log_2(x) https://www.desmos.com/calculator/1ceyqmelry
  goes through (-0.05,-1) - what's so special about 0.05? ...It's the base of the log divided by
  10...why 10? That seems arbitrary. I mean, it's the base of the number system but what has the number system to do with anything here?
-General method to obtain a series from a sequence that leads to thinks like Taylor series:
   a_n  ->  \sum_{n=0}^{\infty}  a_n  f(x, n)
  The Taylor series of exp(x) arises from choosing f(x,n) = x^n / n! for example. What other 
  interesting choices could ther be for f(x,n)? n^(-x) gives a Dirichlet series, I think
  


Maybe make a subsection about asymptotic series:
https://en.wikipedia.org/wiki/Asymptotic_analysis
https://en.wikipedia.org/wiki/Asymptotic_expansion
https://en.wikipedia.org/wiki/Borel_summation
https://en.wikipedia.org/wiki/Singular_perturbation
but maybe is should go into a section "Asymptotic Analysis" in its own right after the 
Series section. Or maybe in some section Mathematical Modeling in an Applications part.
Maybe in a section about "Approximation Theory" together with subsctiosn about "Curve Fitting",
"Polynomial Approximation" (Chebychev, Lest Squares, MiniMax), etc.
-How about a Taylor-Series around "infinity"? Maybe derive expressions for a Taylor series
 around a general x_0 and then let x_0 approach infinity for each term separately?
-Stirling formula for factorial

https://www.youtube.com/watch?v=G-29AZf-NkU
-12:16 The limit of a sequence of continuous functions is not necessaricly continuous. Example:
 f_n(x) = x^n on [0,1]  ->  converges to 0 for x < 1 and to 1 for x = 1 and diverges for x > 1
 we need a stronger condition "gleichmÃ¤Ãige Konvergenz" ..dunno what that is in english

Make a section about divergent series and how we may deal with them:
-Generalized summation methods (e.g. Cesaro summation)
-Riemann reordering theorem
-Analytic Continuation
See:
https://www.youtube.com/watch?v=FmLIGN8ZGdw  The Return of -1/12 - Numberphile
https://www.youtube.com/watch?v=YuIIjLr6vUA  Numberphile v. Math: the truth about 1+2+3+...=-1/12
https://www.youtube.com/watch?v=jcKRGpMiVTw  Ramanujan: Making sense of 1+2+3+... = -1/12 and Co.
https://www.youtube.com/watch?v=U0w0f0PDdPA  Can you change a sum by rearranging its numbers? --- The Riemann Series Theorem

Trevor Bazett on how to use Cesaro summation to counteract the gibbs Phenomenon in Foruier 
series:
https://www.youtube.com/watch?v=AkPZcS8eqmA  Could 1-1+1-1+1-1+1-1+... actually converge?

https://en.wikipedia.org/wiki/Divergent_series

Has also Cesaro summationa and Abel Summation
https://www.youtube.com/watch?v=LDuD_ClkkG8
 

Vergrabene SchÃ¤tze: Laurent-Kriterium (Reihenkonvergenz)
https://www.youtube.com/watch?v=X2Ad8kI4Onk
 

Can you change a sum by rearranging its numbers? --- The Riemann Series Theorem
https://www.youtube.com/watch?v=U0w0f0PDdPA
-At the end (at 20:32) it discusses the reason why in the space of sequences, the subspace of 
 absolutely summable sequences is of importance
-My idea for two sequences that diverge but whose dot-product converges:
 a_n = 1/n, b_n = (-1)^n. a_n is the harmonic series which diverges to infinity. b_n just alternates
 between +1 and -1. The dot product between a and b is the alternating harmonic series which 
 converges (conditionally). 
-Can we create two sequences whose sum converges but whose dot product doesn't? 


Metatheorem on series: Is there a slowest diverging series?
https://www.youtube.com/watch?v=_ztFepRMRSE
-Abel Reihen, Cauchy Verdichtungskriterium
 https://de.wikipedia.org/wiki/Cauchysches_Verdichtungskriterium

Absolutely Amazing Summations (Series Compilation)
https://www.youtube.com/watch?v=k7ltYMM3oyM

 
\end{comment}
