\section{Functionals} 

% This is now obsolete - Lebesgue integration is now treated in Calculus/Integration
%\subsection{Lebesgue Integration}
%In functional analysis, we will encounter a lot of integrals. Sometimes, however, we will have to deal with functions or function-like objects that are not Riemann integrable. A new notion of integrals is desired that will give the same results in the case of Riemann integrable functions but is applicable to a wider range of objects. That's what the Lesbesgue integral does for us. To define the Lebesgue integral, we will first need to define the notion of a measure.

%\subsubsection{The Lebesgue Integral}


\subsection{Test Functions  [TODO: move to function spaces]}
For the following material, we will need to work with a set of functions with some special properties. We want them to be smooth, which means they should be differentiable infinitely often everywhere. We also want them to have a finite support, which means they should be identically zero everywhere except within some finite interval $(a,b)$. This interval is assumed to be fixed and the same for each of the functions within the set. [VERIFY! Nope! I think, that's wrong!] The set of all functions that satisfy these two requirements will be called the set of "test functions". . At first glance, it may be a bit surprising that functions that satisfy both of these criteria even exist (it certainly suprised me). To make a function identically zero outside a given interval typically requires a piecewise definition and such piecewise definitions tend to make the functions non-smooth at the junctions of the pieces. However, such smooth, piecewise-defined functions do indeed exist, and a standard example is the so called bump function:
\begin{equation}
f(x) = 
 \begin{cases}
 \exp \Bigl( \frac{1}{x^2-1} \Bigr)  \qquad &  |x|   <  1 \\
 0                                   \qquad &  |x| \geq 1
 \end{cases}
\end{equation}
It is indeed infinitely often differentiable everywhere, even at the potentially problematic junctions at $-1$ and $+1$. It is, however, not analytic there: a power series expansion centered at these points will not converge to the function $f$.

% https://en.wikipedia.org/wiki/Bump_function
% https://en.wikipedia.org/wiki/Spaces_of_test_functions_and_distributions
% https://en.wikipedia.org/wiki/Schwartz_space

% ToDo:
% -explain where the name "test function" comes from
% -show smoothness of bump function
% -show that the Taylor series of the bump function around -1 and +1 does not converge to the
%  function. Figure out to what it converges, if at all (maybe to the zero-function?).
% -Plot the bump function. Maybe try to make a small plot next to the formula. Maybe that would be
%  a nice general way to do it throughout the book - it would be space efficient. There's typically
%  a lot of whitespace around formulas anyway


\paragraph{Notation}
As mentioned above, the set of functions that are continuous on a given interval $(a,b)$ is denoted as $C(a,b)$ and the set of functions that are continuously differentiable $k$ times on $(a,b)$ is denoted as $C^k(a,b)$. A smooth function is continuously differentiable infinitely often, so here $k = \infty$ and hence the set of smooth functions on $(a,b)$ is denoted as $C^{\infty}(a,b)$. Finally, the set of smooth functions that are zero outside the interval $(a,b)$, i.e. the set of our test functions, is denoted as  $C_0^{\infty}(a,b)$.
% Maybe do not regurgitate this stuff! Just explain the new thing:  $C_0^{\infty}(a,b)$
%or should we use brackets [] instead of ()?

% ToDo:
% Explain interpretation of test functions as weighting functions in a weighted average.


\subsection{Distributions}
In general, a dual space $V^*$ with respect to a given vector space $V$ is defined as the set of all linear functions that take as input an element of the set $V$ and spit out a scalar, i.e. a (typically real) number. What we want to define here is the space that is dual to the space of our test functions. This is the space of all linear functionals that take as input a test function and produce a real number as output. Elements of this set are called distributions and in a certain sense, they can be seen as a generalization of the idea of a function. 

% https://en.wikipedia.org/wiki/Distribution_(mathematics)

\subsubsection{Regular Distributions}
One way to construct such a linear functional is to multiply the input (test) function $f$ with some fixed other function $g$ and integrate the product over the interval $(a,b)$ over which our test function $f$ is nonzero\footnote{Which is obviously equivalent to integrating from $-\infty$ to $\infty$}. Let's call the resulting functional $G$, so we have:
\begin{equation}
 G(f) = \int_a^b g(x) f(x) \; dx
\end{equation}
% does the order of f and g matter? could we be dealing with a non-commutative multiplication?
The so constructed functionals are what we will call \emph{regular distributions}. If we identify the functionals with the functions that induce them (i.e. here, identify $G$ with $g$, assuming $a,b$ are fixed once and for all), we have a space that looks a lot like a function space, i.e. a vector space, whose elements are functions. However, the space of functionals can contain also more general objects, i.e. functionals that are not induced by a function via such an integral. A simple example of such a functional could be to just take $f$ and spit out the value of $f$ at some given fixed $x_0$. This also does everything we would expect from a functional: take a function as input, spit out a number. It is linear, too. We will call this functional a distribution, too - but it's not a regular one anymore because it's not defined by our integral construction. Instead, it's called a \emph{singular} distribution.

% claude.ai says that these are called singular distributions.
% Here are some more singular distributions:
% https://claude.ai/chat/710cfb21-fc76-4a6f-934b-a246230f9fd8

% Q: what do we need to assume about g? does it need to be square-integrable, for example? or will any arbitrary function do?

\subsubsection{Evaluating Distributions} ...TBC...ToDo: Explain that for distributions, it doesn't make sense (in general) to ask what value they have at a given point. 

% Instead, we are only allowed to ask questions about what the integral above evaluates to for a given test function $f$. We can "test" the distribution $g$ with $f$, where $f$ may be localized (I guess) but we cannot ask for the value of $g$ at a specific $x$. So, "evaluating" a distribution amounts to testing / probing it with a test function?

% ToDo:
% -explain equality of distributions and its relation to weak equality of functions - I think:
%  -Two (regular) distributions G and H are equal, if they produce the same value of the integral
%   given above for *any* (test) function f
%  -Two functions g and h are weakly equal if their associated regular distributions are equal
%  -weakly equal functions may differ from one another in isolated points as long as their values
%   are finite in these points
%  -This notion of weak equality establishes an equivalence relation of the set of (suitable)
%   functions

\subsubsection{The Dirac Delta-Distribution}
One very important example of such a singular distribution is the Dirac delta distribution that just maps any given function $f$ to its value at $x = 0$:
\begin{equation}
 D(f) = f(0)
\end{equation}
If we would try to define this distribution $D(f)$ via an integral over a product of $f(x)$ with some fixed function $\delta(x)$ as we did above for the regular distributions, we would formally get something like this:
\begin{equation}
\label{Eq:DiracDeltaDesired}
 D(f) = f(0) = \int_a^b \delta(x) f(x) \; dx
\end{equation}
and now we would ask ourselves, how the hypothetical function $\delta(x)$ would have to look like to make this work (spoiler alert: there is no such function). We would need to construct a function $\delta(x)$ which for any function $f$, when their product is integrated over an interval $(a,b)$ (which we assume to contain zero, i.e. $a < 0 < b$), gives the value of $f$ at zero. Let's try something that almost works: Pick a small positive number $\epsilon$, at least small enough such that the interval $(-\epsilon, \epsilon)$ is completely contained in $(a,b)$. Then define $\delta_{\epsilon}$ as:
\begin{equation}
\delta_{\epsilon} (x) = 
\begin{cases} 
 \frac{1}{2 \epsilon} \qquad & |x|   <  \epsilon \\
 0                    \qquad & |x| \geq \epsilon
\end{cases} 
\end{equation}
% 1/(b-a) for |x| < epsilon, 0 otherwise. It's a rectangular impulse of height 1/(b-a). Integrating delta_epsilon from a to b (or, equivalently, from -epsilon to epsilon) gives 1
Because we assumed that the interval $(-\epsilon, \epsilon)$ is completely contained in $(a,b)$, the integral of $\delta_{\epsilon}$ from $a$ to $b$ would be equivalent to the integral from $-\epsilon$ to $\epsilon$ because outside that smaller interval, the function is zero anyway. The value of the integral would be exactly $1$ because the width of the integration interval is $2 \epsilon$ and the height (i.e. the function value) is constantly $\frac{1}{2 \epsilon}$ such that the area computed by the integral is just the area of a rectangle with width $2 \epsilon$ and height $\frac{1}{2 \epsilon}$. When we would use $\delta_{\epsilon}$ in place of $\delta$ in an integral such as the one in equation (\ref{Eq:DiracDeltaDesired}) above, the output would not be exactly $f(0)$ as we would like it to be but it would be close. Specifically, it would be the average value of $f$ over a small interval (of width $2 \epsilon$) centered around zero. If we assume $f$ to be nice enough (i.e. continuous), then the smaller we take $\epsilon$, the closer the \emph{average around} zero will be to the actual \emph{value at} zero, so the better our approximation will get. If we now imagine to let $\epsilon$ approach zero, it may actually work out. The average value \emph{around} zero will certainly approach the value \emph{at} zero when the averaging interval approaches zero. However, letting  $\epsilon$ approach zero will let $\delta_{\epsilon}$ approach a nonsensical function: it approaches a function that is zero everywhere except at $x=0$ where its value is infinite. Infinity, however, is not a real number and we cannot really meaningfully treat it as such. Paul Dirac and with him the whole physics community nevertheless use this limiting "function" anyway and very successfully so. This "function" $\delta(x)$ is the (in)famous Dirac delta function. In practice (e.g. physics), it usually works well enough to think about this strange object $\delta(x)$ as a bizarre function-like object resulting from a limiting process over perfectly reasonable functions such as $\delta_{\epsilon}(x)$. In theory (rigorous mathematics), one has to accept, that a Dirac "function"  $\delta(x)$ does not really exist as such and it makes only sense to talk about the Dirac distribution $D(f)$ as a functional which is not defined via an integral. We shall take the practical point of view and pretend that $\delta(x)$ exists as a function. If it's good enough for Dirac, it's good enough for me! In the literature, the distinction between "delta-function" and "delta-distribution" is blurred and the two terms are often used interchangably and you will also find notations like $\delta(f)$ for what I have denoted here as $D(f)$. I did this to make explicit the distinction between the (non-existent) function $\delta$ and the (indeed existent) functional $D$ that $\delta$ would induce, if it would exist.

% todo: sifting property, unit-integral porperty, applications
% in precisely such a way that the integral of the whole function is unity for any integration interval that contains zero

% Couldn't we define the delate function as an actual funtion from the reals to the hyperreals?
% There, a weighted infinity is actually a thing. We could say: \delta(0) = \omega or something.

% see here:
% https://www.youtube.com/watch?v=zJk4yuzJU3s
% Demystifying the Dirac Delta - #SoME2
% -Dirac measure
% -Riesz-Markov-Kakutani theorem

% When functions have no value(s): Delta functions and distributions
% https://math.mit.edu/~stevenj/18.303/delta-notes.pdf  
% -Very well writtrn lecture notes. It's like a tutorial.
% -Has an example for when one cannot exchange limits and differentiation. Maybe that example could
%  be brought up int the Differentiation section

\subsubsection{Distributional Derivatives}
One of the motivations to develop this theory was to make sense of situations where we need to take derivatives, for example in differential equations, but the functions we have to deal with are not necessarily differentiable in the usual sense. What we want is a generalization of the derivative that is applicable to a wider class of functions or function-like objects. This wider class is the set of distributions. Remember that our plain old functions can be identified with the regular distributions which are a subset of all distributions. A distribution, seen as a functional, is completely defined by its input/output relation: if we know what it does to any input function $f$, i.e. which output number it produces, we know everything there is to know about the functional. Let's revisit our definition of a regular distribution $G(f)$, which was defined via an integral over a product of $f$ and some inducing function $g$, and see what happens, when we take the derivative $g'$ of $g$ instead of $g$ itself inside this integral. Let's call the resulting functional $G'$:
\begin{equation}
 G'(f) = \int_a^b g'(x) f(x) \; dx
\end{equation}
Now let's assume, $g$ is actually some nasty, non-differentiable function such that $g'$ makes no sense. We can nevertheless make sense of $G'$ by moving the prime in $g'$ over to $f$. What allows us to do this is the rule for integration by parts:
\begin{equation}
 \int_a^b g'(x) f(x) \; dx = \Big[g(x) f(x)\Big]_a^b - \int_a^b g(x) f'(x) \; dx
\end{equation}
Now there's no $g'$ anymore in the right hand side. Instead, we have to differentiate $f$ and that will never pose a problem to us because $f$ was assumed to be a test function, so it's differentiable infinitely often. For test functions, we also we have $f(a) = f(b) = 0$ which makes the boundary terms vanish. So, what remains is:
\begin{equation}
 G'(f) = - \int_a^b g(x) f'(x) \; dx
\end{equation}
So, we may compute the functional $G'(f)$ for any test function $f$ even for a non-differentiable inducing function $g$ by simply moving the prime from $g$ to $f$ and prepending a minus sign. This motivates the following definition of the \emph{distributional derivative}:
\begin{equation}
  G'(f) = -G(f')
\end{equation}
The distributional derivative is also called the \emph{weak derivative}. ...TBC...
% give general formula for the n-th distributional derivative using the scalar-product notation

% The distributional derivative is also called the weak derivative

% todo: consider as example f(x) = |x| and give the expressions for some of its distributional derivatives. the 1st will be a scaled and shifted Heaviside function, the 2nd will be a scaled dirac delta function, etc.

% introduce the scalar-product alike notation for the dual pairing of distributions and tes functions

% https://de.wikipedia.org/wiki/Schwache_Ableitung
% https://de.wikipedia.org/wiki/Distribution_(Mathematik)
% -Räume schwach differenzierbarer Funktionen sind die Sobolev-Räume. Ein noch allgemeinerer Begriff der Ableitung ist die Distributionenableitung

\paragraph{Example: Derivative of $|x|$}
To see our new tool in action, consider the absolute value function $f(x) = |x|$. We can write down the derivative as:
\begin{equation}
f'(x) = 
 \begin{cases}
 -1 \qquad  x < 0 \\
 +1 \qquad  x > 0
 \end{cases}
\end{equation}
But note that this doesn't say anything about $f'(x)$ at $x = 0$. We can't assign a value there because $f(x) = |x|$ not differentiable at $x = 0$ in the usual sense. But it is differentiable in the weak sense. The derivative given above has only values for $x \neq 0$. But that is no problem if we view $f'$ as a distribution. Distributions are not supposed to have values everywhere - or anywhere at all, for that matter. All that we want from a distribution is to test it with a test function. We do not generally expect to be able to evaluate it a an arbitrary point in the sense of asking: "What's your value there?". If we can, that's a bonus but it's not required. ...TBC...explain the Heaviside step function - if shift and scale $f'$ above appropriately and arbitrarily define the value at zero to 1, we obtain it.

%Note that for the weak derivative

\subsection{Weak Form of Differential Equations}

% We only care about weak equality between LHS and RHS, i.e. equality "under the averaging integral" rather than exact pointwise equality


\begin{comment}

-Frechet-derivative: derivative for functionals - is this similar to a directional derivative? nah! that's the gateaux derivative
https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative
https://en.wikipedia.org/wiki/Gateaux_derivative
...maybe the gateaux derivative can be introduced in the context of functionals but the frechet derivative only after operators have been discussed. the frechet derivative seems to have stronger preconditions to exist...but maybe they should both be introduced together in a chapter after functionals and operators - maybe it fits into calculus of variations, see also:

https://en.wikipedia.org/wiki/Functional_derivative
https://cds.cern.ch/record/1383342/files/978-3-642-14090-7_BookBackMatter.pdf
...looks like the functional derivative is a continuous analog of the total differential (the sum over partial derivatives is replaced by an integral)




https://en.wikipedia.org/wiki/Lebesgue_integration
https://de.wikipedia.org/wiki/Lebesgue-Integral

https://en.wikipedia.org/wiki/Distribution_(mathematics)
https://en.wikipedia.org/wiki/Bump_function
https://en.wikipedia.org/wiki/Mollifier

https://www.math.arizona.edu/~kglasner/math456/GREENS1.pdf

Example functionals:
https://en.wikipedia.org/wiki/List_of_mathematic_operators
-expectation value, variance, moments of probability distribitions
-norm
-differential entropy

Maybe it could make sense to view the delta distribution as a kind of bridge between continuous and discrtete math? It's possible to use it, for exaample, to define continuous probability density functions that are nonzero only at the integers. Or - of course - discrete time signals can be defined in terms of scaled and shifted delta functions on a continuous time axis.


https://www.youtube.com/watch?v=6uqrTMAD4PA
at 1:25 Riesz Theroem:
Every (bounded, continuous) linear functional f over a Hilbert space H can be represented by a member y of that space itself. The functional is then given by a inner product f(x) = <x,y> for all x in H. ...maybe use as notation F for the functional g for y and f for x such that 
F(f) = <f,g>  for all f in H. Every such functional is thus just an inner product of the member x (or f) with a fixed function y (or g)


Impossible? Nope — You Can Differentiate This
https://www.youtube.com/watch?v=WEcms_mLij0

ToDo:
-Give some example functionals that are important in applications
 -A typical penalty term for data modeling looks like:  P = \int_a^b (f'')^2 dx
  It penalizes the aggregated curvature of the function f to favor less wiggly functions.
 -In general, we could have some form like  P = \int_a^b g( T[f](x) ) dx  where T is an arbitrary
  operator (like d^2/dx^2) and g is an arbitrary function (like x^2).

\end{comment}