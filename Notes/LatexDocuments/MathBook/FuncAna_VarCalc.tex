\section{Calculus of Variations} 


In multivariable calculus, we were interested in finding minima and maxima of functions that had multiple inputs. The strategy was to compute an expression for the gradient and requiring it to be the zero vector. In the calculus of variations, aka variational calculus, we will take this a step further: We will minimize functionals with respect to their input function. That means we are interested in finding a function that minimizes a given functional. We may interpret this as minimizing something that depends on (uncountably) infinitely many inputs - not just 2 or 3 or 1000 as in multivariable calculus.


\begin{comment}

-derivatives of functionals: variation (as continuous analog of the total differential), Frechet- and Gateaux derivative

-can we interpret the functional derivative (i.e. the variation) as some continuous analog of the norm of the
 gradient vector? can the minimization problem be cast into setting this norm to zero?
 
-make derivation of the variation similar to the one in Susskind's theorectical minimum, volume 3

-minimization of functionals 
 -applications: 
  -math: minimal surfaces, catenary, straight line as minimization problem
  -physics: principle of least action and least time, Lagrangian mechanics, 

-connection between variational problems and differential equations: a diffeq is the solution to a variational problem - how can we go the other way around and find the variationl problem when given a diffeq?

\end{comment} 