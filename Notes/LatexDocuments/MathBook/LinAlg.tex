\chapter{Linear Algebra}
Linear algebra is the study of vector spaces and linear operations that can be applied to elements of these spaces. It provides tools for solving linear systems of equations and for doing geometric calculations. A vector space has two ingredients: a set of elements which are called vectors and a set of scalars which are elements of another set but can "interact" with the vectors. With these vectors and scalars, you can do two things: you can add together two vectors to get another vector and you can multiply a vector by scalar to get another vector. Scalars derive their name from the fact that they "scale" vectors, i.e. change their "size". You can imagine the scalars to be just regular old real numbers. These operations of vector addition and multiplication by a scalar must follow the usual associative, commutative and distributive laws. An operation is said to be linear, if two properties hold: First: applying the operation and then scaling the output by a factor should give the same result as scaling the input by the same factor and then applying the operation. This feature is called homogeneity. Second: adding two inputs and then applying the operation to the sum must give the same result as applying the operation to both inputs separately and then summing the results. This feature is called additivity. In formulas, saying that a function $f = f(x)$ is linear means:

\medskip
\begin{tabular}{l l l}
Homogeneity:   & $f(a x) = a f(x)$ & \text{where $a$ is a constant}        \\
Additivity:    & $f(x + y) = f(x) + f(y)$  \\
\end{tabular}
\medskip

So, linearity means homogeneity and additivity taken together. It is actually possible to condense these two requirements into a single equation:

\medskip
\begin{tabular}{l l l}
Linearity:    & $f(a x + b y) = a f(x) + b f(y)$  & \text{where $a,b$ are constants}  \\
\end{tabular}
\medskip

...but it is usually more convenient to split this single requirement into the two above because they are easier to understand. If the space is $n$-dimensional where $n$ is a natural number, vectors can be represented by an array of $n$ numbers and linear operations can be represented by $n$-by-$n$ matrices (i.e. 2D arrays) of numbers. These numbers will depend on the choice of a basis which determines our coordinate axes, but the vector or operation (which is \emph{represented} by the array or matrix respectively) does not depend on that choice. Vectors in 2D or 3D can be visualized as arrows and linear operations can be visualized as moving the tips of the arrows around in the space (in particular, constrained ways). In a more general setting, operations can also be defined to take a vector from one vector space as input and produce an output that lives in a different vector space. In this case, the representing matrix will be $m$-by-$n$ ($m$ rows, $n$ columns) where $m$ is the dimensionality of the output space and $n$ the dimensionality of the input space and you have to make a choice for the basis for both of these spaces. Vector spaces can also be infinite dimensional. For example, you may consider the "space" of all functions defined on some interval. In such a case, the operations are usually called "operators". These operators take a function as input and produce another function as output. Examples of such operators are: take the derivative or antiderivative, multiply by a number, multiply by some other (fixed) function, take the square, etc. (the last one being actually a nonlinear operator). But this "infinite linear algebra" is part of a more advanced subject, called functional analysis, which we will come back to later when we have more mathematical tools in or toolbag. Linear algebra is an important subject because many real world problems can be posed in the language of linear algebra and therefore solved by its machinery. An example of that would be a ("least-squares") optimization problem. We'll see later what that is. Also, many higher level math subjects make heavy use of concepts from linear algebra. For example, in multivariable calculus, we will encounter vectors and matrices of derivatives. In the field of linear differential equations, which are important in modeling a lot of physical (and also chemical, biological, ecological, social, etc.) phenomena, we will encounter concepts from linear algebra as well. And even when the models become nonlinear, it will still help because we can often gain insights by "linearizing" (i.e. using a linear approximation of) our models around a certain point of interest. So, linear algebra is a foundational topic that everybody who wants to learn some higher level math needs to be familiar with.


%First things first, so let's now take a look at finite dimensional vector spaces.

% Maybe write some more - the amount of text just bleeds into a second page - that's ugly
% maybe explain why linear algebra is important - what problems does it solve?
% Explain what linearity implies: if we have two solutions of a problem, we can generate more
% by forming linear combinations...or something.

\begin{comment}

https://en.wikipedia.org/wiki/Linearity

\end{comment}