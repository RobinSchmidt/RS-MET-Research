\section{Matrix Decomposition}
By a matrix decomposition, we mean a way to express a given matrix $\mathbf{A}$ in some way via other matrices. This "some way" is usually a matrix product and these "other matrices" typically have some desirable features which makes them more conducive to understand what is going on and/or to facilitate certain algorithmic computations. For example, the so called "singular value decomposition" (SVD) is good for building intuition because it decomposes the matrix into a rotation, an axis-aligned scaling and another rotation - all these operations are easily understood geometrically. The "lower-upper" (LU) decomposition is good for solving linear systems of equations. There are also additive decompositions, i.e. decompositions based on a sum of matrices. Finally, there are also decompositions based on other types of matrix products such as the Kronecker product. However, the most important ones are certainly the decompositions based on the regular matrix product.


% https://en.wikipedia.org/wiki/Matrix_decomposition

%===================================================================================================
\subsection{LU Decomposition}
The LU-decomposition decomposes the matrix into a product $\mathbf{A = L U}$ where $\mathbf{L}$ is a lower triangular matrix and $\mathbf{U}$ is an upper triangular matrix. The LU-decomposition is algorithmically basically Gaussian elimination with some modifications. If you have an LU decomposition of a matrix $\mathbf{A}$ available, then it is straightforward to solve an equation $\mathbf{Ax = b}$. In practice, one often computes a decomposition of the form 

..TBC...explain details of algorithms for the decomposition and the solution of linear systems.
% Lower Triangular, Upper Triangular

% https://en.wikipedia.org/wiki/LU_decomposition

%===================================================================================================
\subsection{QR Decomposition}
% Orthonormal, Upper Triangular
% I think, the R stands for right-triangular?

%https://en.wikipedia.org/wiki/QR_decomposition

%===================================================================================================
\subsection{Eigendecomposition aka Diagonalization}
The most convenient decomposition of a matrix in terms of ease of solving equations involving the matrix and in terms of geometric interpretability of what the matrix actually does is the so called \emph{diagonalization} of the matrix. It's also called \emph{eigendecomposition} or \emph{spectral decomposition}. The latter name derives from the fact that the \emph{spectrum} of a matrix is defined to be the set of its eigenvalues. As you may guess from the term "eigendecomposition", the eigenvalues and eigenvectors play a prominent role in this kind of decomposition. The matrix $\mathbf{A}$ is expressed as $\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}$ where the $i$-th column of $\mathbf{V}$ is the $i$-th eigenvector $\mathbf{v}_i$ of $\mathbf{A}$ and $\mathbf{\Lambda}$ is a diagonal matrix whose diagonal entry at position $(i,i)$ is the eigenvalue $\lambda_i$. Unfortunately, such a diagonalization is not always possible. If it is possible for a given matrix, then we call that matrix \emph{diagonalizable}. Compare the expression $\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}$ to what we had in the section about similarity transformations. There, we said that a matrix $\mathbf{A}$ is similar to another matrix $\mathbf{B}$, if there exists a matrix $\mathbf{S}$ such that $\mathbf{A} = \mathbf{S} \mathbf{B} \mathbf{S}^{-1}$. Here, we have a special case of that more general situation, namely a case where the matrix $\mathbf{B}$ is diagonal. That tells us that if $\mathbf{A}$ is diagonalizable, it is similar to a diagonal matrix. That means, there exists some coordinate system in which the linear transformation represented by $\mathbf{A}$ takes the form of an axis-aligned scaling [VERIFY!].



...TBC...

% 


% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Simultaneous_diagonalization
% A set of matrices commutes iff it is simultaneously diagonalizable

% When we apply the product A = S^-1 * D * S to a vector, the rightmost matrix S transforms the
% standard basis vectors (1,0,0),(0,1,0),(0,0,1) to the eigenvectors of A because its columns *are*
% the eigenvectors. The columns of a matrix always say, where they map the standard basis vectors
% to. Then we apply a scaling along the new axes in the new coordinate system. Then we undo the
% initial transformation.

% The eigendecomposition is also useful for computing the matrix exponential

% Explain connection to similarity transforms.

% https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix

%---------------------------------------------------------------------------------------------------
\subsection{Jordan Normal Form}
A diagonalization of a matrix is usually the "best" decomposition we can hope for - but it is unfortunately not always possible. In such unfortunate cases, a \emph{Jordan normal form} is one of the two second best things that we can do. The good news is that such a form always exists [VERIFY!]. The bad news is that finding it is a rather complicated business ...TBC...


% "best" in what sense?  In terms of convenience, facilitation of computations, interpretability?



% interpretation of the Jordan cells as representing a pair of complex conjugate eigenvalues?
% is the failure to be diagonalizable due to complex eigenvalues? Nah - I think, it's due to
% multiplicity

% Here:
% https://en.wikipedia.org/wiki/Matrix_similarity
% it says that over the complex numbers, every matrix is similar to a Jordan Normal Form

% https://en.wikipedia.org/wiki/Jordan_normal_form
% https://en.wikipedia.org/wiki/Generalized_eigenvector

% Jorand-Arnold Canonical Form (Mathe mit 2x2 Matrizen pg 44)

%---------------------------------------------------------------------------------------------------
\subsection{SVD - Singular Value Decomposition}
% Orthonormal, Diagonal, Orthonormal

I said that for non-diagonalizable matrices, the Jordan normal form is one of the two second best things we can do. The other second best thing is the so called singular value decomposition. It is another decomposition that always exists....TBC...

% Does it exist also for non-square matrices? I think so

%we can do when a matrix is not diagonalizable ...TBC...




% https://de.wikipedia.org/wiki/Schur-Zerlegung
% https://en.wikipedia.org/wiki/Schur_decomposition



%===================================================================================================
\subsection{Non-Multiplicative Decompositions}
The matrix decompositions that we have seen so far decompose a given matrix $A$ into a product of other matrices with certain desirable features where "product" is understood as matrix multiplication. There are some more exotic decompositions which are not based on the matrix product but on other matrix operations. We'll consider them here.

%---------------------------------------------------------------------------------------------------
\subsubsection{Symmetric-Antisymmetric Decomposition}
Any square matrix $\mathbf{M}$ can be decomposed into a sum of a symmetric matrix $\mathbf{S}$ and an antisymmetric $\mathbf{A}$ matrix as follows:
\begin{equation}
\mathbf{M} = \mathbf{S + A} \qquad \text{where}  \qquad
\mathbf{S} = (\mathbf{M} + \mathbf{M}^T) / 2,  \quad
\mathbf{A} = (\mathbf{M} - \mathbf{M}^T) / 2
\end{equation}
Note how this is analogous to decompose a function $f(x)$ into its symmetric part $f_s(x)$ and antisymmetric part $f_a(x)$ like $f(x) = f_s(x) + f_a(x)$ where $f_s(x) = (f(x) + f(-x)) / 2$ and $f_a(x) = (f(x) - f(-x)) / 2$. We used to call them "even" and "odd" parts of the function rather than "symmetric" and "antisymmetric" but it is the same idea. [TODO: explain what this decomp can be used for.]

% https://math.stackexchange.com/questions/2559915/decompose-matrix-a-into-its-symmetric-and-skew-symmetric-parts

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Splitting}
Of course, we can always split a matrix $\mathbf{A}$ additively into a sum $\mathbf{A} = \mathbf{P + R}$ where $\mathbf{P}$ could literally be \emph{any} matrix whatsoever. We would just have to choose $\mathbf{R} = \mathbf{A - P}$. We can interpret $\mathbf{P}$ as "prescribed" or - if it arises from taking a part out of $\mathbf{A}$ - as "partial". Then we can interpret $\mathbf{R}$ as the "residual" or "rest" or "remainder" - "full matrix equals part plus rest" sounds pretty reasonable. For example, we could choose $\mathbf{P}$ to be the upper triangular part of $\mathbf{A}$, i.e. matrix where we just zero out all elements of $\mathbf{A}$ below the main diagonal. Then the residual $\mathbf{R}$ would be lower triangular with zeros on the diagonal. In the context of iterative numerical algorithms, it is sometimes desirable to split a matrix in such a way. Sometimes, we may also split a matrix into more than two summands. Examples for the application of such splitting methods are Jacobi- and Gauss-Seidel iterations.

% P: prescirbed, partial

% Lower/Upper triangular - used as interation matrix in numerical algorithms like Gauss-Seidel.
% This can actually be generalized to any sort of $zero-out-component$ and $residual-component$
% I think they can be called splitting decompositions? see Numerik

% https://en.wikipedia.org/wiki/Matrix_splitting

%---------------------------------------------------------------------------------------------------
\subsubsection{Jordan-Chevalley (SN) Decomposition}
The Jordan-Chevalley (SN) decomposition decomposes a matrix as $\mathbf{A} =  \mathbf{S + N}$ where $\mathbf{S}$ is diagonalizable, $\mathbf{N}$ is nilpotent and $\mathbf{S}$ and $\mathbf{N}$ commute:
$\mathbf{SN} = \mathbf{NS}$. ...TBC...

% ToDo: explain application
% see: Mathe mit 2x2 Matrizen, pg 53
% https://en.wikipedia.org/wiki/Jordan%E2%80%93Chevalley_decomposition



\begin{comment}
	
Dear linear algebra students, This is what matrices (and matrix manipulation) really look like	
https://www.youtube.com/watch?v=4csuTO7UTMo	

Interpretation of matrices:

-in the context of solving linear systems of equations
 -each row gives a left-hand-side of one equation the system
 -each row defines a hyperplane, the solution of the system is the point where all
  hyperplanes intersect
 -each column represents a vector and we are looking for the coeffs to scale the 
  cols by to obtain agiven target vector on the RHS

-In the context of linear transformations:
 -the j-th column tells us where the j-th unit basis vector is mapped to
 
Maybe have sections for
-Product Decompositions


https://www.youtube.com/watch?v=nTwRjQ4xqUc  Five Factorizations of a Matrix
-has CR decomposition
	
\end{comment}