\section{Matrix Decomposition}



% https://en.wikipedia.org/wiki/Matrix_decomposition

%===================================================================================================
\subsection{LU Decomposition}
The LU-decomposition decomposes the matrix into a product $\mathbf{A = L U}$ where $\mathbf{L}$ is a lower triangular matrix and $\mathbf{U}$ is an upper triangular matrix. The LU-decomposition is algorithmically basically Gaussian elimination with some modifications. If you have an LU decomposition of a matrix $\mathbf{A}$ available, then it is straightforward to solve an equation $\mathbf{Ax = b}$. In practice, one often computes a decomposition of the form 

..TBC...explain details of algorithms for the decomposition and the solution of linear systems.
% Lower Triangular, Upper Triangular

% https://en.wikipedia.org/wiki/LU_decomposition

%===================================================================================================
\subsection{QR Decomposition}
% Orthonormal, Upper Triangular
% I think, the R stands for right-triangular?

%https://en.wikipedia.org/wiki/QR_decomposition

%===================================================================================================
\subsection{Eigendecomposition aka Diagonalization}
The most convenient decomposition of a matrix in terms of ease of solving equations involving the matrix and in terms of geometric interpretability of what the matrix actually does is the so called diagonalization of the matrix. ...TBC...

% it's also called Eigendecomposition or Spectral Decomposition


%===================================================================================================
\subsection{Non-Multiplicative Decompositions}
The matrix decompositions that we have seen so far decompose a given matrix $A$ into a product of other matrices with certain desirable features where "product" is understood as matrix multiplication. There are some more exotic decompositions which are not based on the matrix product but on other matrix operations. We'll consider them here.

%---------------------------------------------------------------------------------------------------
\subsubsection{Symmetric-Antisymmetric Decomposition}
Any square matrix $\mathbf{M}$ can be decomposed into a sum of a symmetric matrix $\mathbf{S}$ and an antisymmetric $\mathbf{A}$ matrix as follows:
\begin{equation}
\mathbf{M} = \mathbf{S + A} \qquad \text{where}  \qquad
\mathbf{S} = (\mathbf{M} + \mathbf{M}^T) / 2,  \quad
\mathbf{A} = (\mathbf{M} - \mathbf{M}^T) / 2
\end{equation}
[VERIFY]. Note how this is analogous to decompose a function $f(x)$ into its symmetric part $f_s(x)$ and antisymmetric part $f_a(x)$ like $f(x) = f_s(x) + f_a(x)$ where $f_s(x) = (f(x) + f(-x)) / 2$ and $f_a(x) = (f(x) - f(-x)) / 2$. We used to call them "even" and "odd" parts of the function back in the section about functions but it is the same idea.

% TODO: explain what this is good for.

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Splitting}
Of course, we can always split a matrix $\mathbf{A}$ additively into a sum $\mathbf{A} = \mathbf{P + R}$ where $\mathbf{P}$ could literally be \emph{any} matrix whatsoever. We would just have to choose $\mathbf{R} = \mathbf{A - P}$. We can interpret $\mathbf{P}$ as "prescribed" or - if it arises from taking a part out of $\mathbf{A}$ - as "partial". Then we can interpret $\mathbf{R}$ as the residual. For example, we could choose $\mathbf{P}$ to be the upper triangular part of $\mathbf{A}$, i.e. matrix where we just zero out all elements of $\mathbf{A}$ below the main diagonal. Then the residual $\mathbf{R}$ would be lower triangular with zeros on the diagonal. In the context of iterative numerical algorithms, it is sometimes desirable to split a matrix in such a way. Sometimes, we may also split a matrix into more than two summands. Examples for the application of such splitting methods are Jacobi- and Gauss-Seidel iterations.

% P: prescirbed, partial

% Lower/Upper triangular - used as interation matrix in numerical algorithms like Gauss-Seidel.
% This can actually be generalized to any sort of $zero-out-component$ and $residual-component$
% I think they can be called splitting decompositions? see Numerik

% https://en.wikipedia.org/wiki/Matrix_splitting

%---------------------------------------------------------------------------------------------------
\subsubsection{Jordan-Chevalley (SN) Decomposition}
The Jordan-Chevalley (SN) decomposition decomposes a matrix as $\mathbf{A} =  \mathbf{S + N}$ where $\mathbf{S}$ is diagonalizable, $\mathbf{N}$ is nilpotent and $\mathbf{S}$ and $\mathbf{N}$ commute:
$\mathbf{SN} = \mathbf{NS}$.

% ToDo: explain application
% see: Mathe mit 2x2 Matrizen, pg 53
% https://en.wikipedia.org/wiki/Jordan%E2%80%93Chevalley_decomposition


%---------------------------------------------------------------------------------------------------
\subsubsection{Kronecker Product}
% -mention approximation as Kronecker product
% -maybe htis is also knwon as tensor decomposition? figure out!


%---------------------------------------------------------------------------------------------------
\subsubsection{Block Matrices}
Matrices whose entries are themselves also matrices (and not numbers as usual) are called block matrices. This has not really anything to do with decomposing a given matrix into constituents, though ...TBC...




\begin{comment}
	
Dear linear algebra students, This is what matrices (and matrix manipulation) really look like	
https://www.youtube.com/watch?v=4csuTO7UTMo	

Interpretation of matrices:

-in the context of solving linear systems of equations
 -each row gives a left-hand-side of one equation the system
 -each row defines a hyperplane, the solution of the system is the point where all
  hyperplanes intersect
 -each column represents a vector and we are looking for the coeffs to scale the 
  cols by to obtain agiven target vector on the RHS

-In the context of linear transformations:
 -the j-th column tells us where the j-th unit basis vector is mapped to
 
Maybe have sections for
-Product Decompositions

	
\end{comment}