\section{Linear Systems of Equations}
One important area of application of matrices and vectors is the solution of a \emph{system of linear equations} aka \emph{linear system of equations} (which we shall abbreviate as LSE). Such linear systems are foundational to a lot of tasks in applied mathematics such as numerical simulations of physics, statistical analysis of data, optimization of processes and many more. 

%===================================================================================================
\subsection{Writing Down the System}
There are several ways to write down an LSE. In its original form, we have  $m$ equations in $n$ unknowns and in each equation (with index $i = 1,\ldots, m$), each unknown $x_j$ (where $j=1,\ldots,n$) gets multiplied by a coefficient $a_{ij}$ and that whole weighted sum gets equated to a given right hand side value $b_j$. It looks like this:
\begin{eqnarray}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &=& b_1    \\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &=& b_2    \\
                                       \vdots &\vdots& \vdots \\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &=& b_m 
\end{eqnarray}
The whole system can also be written down more compactly with sum notation like this:
\begin{equation}
\sum_{j=1}^n a_{ij} x_j = b_i \qquad \qquad i=1, \ldots, m
\end{equation}
The most common form is the matrix form:
\begin{equation}
\mathbf{A x} = \mathbf{b} \quad \text{where} \quad
\mathbf{A} = 
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn} 
\end{pmatrix}, \quad
\mathbf{x} = 
\begin{pmatrix}
x_{1} \\
x_{2} \\ 
\vdots\\
x_{n} 
\end{pmatrix}, \quad
\mathbf{b} = 
\begin{pmatrix}
b_{1} \\
b_{2} \\ 
\vdots\\
b_{n} 
\end{pmatrix}
\end{equation}
This is very convenient. The whole system is encapsulated in the short expression $\mathbf{A x} = \mathbf{b}$. It is also instructive to interpret the system as way to construct a right hand side vector $\mathbf{b}$ as a weighted sum of the columns of the matrix $\mathbf{A}$ where the $x_j$ are the weighting coefficients. We could write this down as:
\begin{equation}
x_1 \cdot \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{pmatrix} + 
x_2 \cdot \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2} \end{pmatrix} + 
\cdots +
x_n \cdot \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn} \end{pmatrix} 
= 
\begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}
\end{equation}
Yet another form to state an LSE is in the form the \emph{augmented coefficient matrix}. In this form, we write $(\mathbf{A|b})$ and by that we mean a matrix that is built from taking $\mathbf{A}$ and adding the vector $\mathbf{b}$ to it as an additional $(n+1)$-th column:
\begin{equation}
(\mathbf{A|b}) = ...
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} & \vline & b_1    \\
a_{21} & a_{22} & \ldots & a_{2n} & \vline & b_2    \\ 
\vdots & \vdots & \ddots & \vdots & \vline & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn} & \vline & b_m  
\end{pmatrix},
\end{equation}

% https://en.wikipedia.org/wiki/System_of_linear_equations#General_form
% https://en.wikipedia.org/wiki/Augmented_matrix
% https://de.wikipedia.org/wiki/Erweiterte_Matrix

%Due to the fact that this expression is stated in terms of vectors and matrices, the whole machinery of matrix decomposition becomes available 

%===================================================================================================
\subsection{Solvability the System}
Our main goal is to solve this system, i.e. to find one or more vectors $\mathbf{x}$ that we can plug into the equation $\mathbf{A x} = \mathbf{b}$ such that this equation actually holds true. the coefficient matrix $\mathbf{A}$ and the right hand side vector $\mathbf{b}$ are given. In general, there three possibilities: (1) No solution exists. (2) A unique solution exists. (3) A continuum of solutions exists. There are no other possibilities - a system with precisely two solutions, for example, cannot happen. If we are in case (3) where we have a continuum of solutions, that continuum can be one-dimensional, two-dimensional, etc. The mathematical theory that we want to develop will tell us, how we can recognize which of the 3 cases we are in and how to actually find the set of solutions.

%---------------------------------------------------------------------------------------------------
\subsubsection{The Critically Determined Case}
A first hint for which of the three situations we are dealing with is how the number of equations $m$ relates to the number of unknowns $n$. If the number of equations $m$ is equal to the number of unknowns $n$, such that $m=n$, we called this situation the \emph{critically determined case} because the number $m$ of constraints matches exactly the number $n$ of the degrees of freedom $x_1, \ldots, x_n$. In such a situation we may expect a unique solution. The idea is that each constraint reduces the number of degrees of freedom by one. After all, in such a case, the matrix $\mathbf{A}$ is an $n \times n$ matrix and we should be able to get our $\mathbf{x}$ via $\mathbf{x} = \mathbf{A}^{-1} \mathbf{b}$ and $\mathbf{A}^{-1} \mathbf{b}$ is unique vector, that this formula explicitly computes, right? This is indeed a good line of reasoning but there are some pitfalls in that. Foremostly, for this to work, the matrix $\mathbf{A}$ needs to be invertible. We already know that a matrix is invertible, iff its determinant is nonzero. So that's a criterion we could apply: iff $m = n$ and $\det(\mathbf{A}) \neq 0$, we will get a unique solution vector $\mathbf{x}$ which we could (theoretically) compute via $\mathbf{x} = \mathbf{A}^{-1} \mathbf{b}$. I say "theoretically" because that's a rather inefficient way to do it and in practice, we'll want to use a better algorithm - but at least it's one possible way. A square matrix with nonzero determinant is also called a \emph{regular} matrix.

\medskip
Now we want to look into the much more messy situation when $\det(\mathbf{A}) \neq 0$. 
In such a situation, the LSE can have either no solution at all or a continuum of solutions. Which of these two subcases we will find ourselves in will depend on the right hand side vector $\mathbf{b}$

% Take as 1st example a 2x2 matrix where the 2nd row is 1.5 times the first:
%   2  6
%   3  9
% Take some x like 3,1 to get
%   [2  6] * [3] = [12]
%   [3  9]   [1]   [18]
% But treat (3,1) as unknown (x1,x2). Show that for the rhs 12,18, we get a continuum of solutions
% and for some other vector that is not a multiple of the b, we get no solutins...is that true?


% Take as second example a 3x3 matrix where th 3rd row is obatined a 3*1st - 2*2nd, For example:
%   3  2  5
%   8  3  7 
%  -7  0  1


% some of the matrix rows may be redundant - if that is the case, it

...TBC...

%---------------------------------------------------------------------------------------------------
\subsubsection{The Underdetermined Case}
When we have less equations than unknowns, i.e. less constraints than degrees of freedom such that $m < n$, the we call this the \emph{underdetermined case}. We may expect a continuum of solutions, i.e. being able to choose $n-m$ parameters freely. Again, this line of reasoning is reasonable but there are again some pitfalls. ...TBC...

\paragraph{Minimum Norm Solution}
\begin{equation}
|\mathbf{x}|^2 = \mathbf{x}^T \mathbf{x} = \min 
\quad \text{subject to} \quad 
\mathbf{A x} = \mathbf{b}
\quad \Rightarrow \quad
L(\mathbf{x}, \boldsymbol{\lambda}) 
= \mathbf{x}^T \mathbf{x} + \boldsymbol{\lambda}^T (\mathbf{b} - \mathbf{A x}) = \min
\end{equation}

% give formulas for minimum norm solution

% give examples for when even an underdetermined system has no solution, like
%  1 x + 3 y + 2 z = 1
%  2 x + 6 y + 4 z = 2
%  3 x + 9 y + 6 z = 4   (if it would be 3, it would work)
% maybe take an example, where the linear dependency of the matrix rows is less obvious

% https://en.wikipedia.org/wiki/Underdetermined_system

% https://quickmathintuitions.org/intuition-for-overdetermined-and-underdetermined-systems-of-equations/

%---------------------------------------------------------------------------------------------------
\subsubsection{The Overdetermined Case}
When we have more equations than unknowns, i.e. more constraints than degrees of freedom, such that $m > n$, we call this the \emph{overdetermined case}. In such a case, we cannot generally expect an exact solution to exist. Again, there may be pitfalls for certain lucky "coincidences" of matrices and right hand sides but in general, we don't expect an exact solution.

\paragraph{Least Squares Approximation}
If an exact solution is not possible, the next best thing that we could look out for is an approximate solution. ...TBC...
\begin{equation}
|\mathbf{b} - \mathbf{A x}|^2 = (\mathbf{b} - \mathbf{A x})^T (\mathbf{b} - \mathbf{A x}) = \min
\quad \Rightarrow \quad
\boxed{ \mathbf{x} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbb{b} }
\end{equation}

% mention the pseudo-inverse, least squares solutions

% https://people.csail.mit.edu/bkph/articles/Pseudo_Inverse.pdf

% https://en.wikipedia.org/wiki/Overdetermined_system

%---------------------------------------------------------------------------------------------------
\subsubsection{The Homogeneous Solution}


%---------------------------------------------------------------------------------------------------
\subsubsection{The Particular Solution}


%---------------------------------------------------------------------------------------------------
\subsubsection{The General Solution}



% solvability, rank (may also be filed under matrix features - maybe introduce the concept here and mnetion it there again)

% solution structure: particular solution plus general solution of homogeneous system
% explain, why that structure arises
% A solutions of the homogenous system gives zero by definition, so adding any multiple it to a
% particular aolution does not destroy the solution property...or something
% This solution structure is really only relevant for (consistent) singular systems that have a 
% whole space of solutions. If the solution is unique, I think we get the special case where the
%% space spanned by the solution of the homogeneus system is 0-dimensional...or soemthing?

% b must be in the column space of A
% 3 posiibilities

% https://de.wikipedia.org/wiki/Satz_von_Kronecker-Capelli

%===================================================================================================
\subsection{An Algorithm for the Solution}

\begin{comment}

-"Algebra" is generally about solving equations. Questions liek: 
 -How many solutions are there?
 -How can we find them? Is there a systematic algorithm to produce the solutions?
 -Is there some structure to the set of solutions.
  -In case of linear algebra: the structure of the solution set of a linear system of equations is:
   x_g = x_p + x_h where: x_g is the general solution, y_p is a particular solution and y_h is the
   homogeneous solution. The latter is a subspace of the space we are seeking solutions in that is 
   given by the solution of the corresponding
   

\end{comment}