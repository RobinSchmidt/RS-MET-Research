\section{Linear Systems of Equations}
One important area of application of matrices and vectors is the solution of a \emph{system of linear equations} aka \emph{linear system of equations} (which we shall abbreviate as LSE). Such linear systems are foundational to a lot of tasks in applied mathematics such as numerical simulations of physics, statistical analysis of data, optimization of processes and many more. 

%===================================================================================================
\subsection{Writing Down the System}
There are several ways to write down an LSE. In its original form, we have  $m$ equations in $n$ unknowns and in each equation (with index $i = 1,\ldots, m$), each unknown $x_j$ (where $j=1,\ldots,n$) gets multiplied by a coefficient $a_{ij}$ and that whole weighted sum gets equated to a given right hand side value $b_j$. It looks like this:
\begin{eqnarray}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &=& b_1    \\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &=& b_2    \\
                                       \vdots &\vdots& \vdots \\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &=& b_m 
\end{eqnarray}
The whole system can also be written down more compactly with sum notation like this:
\begin{equation}
\sum_{j=1}^n a_{ij} x_j = b_i \qquad \qquad i=1, \ldots, m
\end{equation}
The most common form is the matrix form:
\begin{equation}
\mathbf{A x} = \mathbf{b} \quad \text{where} \quad
\mathbf{A} = 
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn} 
\end{pmatrix}, \quad
\mathbf{x} = 
\begin{pmatrix}
x_{1} \\
x_{2} \\ 
\vdots\\
x_{n} 
\end{pmatrix}, \quad
\mathbf{b} = 
\begin{pmatrix}
b_{1} \\
b_{2} \\ 
\vdots\\
b_{n} 
\end{pmatrix}
\end{equation}
This is very convenient. The whole system is encapsulated in the short expression $\mathbf{A x} = \mathbf{b}$. It is also instructive to interpret the system as way to construct a right hand side vector $\mathbf{b}$ as a weighted sum of the columns of the matrix $\mathbf{A}$ where the $x_j$ are the weighting coefficients. We could write this down as:
\begin{equation}
x_1 \cdot \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{pmatrix} + 
x_2 \cdot \begin{pmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2} \end{pmatrix} + 
\cdots +
x_n \cdot \begin{pmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn} \end{pmatrix} 
= 
\begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}
\end{equation}
% It is instructive because it tells us what we are trying to achieve geometrcially: we try to build the vector b from the columns of A
% Leupold has 2 other ways to write it down
% $x_1 \mathbf{a}_1 + x_2 \mathbf{a}_2 + \ldots + x_n \mathbf{a}_n  = \mathbf{b} $
% and
% $(\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_n) \mathbf{x} = \mathbf{b}$

Yet another form to state an LSE is in the form the \emph{augmented coefficient matrix}. In this form, we write $(\mathbf{A|b})$ and by that we mean a matrix that is built from taking $\mathbf{A}$ and adding the vector $\mathbf{b}$ to it as an additional $(n+1)$-th column:
\begin{equation}
(\mathbf{A|b}) = ...
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} & \vline & b_1    \\
a_{21} & a_{22} & \ldots & a_{2n} & \vline & b_2    \\ 
\vdots & \vdots & \ddots & \vdots & \vline & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn} & \vline & b_m  
\end{pmatrix},
\end{equation}

% https://en.wikipedia.org/wiki/System_of_linear_equations#General_form
% https://en.wikipedia.org/wiki/Augmented_matrix
% https://de.wikipedia.org/wiki/Erweiterte_Matrix

%Due to the fact that this expression is stated in terms of vectors and matrices, the whole machinery of matrix decomposition becomes available 



%===================================================================================================
\subsection{Solvability the System}
Our main goal is to solve this system, i.e. to find one or more vectors $\mathbf{x}$ that we can plug into the equation $\mathbf{A x} = \mathbf{b}$ such that this equation actually holds true. the coefficient matrix $\mathbf{A}$ and the right hand side vector $\mathbf{b}$ are given. In general, there three possibilities: (1) No solution exists. (2) A unique solution exists. (3) Infinitely many solutions exists. There are no other possibilities - a system with precisely two solutions, for example, cannot happen. If we assume assume that we work in a continuous number system, i.e. the $x_j$ are continuous numbers (e.g. real or complex), then this infinity will also be of the continuous kind (uncountable). If we are in case (3) where we have a continuum of solutions, that continuum can be one-dimensional, two-dimensional, etc. The mathematical theory that we want to develop will tell us, how we can recognize which of the 3 cases we are in and how to actually find the set of solutions.

% b must be in the span of the columns of A. If A has full rank, then that span is the whole space, so it should be possible to find a solution

%---------------------------------------------------------------------------------------------------
\subsubsection{The Critically Determined Case}
A first hint for which of the three situations we are dealing with is how the number of equations $m$ relates to the number of unknowns $n$. If the number of equations $m$ is equal to the number of unknowns $n$, such that $m=n$, we called this situation the \emph{critically determined case} because the number $m$ of constraints matches exactly the number $n$ of the degrees of freedom $x_1, \ldots, x_n$. In such a situation we may expect a unique solution. The idea is that each constraint reduces the number of degrees of freedom by one. After all, in such a case, the matrix $\mathbf{A}$ is an $n \times n$ matrix and we should be able to get our $\mathbf{x}$ via $\mathbf{x} = \mathbf{A}^{-1} \mathbf{b}$ and $\mathbf{A}^{-1} \mathbf{b}$ is unique vector, that this formula explicitly computes, right? This is indeed a good line of reasoning but there are some pitfalls in that. Foremostly, for this to work, the matrix $\mathbf{A}$ needs to be invertible. We already know that a matrix is invertible, iff its determinant is nonzero. So that's a criterion we could apply: iff $m = n$ and $\det(\mathbf{A}) \neq 0$, we will get a unique solution vector $\mathbf{x}$ which we could (theoretically) compute via $\mathbf{x} = \mathbf{A}^{-1} \mathbf{b}$. I say "theoretically" because that's a rather inefficient way to do it and in practice, we'll want to use a better algorithm - but at least it's one possible way. A square matrix with nonzero determinant is also called a \emph{regular} matrix.

\paragraph{The Significance of the Rank}
Now we want to look into the more messy situation when $\det(\mathbf{A}) \neq 0$. In such a situation, the LSE can have either no solution at all or a continuum of solutions. Which of these two subcases we will find ourselves in will depend on the right hand side vector $\mathbf{b}$. An even more informative number than the determinant (which gives us only a binary yes/no information) is the rank $r$ of the matrix $\mathbf{A}$ and how that rank compares to the rank of the augmented matrix $(\mathbf{A|b})$. ...TBC...explain rank deficiency

% Take as 1st example a 2x2 matrix where the 2nd row is 1.5 times the first:
%   2  6
%   3  9
% Take some x like 3,1 to get
%   [2  6] * [3] = [12]
%   [3  9]   [1]   [18]
% But treat (3,1) as unknown (x1,x2). Show that for the rhs 12,18, we get a continuum of solutions
% and for some other vector that is not a multiple of the b, we get no solutins...is that true?


% Take as second example a 3x3 matrix where th 3rd row is obatined a 3*1st - 2*2nd, For example:
%   3  2  5
%   8  3  7 
%  -7  0  1


% some of the matrix rows may be redundant - if that is the case, it



%---------------------------------------------------------------------------------------------------
\subsubsection{The Underdetermined Case}
When we have less equations than unknowns, i.e. less constraints than degrees of freedom such that $m < n$, the we call this the \emph{underdetermined case}. We may expect a continuum of solutions, i.e. being able to choose $n-m$ parameters freely. Again, this line of reasoning is reasonable but there are again some pitfalls. ...TBC...

\paragraph{Minimum Norm Solution}
In a practical situation, we may want to pick one of the solutions from this infinite space of solutions. One possible criterion for making this choice is to say that we want to pick that particular solution vector $\mathbf{x}$ with the smallest possible norm, i.e. we want to pick the shortest possible solution vector. This type of problem is a called a \emph{constrained optimization problem} which we will treat later in the chapter on multivariable calculus. Here, I will just say that we minimize $|\mathbf{x}|^2$ subject to the constraint $\mathbf{A x} = \mathbf{b}$. This results in the following Lagrange function $L$ (which you may ignore if you don't know what that means):
\begin{equation}
|\mathbf{x}|^2 = \mathbf{x}^T \mathbf{x} = \min 
\quad \text{subject to} \quad 
\mathbf{A x} = \mathbf{b}
\quad \Rightarrow \quad
L(\mathbf{x}, \boldsymbol{\lambda}) 
= \mathbf{x}^T \mathbf{x} + \boldsymbol{\lambda}^T (\mathbf{b} - \mathbf{A x}) = \min
\end{equation}
Forming the derivative of the Lagrange function $L$ with respect to $\mathbf{x}$ and $\boldsymbol{\lambda}$ and equating it to zero leads to the following solution:
\begin{equation}
\boldsymbol{\lambda} = 2 (\mathbf{A} \mathbf{A}^T)^{-1} \mathbf{b}
\qquad
\boxed{\mathbf{x} = \mathbf{A}^T (\mathbf{A} \mathbf{A}^T)^{-1} \mathbf{b}}
\end{equation}
where for us, only the result for $\mathbf{x}$ is relevant. The $\boldsymbol{\lambda}$-vector is a vector of auxiliary variables, called Lagrange multipliers, that we may throw away. A derivation for this formula for the underdetermined case (along with the derivation for and overdetermined case which we will treat next) is given in \cite{LinSysOverUnder}.

\paragraph{The Space of Solutions}
We may envision the space of solutions as a shifted subspace of $\mathbf{R}^n$ [VERIFY]. In 3D, this could be a plane or a line. This plane or line will typically not pass through the origin [VERIFY].

 ...TBC...

% give formulas for minimum norm solution

% give examples for when even an underdetermined system has no solution, like
%  1 x + 3 y + 2 z = 1
%  2 x + 6 y + 4 z = 2
%  3 x + 9 y + 6 z = 4   (if it would be 3, it would work)
% maybe take an example, where the linear dependency of the matrix rows is less obvious

% https://en.wikipedia.org/wiki/Underdetermined_system

% https://quickmathintuitions.org/intuition-for-overdetermined-and-underdetermined-systems-of-equations/

%---------------------------------------------------------------------------------------------------
\subsubsection{The Overdetermined Case}
When we have more equations than unknowns, i.e. more constraints than degrees of freedom, such that $m > n$, we call this the \emph{overdetermined case}. In such a case, we cannot generally expect an exact solution to exist. Again, there may be pitfalls for certain lucky "coincidences" of matrices and right hand sides but in general, we don't expect an exact solution.



\paragraph{Least Squares Approximation}
If an exact solution is not possible, the next best thing that we could look out for is an approximate solution. If the quality of the approximation is measured by the Euclidean length of the "error" vector $\mathbf{b} - \mathbf{A x}$, we will arrive at the following minimization problem:
\begin{equation}
|\mathbf{b} - \mathbf{A x}|^2 = (\mathbf{b} - \mathbf{A x})^T (\mathbf{b} - \mathbf{A x}) = \min
\quad \Rightarrow \quad
\boxed{ \mathbf{x} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b} }
\end{equation}

\paragraph{Geometric Intuition}

% Explain geoemtric intuition: The components x_i of the solution vector should serve as multipliers for the columns of matrix A. We try to find these weights in such a way as to match the right hand side vector. But when this vector is outside the span of the columns of A, there is no way we could match it eaxctly. 

% Maybe call this paragraph The Column Space of A


% In practice, one would define z = A^T b and solve A^T A x = z
% look it up in the codebase...or better: give some python or sage code

% mention the pseudo-inverse, least squares solutions

% https://people.csail.mit.edu/bkph/articles/Pseudo_Inverse.pdf

% https://en.wikipedia.org/wiki/Overdetermined_system

%---------------------------------------------------------------------------------------------------
\subsubsection{The Ostensibly Critically Determined Case}
When the number of equations matches the number of unknowns, we may tend to assume that the system is critically determined and we expect a unique solution. But what if the matrix is rank deficient? [TODO: explain what can happen - ] %In such a case, one or more of the equations are actually redundant. At least one of the rows on the left hand sides can be obtained as linear combination of other rows. If the same linear combination can be applied to the right hand side and the result matches the actual given value on the right hand side of that row, the system is consistent and therfore soluble but one equation is redundant so we can scrap it and land in the underdetermined case. If the rhs value doesn't match, the system is inconsistsent
% I think, if the rhs is consistent, we end up in the underdetermined case and if the rhs is inconsistent in the overdetermined case


%---------------------------------------------------------------------------------------------------
\subsubsection{The Homogeneous Solution}
Recall the definition of homogeneity from the beginning of the chapter: A function $f(x)$ is said to be homogeneous if scaling the input by some constant factor $k$ has the same effect as scaling the output by the same factor: $f(k x) = k f(x)$. The function we are dealing with here is one that takes a vector $\mathbf{x}$ as input and produces the output vector $\mathbf{A x}$ and that function is indeed homogeneous: $\mathbf{A} (k \mathbf{x}) = k \mathbf{A x}$. Now let's assume we want to solve the equation $\mathbf{A x = 0}$. This is a special case of the more general case of $\mathbf{A x = b}$. We call this special case a \emph{homogeneous equation}\footnote{I'm not entirely sure about the etymology. I've never seen an explanation for this terminology but it seems plausible to me that it is called "homogeneous equation" because scaling a solution vector by some number does not change the fact that it is a solution. And the reason for that is the homogeneity of the left hand side. And the argument by homogeneity works only if $\mathbf{b=0}$ because only in this special case we have $k \mathbf{b = b}.$}. Assume that $\mathbf{h}$ is a solution to this homogeneous equation, i.e. we assume that $\mathbf{A h} = \mathbf{0}$. It follows immediately by homogeneity that any scalar multiple $k \mathbf{h}$ of $\mathbf{h}$ is also a solution of that equation because  $\mathbf{A} (k \mathbf{h}) = k \mathbf{A h} = k \mathbf{0} = \mathbf{0}$. That means that if we know any solution $\mathbf{h}$ to $\mathbf{A h = 0}$, then we immediately have at least a whole one dimensional continuum of solutions because we can choose any real number $k$ to get another solution. The only catch to this statement is that our solution $\mathbf{h}$ should not be the zero vector, i.e. we should assume that $\mathbf{h} \neq \mathbf{0}$. If $\mathbf{h}$ is the zero vector, then scaling it by anything will not give a new solution - zero times anything is still zero - so in this case, we don't really get a continuum of solutions by varying $k$. The zero vector is actually always a solution to the homogeneous equation because $\mathbf{A 0} = \mathbf{0}$ is always true. We therefore call $\mathbf{h = 0}$ the \emph{trivial solution} to the homogeneous system and we are really only interested in the nontrivial solutions. [TODO: explain how the rank of the matrix determines the dimension of the solution space]
% This solution may define a whole subsapce

% https://math.ryerson.ca/~danziger/professor/MTH141/Handouts/homogeneous.pdf
% https://www.cuemath.com/calculus/homogeneous-differential-equation/

%---------------------------------------------------------------------------------------------------
\subsubsection{The Particular Solution}
We have seen that in the underdetermined case ()


%---------------------------------------------------------------------------------------------------
\subsubsection{The General Solution}
If $\mathbf{h}$ is a solution to the homogeneous system $\mathbf{A h = 0}$ (1st assumption) and $\mathbf{p}$ is a particular solution to the inhomogeneous system $\mathbf{A p = b}$ (2nd assumption), then for any scalar $k$, the vector $\mathbf{x} = \mathbf{p} + k \mathbf{h}$ is automatically also a solution to the inhomogeneous equation. This consequence is easy to see by writing $\mathbf{A} (\mathbf{p} + k \mathbf{h}) = \mathbf{b} = \mathbf{A p} + k \mathbf{A h}$. Now we note that $k \mathbf{A h}$ is zero because $\mathbf{A h = 0}$ by our first assumption and the only nonzero thing that remains is $\mathbf{A p}$ which equals $\mathbf{b}$ by our second assumption. That means that a general solution vector $\mathbf{x}$ of an inhomogeneous system of the form $\mathbf{A x = b}$ can always be written as a sum of a particular solution $\mathbf{p}$ plus some element from space of solutions to the homogeneous system. [VERIFY, TODO: explain geometric intuition of the solution set as shifted subspace]

% If h is a solution to the homogeneous system A h = 0 and p is a particular solution to A p = b then for any scalar k, x = p + k h is also a soution to the inhomogeneous equation. this is easy to see by writing A (p + k h) = b = A p + k A h where k A h is zero because A h = 0


% solvability, rank (may also be filed under matrix features - maybe introduce the concept here and mnetion it there again)

% solution structure: particular solution plus general solution of homogeneous system
% explain, why that structure arises
% A solutions of the homogenous system gives zero by definition, so adding any multiple it to a
% particular aolution does not destroy the solution property...or something
% This solution structure is really only relevant for (consistent) singular systems that have a 
% whole space of solutions. If the solution is unique, I think we get the special case where the
%% space spanned by the solution of the homogeneus system is 0-dimensional...or soemthing?

% b must be in the column space of A
% 3 posiibilities

% https://de.wikipedia.org/wiki/Satz_von_Kronecker-Capelli

%===================================================================================================
\subsection{An Algorithm for the Solution}

\begin{comment}

-"Algebra" is generally about solving equations. Questions liek: 
 -How many solutions are there?
 -How can we find them? Is there a systematic algorithm to produce the solutions?
 -Is there some structure to the set of solutions.
  -In case of linear algebra: the structure of the solution set of a linear system of equations is:
   x_g = x_p + x_h where: x_g is the general solution, y_p is a particular solution and y_h is the
   homogeneous solution. The latter is a subspace of the space we are seeking solutions in that is 
   given by the solution of the corresponding
   

\end{comment}