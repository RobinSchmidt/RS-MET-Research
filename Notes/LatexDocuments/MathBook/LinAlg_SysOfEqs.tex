\section{Linear Systems of Equations}
One important area of application of matrices and vectors is the solution of systems of linear equations.

% solvability, rank (may also be filed under matrix features - maybe introduce the concept here and mnetion it there again)

% solution structure: particular solution plus general solution of homogeneous system
% explain, why that structure arises
% A solutions of the homogenous system gives zero by definition, so adding any multiple it to a
% particular aolution does not destroy the solution property...or something
% This solution structure is really only relevant for (consistent) singular systems that have a 
% whole space of solutions. If the solution is unique, I think we get the special case where the
%% space spanned by the solution of the homogeneus system is 0-dimensional...or soemthing?

\subsection{Writing Down The System}
There are several ways to write down a linear system of equations. In its original form, we have  $m$ equations in $n$ unknowns and in each equation (with index $i = 1,\ldots, m$), each unknown $x_j$ (where $j=1,\ldots,n$) gets multiplied by a coefficient $a_{ij}$ and that whole weighted sum gets equated to a given right hand side value $b_j$. It looks like this:
\begin{eqnarray}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &=& b_1    \\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &=& b_2    \\
                                       \vdots &\vdots& \vdots \\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &=& b_m 
\end{eqnarray}
The whole system can also be written down more compactly like this:
\begin{equation}
\sum_{j=1}^n a_{ij} x_j = b_i \qquad \qquad i=1, \ldots, m
\end{equation}
The most common form is the matrix form:
\begin{equation}
\mathbf{A x} = \mathbf{b} \quad \text{where} \quad
\mathbf{A} = 
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn} 
\end{pmatrix}, \quad
\mathbf{x} = 
\begin{pmatrix}
x_{1} \\
x_{2} \\ 
\vdots\\
x_{n} 
\end{pmatrix}, \quad
\mathbf{b} = 
\begin{pmatrix}
b_{1} \\
b_{2} \\ 
\vdots\\
b_{n} 
\end{pmatrix}
\end{equation}
This is very convenient. The whole system is encapsulated in the short expression $\mathbf{A x} = \mathbf{b}$. It is also instructive to interpret the system as way to construct a right hand side vector $\mathbf{b}$ as a weighted sum of the columns of the matrix $\mathbf{A}$ where the $x_j$ are the weighting coefficients. We could write this down as:




%Due to the fact that this expression is stated in terms of vectors and matrices, the whole machinery of matrix decomposition becomes available 





\begin{comment}

-"Algebra" is generally about solving equations. Questions liek: 
 -How many solutions are there?
 -How can we find them? Is there a systematic algorithm to produce the solutions?
 -Is there some structure to the set of solutions.
  -In case of linear algebra: the structure of the solution set of a linear system of equations is:
   x_g = x_p + x_h where: x_g is the general solution, y_p is a particular solution and y_h is the
   homogeneous solution. The latter is a subspace of the space we are seeking solutions in that is 
   given by the solution of the corresponding
   

\end{comment}