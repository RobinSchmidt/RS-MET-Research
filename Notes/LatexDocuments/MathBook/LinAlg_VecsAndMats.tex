\section{Vectors and Matrices}

\paragraph{Vectors}
Consider a point in the 2D plane or in 3D space. To represent such a point mathematically, we would need a pair or a triple of numbers respectively. An $n$-dimensional vector can generally be thought of as an $n$-tuple of numbers. For intuition building, it's best to think of real numbers although in general, other kinds of numbers may also be allowed in the more general case. Such a vector can not only represent a position in space but also a translation. Imagine the $xy$-plane. The vector $(x,y) = (3,2)$ could encode the action of first going $3$ units into the $x$-direction (usually to the right) and then going $2$ units into the $y$-direction (usually upward). Of course, we could as well go first the $2$ units into the $y$-direction and the $3$ units into the $x$-direction. The location we would end up would be the same in both cases - namely the point with coordinates $(3,2)$. So, we have two interpretations of the tuple $(3,2)$: (1) The point with coordinates $(3,2)$. We could visualize this by drawing a fat dot at this position. (2) The action of displacing something by three units to the right and two units up. We could visualize this as an arrow pointing from the origin to $(3,2)$. If we apply the "action" interpretation to the origin $(0,0)$, we'll end up at $(3,2)$. That is, if we "apply" the arrow to the origin, we'll arrive at our fat dot. Some authors make a careful distinction between the first and second interpretation and use the term "point" or "position vector" for the first and the term "arrow" or "vector" or "displacement vector" for the second. We'll just use the term vector for everything because we don't care about the interpretation - this is math, after all. Vectors are usually denoted as columns. For example, a vector $\mathbf{v}$ with given $x$ and $y$ values is written as:
\begin{equation}
\mathbf{v} 
= \begin{pmatrix} x \\ y \end{pmatrix} 
=   x \cdot \begin{pmatrix} 1 \\ 0 \end{pmatrix} 
  + y \cdot \begin{pmatrix} 0 \\ 1 \end{pmatrix} 
\qquad \text{or}  \qquad
\mathbf{v} = (x,y) = x \cdot (1, 0) + y \cdot (0, 1)
\end{equation}
where the right hand sides can be seen as an interpretation of what the tuple $(x,y)$ actually encodes. We imagine $x$ to be a scaling factor (aka coefficient) of a unit vector $(1,0)$ into the $x$-direction and $y$ to be a coefficient for a unit vector $(0,1)$ into the $y$-direction. The left and right versions are just different notations. The right notation using tuples is more convenient when embedding an equation in a text or when vertical space shall be saved. In non-embedded equations, most books will usually use the left notation where vectors are written as columns with the components stacked vertically. 

% Some authors distinguish carefully between points in the plane and the arrows that start at zero and have their tip at the respective point

%You may also (rarely) see the notation $\mathbf{v} = (x \;\, y)^T$, i.e. like tuple notation but without the comma and a superscript $T$. This $T$ stands for \emph{transposition} which, in this case, means to turn the row into a column.

% Explain notation: when the components are separated by a comma, we see the vectors as tuples. Without commas, we could write them as $(1 0)^T$ where the superscript $T$ in $(\ldots)^T$ stands for \emph{transposition} which, in this case, means to turn the row into a column.

...TBC...

% Explain the unit basis vectors and the zero vector

% Explain how the sum is "formal" in an algebraic sense but real in a geometric sense. Or ...well...it is actually also real in an algebraic sense - we can collapse it into a single vector, namely the vector (x,y). Writing it as such a sum can actually be interpreted as decomposition into a purely horizontal and a purely vertical component. These components, when added together in a sense that is soon to be defined, give back the original vector. 

%A vector may not only represent a point itself, but also a translation ...tbc...
% position vector vs translation/displacement vector
% Ortsvektor vs Richtungsvektor
% https://byjus.com/physics/position-and-displacement-vectors/


% Geometrische Interpretation linearer Abbildungen
% https://www.youtube.com/watch?v=EQ5Xct2YyLk

\paragraph{Matrices}
Vectors can be used to represent locations and displacements aka translations. Translations are a specific kind of geometric transformation. A matrix represents a different kind of transformation, namely a \emph{linear transformation}, that we can apply to a vector. In geometric terms, linear transformations are scalings, rotations and shears. In this point of view, the columns of the matrices tell us, where the standard basis vectors will get mapped to (we'll see later what these are).

...TBC...


% A matrix is a 2D array of numbers, i.e. a table with rows and columns. Each cell contains a number pretty much like in a spreadsheet.
% the columns of the matrix say where the basis vectors go

% We need to define what the identity matrix is. Maybe the zero matrix, too

% Translations are not among the linear transformations that we can represent by matrices.


\paragraph{Well, actually...}
Strictly speaking, an $n$-tuple of numbers is not what a vector really \emph{is} by its nature. By its nature, a vector is a geometric entity such as a point in space or an arrow with a direction and length. The $n$-tuple of numbers is a specific \emph{representation} of that geometric entity that depends on the coordinate system that we have chosen. But you may take that statement as a foreshadowing to a more advanced viewpoint. For the purposes of this section, it's totally okay to picture a vector as a tuple of numbers. Likewise, matrices are also only a specific representation of a linear transformation. In a different coordinate system, the same transformation would be represented by a different 2D array of numbers. We will say more about this in the context of so called \emph{change of basis} transformations.

\medskip
Moreover, when we look at things more carefully, we may need to distinguish between \emph{points} and \emph{vectors}, after all - which I just said, we don't. A point represents just a location and it does not really make any sense to add a location to another location. What is Berlin plus Paris supposed to mean? It makes no sense - even when expressed as geolocations! (By the way: I'm conveniently assuming a flat map here. Taking the spherical earth into account is a further complication which is of no relevance to my point). A vector, on the other hand, represents an arrow which we interpret as a translation. It does indeed make sense to add translations: we just perform one after the other. We may first go 30 km east and 20 km north and thereafter go 40 km east and 50 km north - that makes perfect sense. So we can add translations to one another. We can actually even add translations to points - that makes sense as well. The result would be another point. What could also make sense is to take the difference of two points and interpret that as a displacement vector that tells us, how we would need to move to get from the first point to the other. However - in practice (like in vector graphics APIs), points are usually identified with their \emph{position vectors} (aka location vector or radius vector), i.e. vectors that point from the origin to the given point. Represented as such, we actually can add points in our graphics code - we just should ask ourselves, if it makes any sense to do so in our particular case and what the result represents. A graphics API could in principle provide different data types for points and vectors and allow only those operations that make sense - but they usually don't and express everything uniformly by vectors and expect the user to know, what they are doing. Don't let my ramblings distract you too much!
% ...TBC...
% the difference between two locations, on the other hand, does make sense. It would be a displacement vector

% https://en.wikipedia.org/wiki/Position_(geometry)

%===================================================================================================
\subsection{Vector Operations}

\paragraph{Scalar Multiplication}
Scalar multiplication refers to the act of multiplying a vector $\mathbf{v}$ by a scalar, i.e. a number, let's say $a$. This is just done by multiplying every component of the vector by that number. Geometrically, this scales the length of the vector by a factor of $a$. That's where the name "scalar" comes from. You may interpret it as "scaler". A thing that "scales". If $a > 1$, the vector just gets longer. If $a < 1$ but still $a > 0$, the vector gets shorter but retains its direction. If $a < 0$, the vector reverses direction in addition to being lengthened or shortened (in the special case of $a=-1$, it gets only reflected). If $a = 0$, the vector actually gets collapsed into the zero vector $\mathbf{0}$.

\paragraph{Vector Addition}
Algebraically, there is not much to say about vector addition - we just add the tuples component-wise and that's it. We can interpret such a vector addition geometrically at placing the arrows that represent the vectors tip to tail. ...TBC... [ToDo: insert figure with the parallelogram picturing $\mathbf{a + b}$ and $\mathbf{b + a}$].

% What about when dimensionalities don't match? Maybe we can zero-pad the shorter vector in certain
% cases. Geometrically, this would mean to iamgine the low-dimensional vector spcae as being 
% embedded into a higher dimensional space by setting the extra coordinates in the higher
% dimensional space to zero. For example, a 2D vector (x,y) could be augmented to a 3D vector
% (x,y,z) by just setting $z=0$. Check how graphics libraries like OpenGL handle that.
% 

\paragraph{Vector Subtraction}
Algebraically, vector subtraction is also simple - just do it component-wise. Geometrically, when we subtract a vector $\mathbf{b}$ from another vector $\mathbf{a}$ such that $\mathbf{c = b-a}$, then the arrow that represents the difference vector $\mathbf{c}$ could be visualized as pointing from the tip of $\mathbf{b}$ to the tip of $\mathbf{a}$. But then it wouldn't have its tail at the origin anymore. But that's no problem - we can just imagine to slide it back such that its tail again coincides with the origin ...TBC...explain that better, draw a figure
% What about subtraction? Algebraically, it's just addition of the negative but geometrically, it
% has a meaningful interpretation, so maybe it should get is won section

% maybe generalize to "Linear Combinations"

% explain that vectors can be moved around freely in the plane

\paragraph{Linear Combinations}
A linear combination of a bunch of vectors is just a weighted sum of those vectors. That means each vector gets multiplied by a weight via our scalar multiplication and then these scaled vectors are added up via our vector addition. Vector subtraction is also just a special case of a linear combination of two vectors where one vector gets a weight of $+1$ and the other a weight of $-1$. 



\paragraph{Inner Product}
The \emph{inner product}, also known as the \emph{scalar product} or \emph{dot product} between two vectors $\mathbf{v}$ and $\mathbf{w}$ does not give another vector. Instead, the result is just a number, i.e.a scalar. It is computed by taking all the component-wise products and summing them all up. That is the algebraic definition and the calculation recipe. The inner product can also be defined geometrically as follows: it is the product of the lengths of the two vectors times the cosine of the angle between them. In 3D with $\mathbf{v} = (v_x, v_y, v_z)$ and $\mathbf{w} = (w_x, w_y, w_z)$ we have:
\begin{equation}
 \mathbf{v} \cdot \mathbf{w} 
 = v_x w_x + v_y w_y + v_z w_z 
 = |\mathbf{v}| |\mathbf{w}| \cos( \angle(\mathbf{v}, \mathbf{w}) )
\end{equation}
where on the right we have used two notations that we have yet to explain: By $|\mathbf{v}|$ we denote the length or so called \emph{norm} of the vector $\mathbf{v}$. We can compute it simply by taking the square root of the dot product of the vector $\mathbf{v}$ with itself. By $\angle(\mathbf{v}, \mathbf{w})$ we mean the angle between the two vectors $\mathbf{v}$ and $\mathbf{w}$. To compute this angle, we can just take the arc-cosine of the dot product of the two normalized vectors $\mathbf{\hat{v}} = \mathbf{v} / |\mathbf{v}|$ and  $\mathbf{\hat{w}} = \mathbf{w} / |\mathbf{w}|$. So we have:
\begin{equation}
|\mathbf{v}| = \sqrt{ \mathbf{v} \cdot \mathbf{v}}, 
\qquad 
\angle(\mathbf{v}, \mathbf{w}) = 
 \arccos \left( \frac{\mathbf{v} \cdot \mathbf{w}} {|\mathbf{v}|  |\mathbf{w}|} \right)
\end{equation}
If this definition appears circular, remember that we can always use the purely algebraic definition to compute the dot product which does not yet reference the norm or angle. We see that geometrically, the dot product gives us two things: lengths and angles. Using a hat above a vector is a common notation for \emph{normalized} vectors, i.e. vectors with a norm of one. Dividing a vector by its own norm does indeed yield a vector with unit norm. It has the same direction as the original vector, of course - because scaling all components by the same amount doesn't change the direction of a vector.

 ...TBC...
% explain geometric interpretations
% -coefficient of projection
% -correlation in the sense of: how similar is the direction of the vectors (when both are
%  normalized)
% Mention different (more general) definitions: 
% -with complex conjugation,
% -with a (positive definite) matrix sadwiched in between
% -explain generalization to comlex vectors. I think the left factor is conjugated. Or is it the
%  right one? Does it matter? Maybe not with complex numbers but maybe with further generalizations
%  to vectors of quaternions or other kinds of non-commutative numbers.
% -Maybe the discussion of the geometric meaning could be deferred to the section about analytic
%  geometry. but we can actually mention it here already - it doesn't hurt to be a bit repetitive
% -the inner product is commutative (at least for real-valued vectors) and distributive over vector
%  addition and subtraction. It does *not* satisfy the rule that if the product is zero then at least
%  one of the factors has to be zero.
% -mention the different notations (dot, v^T w, <v,w>, (v,w))

%\paragraph{Vector Products}
% cross-product, triple-product (but this gives a scalar), wedge-product

% Lengths, angles, projection, correlation | Linear algebra episode 2
% https://www.youtube.com/watch?v=mCi_0ML0lG8

%\paragraph{Distance}

\paragraph{Orthogonal Projection}
% -explain how the dot-product being zero means that the vectors are orthogonal and how we can
%  measure to how much amount two vectors point into the same direction by way of the dot product
%   ...interpreted as correlation or angle

\paragraph{Vector Decomposition}
% We have already seen how a vector $(x,y)$ can be decomposed into a purely horizontal and a purely vertical component. That decomposition can be generalized.
% we project the vector onto one other vector to obtain it's component
% We often choose the vectors into which we decompose a given vector to be orthogonal but that doesn't need to be the case. ...but I think, the general decomposition cannot be obtained by simple projections. I think, we really need to solve a linear system of equations for this.

% https://ccrma.stanford.edu/~jos/st/Projection_onto_Non_Orthogonal_Vectors.html

% https://math.stackexchange.com/questions/1882096/how-to-decompose-a-vector-into-non-orthogonal-components


\paragraph{Outer Product}

\paragraph{Cross Product}
% -give algebraic and geometric definitions (geometric: sine of the angle, perpedicular to 
%  the plane spanned etc.)

% How do we generalize the cross product to other dimensions?
% https://www.youtube.com/watch?v=MaWJsZEPg-c

\paragraph{Wedge Product}
% does not really belong here because it results in a bivector

\paragraph{Triple Product}






% Norm, Normalization

% Outer product (is actually a special case tensor product or Kronecker product)

% Inversion
% Divide by its length twice. Normalize and then divide by the former length. Reciprocate the length
% but retain direction. Geometrically, it's inversion in a (hyper)sphere

% In linear algebra, we mostly deal with linear combinations, the scalar product and orthogonal projections. In analytic geometry of 3D space, the cross product is also common and the triple product will occasionally be seen. The rest is less common but included for completeness sake and will be picked up in later sections. The wedge product is important in exterior and geometric algebra. The outer product will be revisited in tensor algebra (in a different guise).

%===================================================================================================

\subsection{Matrix Operations}

%---------------------------------------------------------------------------------------------------
\subsubsection{Addition and Subtraction}
Matrices are added and subtracted element-wise. For that to work, the two matrices must have the same shapes. TODO: Define the zero matrix and its notation

%---------------------------------------------------------------------------------------------------
\subsubsection{Multiplication} In order to multiply two matrices, we must demand that the number of columns of the left factor must be equal to the number of rows in the right factor. When we multiply an $m \times p$ matrix $\mathbf{A}$ with a $p \times n$ matrix $\mathbf{B}$ and call the product $\mathbf{C}$ such that $\mathbf{A} \mathbf{B} = \mathbf{C}$, then the element $c_{ij}$ of the product can be computed from the elements $a_{ij}, b_{ij}$ of $\mathbf{A}, \mathbf{B}$ as follows:
\begin{equation}
 \mathbf{C} = \mathbf{A} \mathbf{B} 
 \quad \text{where} \quad
 c_{ij} = \sum_{k=1}^p a_{ik} b_{kj}
\end{equation}
The matrix $\mathbf{C}$ will have a shape of $m \times n$. Note that the element $c_{ij}$ can be interpreted as a scalar product of the $i$-th row of $\mathbf{A}$ with the $j$-th column of  $\mathbf{B}$ [VERIFY!]. The matrix that has zeros everywhere except on the main diagonal is called the identity matrix and denoted as usually denoted as $\mathbf{I}$. One could perhaps argue that $\mathbf{1}$ could also be a good notation, but I have never seen that - on the other hand, the difference is subtle anyway. This matrix is the neutral element of matrix multiplication. That means that  $\mathbf{I} \mathbf{A} = \mathbf{A} = \mathbf{A} \mathbf{I}$ for any (square) matrix $\mathbf{A}$ whatsoever. In this notation, it is usually understood that the size of $\mathbf{I}$ is matched to the size of $\mathbf{A}$. Matrix multiplication is associative: $(\mathbf{A} \mathbf{B}) \mathbf{C} = \mathbf{A} (\mathbf{B} \mathbf{C})$ and distributive over matrix addition and subtraction: $\mathbf{A} (\mathbf{B} \pm \mathbf{C}) = \mathbf{A} \mathbf{B} \pm \mathbf{A} \mathbf{C}$ but it is not generally commutative: $\mathbf{AB} \neq \mathbf{BA}$. 

% -maybe add also the Kronecker product and list formulas for its connection with the regular
%  product
% -maybe explain block matrices
% -But maybe put these things into a section Lesser Known Stuff
%  ...although, it could actually also fit into the "decomposition" section
% -The columns of a matrix may be interpreted as the images of the standard basis vectors. But maybe
%  this should go to a section matrix-vector operations. It may contain products like A x, x^T A,
%  x^T A x (sandwich product)
% -The matrix product can be seen as being made up from scalar products of the rows of the left 
%  factor with the columns of the right factor



%---------------------------------------------------------------------------------------------------
\subsubsection{Transposition}
The operation of turning the rows of a matrix into the columns of a new matrix and therefore also the columns into rows is called \emph{transposition} of a matrix. This is denoted by a superscript $T$ as in $\mathbf{A}^T$. When a matrix is complex, then most of the time we want to combine transposition with complex conjugation of the elements because that's how many formulas for real matrices involving the transpose generalize to the complex case. It's rare to see transposition or complex conjugation all by itself applied to a complex matrix (it happens occasionally, though). The combined operation of transposition and conjugation is sometimes called \emph{Hermitian transpose} and denoted by a superscript $H$ as $\mathbf{A}^H$. But there are other notations for that too, for example using a dagger: $\mathbf{A}^\dagger$ or an asterisk $\mathbf{A}^*$ [VERIFY!]. But watch out - often the asterisk denotes only complex conjugation alone and the dagger is also often used for the (pseudo-)inverse.

% https://en.wikipedia.org/wiki/Adjugate_matrix
% https://www.varsitytutors.com/hotmath/hotmath_help/topics/adjoint-of-a-matrix
% https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Inverse_of_a_matrix
% cofactor matrix

% In A^T A, we compute dot-products of the columns of the matrix - each columns dot-multiplied
% with it self. In A A^T, the same happens with the rows. See:
% https://www.youtube.com/watch?v=hsJ1-6ybvGM
% at around 25 min

% (AB)^T = B^T A^T

% The Matrix Transpose: Visual Intuition
% https://www.youtube.com/watch?v=wjYpzkQoyD8

%---------------------------------------------------------------------------------------------------
\subsubsection{Inversion}
If we have given a matrix $\mathbf{A}$ and we can find another matrix $\mathbf{B}$ such that  $\mathbf{B} \mathbf{A} = \mathbf{I}$, then we call $\mathbf{B}$ a left inverse of $\mathbf{A}$. A matrix $\mathbf{C}$ with the property $\mathbf{A} \mathbf{C} = \mathbf{I}$ would be called a right inverse. It turns out that there is no difference between these two notions, i.e. left inverses are always also right inverses and vice versa so we call a matrix that has both of these properties just an inverse matrix of $\mathbf{A}$. It turns furthermore out that if such an inverse exists, then it is unique, so we call such a matrix \emph{the inverse} of $\mathbf{A}$ and denote it by $\mathbf{A}^{-1}$. Finding such an inverse for a given matrix $\mathbf{A}$ can be done by an algorithm called Gauss-Jordan elimination which we will be explained later. [VERIFY!] [TODO: explain pseudo-inverse]
% Could be done via Gauss-Jordan Elimination Algorithm decribed in the section about solvin linear 
% systems

% Misc: transposition, hermitian tranpose, etc.

% https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse

%---------------------------------------------------------------------------------------------------
\subsubsection{Commutator}
Matrix multiplication is, in general, not commutative, i.e. $\mathbf{A} \mathbf{B} \neq \mathbf{B} \mathbf{A}$ in general. We also say, the matrices  $\mathbf{A}$ and $\mathbf{B}$ do not \emph{commute} in general. The difference of the two possible products $\mathbf{C} = \mathbf{A} \mathbf{B} - \mathbf{B} \mathbf{A}$ is called the commutator of $\mathbf{A}$ and $\mathbf{B}$. It can be thought of as measuring, how non-commutative the matrices are with respect to one another. If two matrices commute, their commutator is the zero matrix. [TODO: What about $\mathbf{A} \mathbf{B} \mathbf{A}^{-1} \mathbf{B}^{-1}$. That's how the commutator is defined in group theory. It's the identity matrix if  $\mathbf{A}$ and $\mathbf{B}$ commute, I think. Figure out and explain the relation between these two conflicting definitions!]

%---------------------------------------------------------------------------------------------------
\subsubsection{Square Root}

% https://en.wikipedia.org/wiki/Square_root_of_a_matrix
% https://en.wikipedia.org/wiki/Cholesky_decomposition
% https://en.wikipedia.org/wiki/Definite_matrix#Decomposition


%---------------------------------------------------------------------------------------------------
\subsubsection{Sandwich Product, Change of Basis, Similarity Transformation}
Another important operation between two matrices $\mathbf{A}$ and $\mathbf{B}$ is defined as follows: $\mathbf{C} = \mathbf{B}^{-1} \mathbf{A} \mathbf{B}$. This operation is called a \emph{similarity transformation} also known as \emph{change of basis} transformation. Because the matrix $\mathbf{A}$ is sandwiched between the matrices $\mathbf{B}^{-1}$ and $\mathbf{B}$, this operation is sometimes also called a \emph{sandwich product}. It has an interesting interpretation. Consider what the matrix $\mathbf{C}$ does when we apply it to yet another matrix $\mathbf{X}$. Let $\mathbf{Y} = \mathbf{C X} = \mathbf{B}^{-1} \mathbf{A B X}$. Reading the iterated matrix product from right to left, we can interpret $\mathbf{B}^{-1} \mathbf{A} \mathbf{B}$ as doing the same transformation as $\mathbf{C}$ in three steps: Firstly, $\mathbf{X}$ gets transformed by $\mathbf{B}$ into a new coordinate system. Secondly, the result of that gets transformed by $\mathbf{A}$. Thirdly, the result of that gets transformed back into original coordinate system by $\mathbf{B}^{-1}$. We interpret this as follows: the matrix $\mathbf{A}$ and $\mathbf{C}$ represent the \emph{same} transformation but expressed in \emph{different} coordinate systems. The matrices $\mathbf{A}$ and $\mathbf{C}$ are said to be \emph{similar}. They share a lot of properties. The terms have not yet been introduced but for completeness, I'll list those shared properties here anyway\footnote{Did I already mention that putting math topics in a strict linear order without any forward references is difficult? (rhetorical question)}, so don't worry if you do not yet understand them: characteristic polynomial (and therefore the derived properties determinant, trace, eigenvalues and their algebraic and geometric multiplicities), minimal polynomial, rank, index of nilpotence, Jordan normal form, Frobenius normal form, and elementary divisors. Because the coordinate system is determined by the basis vectors, the designation as "change of basis" transformation should now also make sense. 
..TBC...

% https://en.wikipedia.org/wiki/Matrix_similarity
% https://en.wikipedia.org/wiki/Matrix_similarity#Properties
% https://en.wikipedia.org/wiki/Elementary_divisors

% is called \emph{conjugation} of  $\mathbf{A}$ by $\mathbf{B}$. It is defined as: 

% https://math.stackexchange.com/questions/134796/conjugation-of-matrices-and-conjugation-of-complex-numbers


% https://www.youtube.com/watch?v=gAPlRlmhXyI
% A change of perspective | Linear algebra episode 7
% Interpretation of the sandwich product: C = B^-1 A B:
% -Consider what C does to a vector x: y = C x =  B^-1 A B x
% -firstly, x gets transfromed by B into a new basis (or new coordinate system)
% -secondly, the transformation A is applied in this new basis
% -thirdly, the result is transformed back to the original coordinate system by B^-1
% -> this pattern is useful when the desired transformation is easy to express in some specific 
%    coordinate system
% -For example, to project vectors onto an arbitrary line, one could first rotate the line to make it
%  coincide with the x-axis, then set y to zero (project onto the x-axis), then rotate back. A similar
%  procedure works for reflecting in an arbitrary line. Figuring out the partial matrices is easy. The result
%  is a bit more complicated
% This is an example of a more general pattern for problem solving: 
% transform the problem, solve in transformed domain, transform back
% another example is lowpass filtering: discrete fourier transform  (DFT)-> zero out high bins -> IDFT
% ..that's actually also a projection in the DFT domain

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix-Vector Products}
We have seen operations involving two vectors as well as operations involving two matrices. From a geometric perspective, linear algebra really comes to life when considering the interplay between vectors and matrices. Vectors encode positions and therefore also geometric shapes which are defined by the positions of their vertices. Matrices encode geometric transformations such as reflections, rotations, scalings, etc. We can interpret such a transformation as an \emph{action} that is applied to a shape, i.e. to a set of vectors. A matrix can \emph{act on} a vector or - in passive language - can \emph{be applied to} the vector. This action is effected by forming the \emph{matrix-vector product}. We actually already have everything we need. We do not need to define any brand new product for this. Instead, we observe that column vectors can be interpreted as special matrices - namely as matrices that just have a single column, i.e. as $n \times 1$ matrices where $n$ is the dimensionality of the vector. We can multiply such an $n$-vector $\mathbf{v}$, seen as $n \times 1$ matrix, from the left with an $n \times n$ matrix  $\mathbf{A}$ using the rules of matrix multiplication to get another $n \times 1$ matrix, i.e. another $n$-vector $\mathbf{w} = \mathbf{A v}$. Forming such a product between a matrix as left factor and a column vector as right factor will be our usual way of expressing geometric transformations. 

%\medskip
\paragraph{Left vs Right Multiplication}
An entirely equivalent formalism for geometric transformations can be formulated using row vectors, i.e. $1 \times n$ matrices and products where the matrix factor appears as the right factor. To switch from one formalism to the other requires to transpose all matrices and write all products in reverse order [VERIFY!]. The advantage of that would be that we can read such products from left to right and that reading direction corresponds to the order in which the transformations are applied. However, we will stick to column vectors and multiplying by matrices from the left because that's more common in mathematics. 

% https://math.stackexchange.com/questions/2738278/what-exactly-is-a-left-multiplication-transformation
% https://medium.com/geekculture/right-and-left-matrix-multiplication-d21947f195d8
% https://math.stackexchange.com/questions/3252146/linear-transformations-and-left-multiplication-matrix

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Left_and_right_eigenvectors

%\medskip
\paragraph{Dimensionality Conversions}
The rules of matrix multiplication would also allow to use an $m \times n$ matrix - that is, the dimensionality $m$ of the output vector could in general be different from the dimensionality $n$ of the input vector. If $m < n$, we would get a dimensionality reduction. That means, some sort of projection must be involved in the transformation and information will be discarded by forming the product. This is quite common in graphics when we eventually need to project a 3D scene onto a 2D screen. The $m > n$ case would mean that our $n$-vector lives in a (sub)space of lower dimensionality that is embedded in some higher dimensional space. In such a case, the matrix-vector product may lift the vector out of its subspace.

%\medskip
\paragraph{Sandwich Products}
Another kind of \emph{sandwich product} can be formed when sandwiching a matrix between two vectors. It takes as inputs an $m \times 1$ vector $\mathbf{w}$, an $m \times n$ matrix $\mathbf{A}$ and an $n \times 1$ vector $\mathbf{v}$ and produces the product $\mathbf{w}^T \mathbf{A v}$. The transposition of $\mathbf{w}$ into $\mathbf{w}^T$ turns it into an $1 \times m$ row vector which is required to make the matrix shapes (vectors are special matrices!) compatible for multiplication. Due to associativity of matrix multiplication, it doesn't matter if you evaluate it as $\mathbf{w}^T (\mathbf{A v})$ or as $(\mathbf{w}^T \mathbf{A}) \mathbf{v}$. The result will be the same in both cases and it will be a scalar, i.e. just a number. Sandwich products are important in the definition of quadratic forms which in turn are important in defining some basic geometric shapes (ellipsoids, hyperboloids, paraboloids) and also in optimization and approximation applications. These sandwich products also appear a lot in quantum mechanics and they may serve as a formalism for generalizing the scalar product especially in the context of differential geometry.  There, the regular scalar product, which we normally use to compute the squared length of a vector, is replaced by such a sandwich product involving the so called metric tensor - which is basically just the matrix that you sandwich into the regular scalar product. [VERIFY if we also use the sandwich product to compute angles between vectors in DG or if we use it just for the length]








\paragraph{$\star$ Functions of a Matrix}
Now we want to define what is means to apply a function such as the exponential or trigonometric functions to a matrix. What is $f(\mathbf{X}) = \sin(\mathbf{X})$ supposed to mean when $\mathbf{X}$ is a matrix? But before looking at the sine function, let's start with something simpler: the square root function. It makes sense to say that $\sqrt{\mathbf{X}}$ is a matrix which results in $\mathbf{X}$ when being squared. That is: we want to find a matrix $\mathbf{Y}$ such that $\mathbf{Y}^2 = \mathbf{X}$ where it is understood that $\mathbf{Y}^2 = \mathbf{Y Y}$, i.e. squaring a matrix means to just form the matrix product with itself. For this to make sense, we must require that $\mathbf{Y}$ is a square matrix because otherwise we cannot form the product. When faced with this equation $\mathbf{Y}^2 = \mathbf{X}$, the first question that arises is whether or not such a matrix even exists. Assuming that it does, we would like to have some sort of algorithm to compute it. [TODO: figure out if nD Netwon iteration works for this]. Matrix functions like $\exp(\mathbf{X})$ or $\sin(\mathbf{X})$ are typically defined via the Taylor expansion of the respective function. We just take the scalar coefficients from the regular Taylor series expansion for real numbers and replace the variable $x$ with the matrix  $\mathbf{X}$. All we need to know evaluate such a Taylor series is how to multiply a matrix by a scalar and how to multiply two (square) matrices - and these are things we indeed do know by now. Of course, we must also ensure that series converges. It turns out that for $\exp, \sin, \cos, \sinh, \cosh$ and many more of our favorite functions, the series does indeed converge, so we have a working definition. Especially the matrix exponential is a very important function because it let's us write down solutions to certain systems of differential equations explicitly.

%multiplied

% log: find a matrix B such that e^B = A
% explain hwo diagonalization can help so evaluate functions

% How can the sqrt be computed? Will Newton iteration work

% sqrt

\paragraph{$\star$ Functions of a Vector}
We have seen that it is possible to define functions of a matrix via the Taylor series. Can we do something like that with vectors, too? Well, the Taylor series approach requires that we need a way to multiply the variable (repeatedly) by itself to form powers. For that, we need a multiplication to be defined. For vectors, we have seen various products. Can one of them be meaningfully iterated? The scalar product takes two vectors as input - but it produces a scalar as output, so that doesn't seem to work out. The cross product could look like a promising candidate. It takes two vectors as input and produces another vector as output. Unfortunately, it works only in 3D - but even there, it doesn't really help much because the cross product of a vector with itself will just produce the zero vector. I'm not aware of any meaningful way to define a \emph{vector valued} function of a vector similarly as we did for matrices. However, we will later in geometric algebra encounter the so called \emph{multivectors} of which the vectors form a subset. For these multivectors, it is indeed possible to define functions that return another multivector - just like we did for matrices.

\medskip
TODO: explain kernel an image of a matrix

%generalizations of the scalar product (especially in differential geometry) and also in 

% explain the ^T notation

%...TBC...TODO: mention sandwich product $\mathbf{w}^T \mathbf{A v}$.

% Left-product, right -product, sandwich product



%===================================================================================================
\subsection{Vector Features}

\paragraph{Norms}
The \emph{norm} of a vector captures the idea of its length. We can abstractly view it as a function that takes a vector as input and returns a scalar. The most common norm is the Euclidean norm but there are others as well. On page \pageref{Tab:Norm}, we already abstractly defined some properties that we require from a norm. ...TBC...
% are a measure of length, $L_p$-norm, explain the general requirements to a norm
% norm equivalence
% refer back to norms of numbers defined in \ref{Tab:Norm}

\paragraph{Distance}
A \emph{distance function} is a function into which we can plug in two vectors and it spits out a scalar. That scalar should indicate, how far apart the two vectors are. A distance is usually induced by a norm: As distance between two vectors, we take the norm of their difference. Different norms will lead to different notions of distance. ...TBC...
% state abstarct requirements for a distance. ...see topology chapter

% distances emphasize the interpretation of vectors as points/locations

% the scalar product emphasizes the interpretation as arrows or directed lengths


\paragraph{Orthogonality and Angles}
We have observed in $\mathbb{R}^2$ and $\mathbb{R}^3$ that the dot product of two vectors is zero, iff these two vectors are perpendicular (aka orthogonal) to one another. In higher dimensional spaces, we \emph{define} two vectors to be \emph{orthogonal}, if their dot product is zero. ...TBC...

%More generally, we define the angle between two vectors as the arc-cosine of their normalized dot product ...VERIFY!..TBC...
% Angles between a are defined in terms of the cosine of the scalar product
% 

% parallel: if one is a scalar multiple of the other


\paragraph{Span of a Set of Vectors}
As the \emph{span} of a set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$, we define the set of all vectors that can be formed by arbitrary linear combinations of those vectors, i.e. the set of all vectors that can be written as:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_n \mathbf{v}_n 
\end{equation}
for some set of scalar coefficients $a_1, \ldots, a_n$. The span of a set of vectors is itself a vector space: We can add two vectors from that space to get another vector which is again in the same space. And we can also scale the vectors from that space without leaving the space. We also say that our vectors span the space - where "span" is being used as a verb. The span (noun) of a set of vectors is the space that is spanned (verb) by these vectors. I know that this sounds silly and self referential. Often, the span of a set of vectors is a subspace of some larger embedding space. For example, in 3D space, a set of two vectors could span a 2D subspace, i.e. a 2D plane that is embedded in 3D space.


\paragraph{Linear Independence}
A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$ is said to be linearly independent, if none of its elements can be expressed as a linear combination of other elements. A necessary and sufficient mathematical condition for linear independence is that the equation:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_n \mathbf{v}_n = \mathbf{0}
\end{equation}
can be satisfied only if all the $a_i$ are equal to zero, i.e. only the trivial solution for the $a_i$ exists [VERIFY]. Linear independence is an important condition for a set of vectors to have. In a sense, it means that all vectors are needed and none of them is redundant. 

\paragraph{Dimension}
The sense in which none of the vectors is redundant in a linearly independent set of vectors is the following: If we would remove any of the vectors, we would lose a degree of freedom to move around. The space spanned by the remaining set of vectors would get reduced in dimension (by one). The \emph{dimension} of a space is the smallest number of vectors that is needed to span that space, i.e. to make any point in the space "reachable" via a linear combination of these basis vectors. Why do we make a big fuss of this? Clearly, the dimension of $\mathbb{R}^n$ is just $n$ right? So why all the fuss about linearly independent vectors? The reason is that in linear algebra, we also often deal with \emph{subspaces} of a given, higher dimensional embedding space. This embedding space could be $\mathbb{R}^n$ but we could consider a subspace of dimension $m < n$ that is spanned by a set of just $m$ vectors rather than $n$.

% The dimension is the number of degrees of freedom or number of independent directions that we can move in

\paragraph{Basis}
A \emph{set} $\{\mathbf{b}_1, \ldots, \mathbf{b}_n \}$ of linearly independent vectors that spans a given space is called a \emph{basis} for that space. A \emph{tuple} $(\mathbf{b}_1, \ldots, \mathbf{b}_n)$ of such vectors is called an \emph{ordered basis}. The vectors $\mathbf{b}_i$ in such a basis (ordered or not) are called the \emph{basis vectors}. For example, the so called \emph{standard basis} (aka canonical basis or natural basis) for $\mathbb{R}^3$ is given by the 3 vectors $\mathbf{e}_1 = (1,0,0), \mathbf{e}_2 = (0,1,0), \mathbf{e}_3 = (0,0,1)$. It is a common convention to denote the $n$ vectors of the standard basis of $\mathbb{R}^n$ as $\mathbf{e}_i, i = 1,\ldots,n$. Specifically in $\mathbb{R}^3$, one may also encounter the notation $\mathbf{e}_x, \mathbf{e}_y, \mathbf{e}_z$ or $\hat{\mathbf{x}}, \hat{\mathbf{y}}, \hat{\mathbf{z}}$ or  $\mathbf{i}, \mathbf{j}, \mathbf{k}$. We'll stick to the general notation $\mathbf{e}_i$ here.

% https://en.wikipedia.org/wiki/Basis_(linear_algebra)

\paragraph{Gram-Schmidt Orthogonalization}
When we have given an arbitrary ordered basis $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$, the basis vectors $\mathbf{b}_i$ will be in general neither normalized to unit length nor orthogonal to one another. The process of \emph{Gram-Schmidt orthogonalization} may be used to derive another basis $C$ from $B$ that has these two (often desirable) features while spanning the same space as $B$. It's an algorithm that works as follows: ...TBC...

% TODO: explain how we could order a given basis:
% Idea: 
% -Compute for each vector in the basis the center of gravity / center of mass 
% -To do this, use the absoulte values of the entries
% -Order the basis vectors according to ascending center of gravity.
% -The idea is that the ordered basis kind of "roughly resembles" the standard basis. If the algo
%  gets the standard basis in shuffled order as input, it will return it in the correct order.
% -What about a basis like (1,1), (1,-1)? the center of mass is the same for both (at "index" 0.5)
%  Maybe in such a case, we should also consider angles: choose the vector that aligns best with
%  the standard basis vector of that index. If more of them align best, choose the one with
%  positive angle
%
% Try to find a way to canonicalize a basis B = b_1,..,_b_n. That process could involve (in that
% order)
% -Ordering of the vectors according to center of weight
% -Tweaking the initial vector b_1 to align most closely with e_1 = (1,0,0,...) without leaving
%  the span of b_1,...,b_n. That means: if e_1 is actually in the span of B, then the updated b_1
%  will indeed become e_1 itself
% -Gram-Schmidt orthogonalization of the remaining vectors b_k, k = 2,...,n
%  -Maybe in each step, try to align the vector b_k as closely as possible with e_k
% 
% -Maybe find a basis that minimizes the distance to a given basis. Maybe if we write the basis as
%  matrix (basis vectors go into the columns), we could definethe distance as Frobenius norm of the
%  difference between our basis and the given basis.
% -Maybe the given basis should be a suitable subset of the standard basis. Which subset is suitable
%  could be figured out using the "center of gravity" approach. For each b_i, find it's center of
%  gravity (it can be interpreted as a non-integer index in 1...n), round and pick e_n. But some 
%  care must be taken if two cneter of gravities round to the same index like 2.6 and 3.4.



%===================================================================================================
\subsection{Matrix Features}





%---------------------------------------------------------------------------------------------------
\subsubsection{Characteristic Numbers}

\paragraph{Rank}
The \emph{column rank} of a matrix $\mathbf{A}$ is the number of linearly independent columns. Sometimes, it is useful to view the columns $\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_n$ of a matrix $\mathbf{A}$ as vectors. In this view, the rank of $\mathbf{A}$ is the dimension of the span of these column vectors. The \emph{row rank} is defined analogously as the number of linearly independent rows. A fundamental (and non obvious) result of linear algebra is that the column rank and the row rank are always equal. It is therefore justified to scrap the row/column qualifiers and just talk about the \emph{rank} of a matrix. We will denote the rank $r$ of a matrix $\mathbf{A}$ as $r = \rank(\mathbf{A})$. For an $(m \times n)$-matrix with rank $r$, the \emph{rank deficiency} is defined as $d = \min(m,n) - r$ [VERIFY!]. [TODO: explain row-space, column-space, nullity, rank of linear map, rank-nullity-theorem]

% The rank of a linear map or operator is defined as the dimension of its image
% https://en.wikipedia.org/wiki/Rank_(linear_algebra)

\paragraph{Trace}
The trace of a square matrix $\mathbf{A}$ is the sum of its diagonal elements and denoted by $\tr(\mathbf{A})$. 
\begin{equation}
 \tr(\mathbf{A}) = \sum_{i=1}^{n} a_{ii}
\end{equation}
For non-square matrices, the trace is not defined. The trace is also equal to the sum of the eigenvalues and it holds that $\tr(\mathbf{A B}) = \tr(\mathbf{B A})$. This implies that similar matrices have the same trace. The trace is linear such that: $\tr(\mathbf{A + B}) = \tr(\mathbf{A} + \tr(\mathbf{B})$ and $\tr(c \mathbf{A}) = c \tr(\mathbf{A})$. ...TBC...

%https://en.wikipedia.org/wiki/Trace_(linear_algebra)

\paragraph{Determinant}
Geometrically, the \emph{determinant} of a $(n \times n)$-matrix $\mathbf{A}$ is the amount by which the $n$D hypervolume of any chunk of the vector space $\mathbb{R}^n$ gets expanded or shrunken by multiplying by the matrix. You can imagine this as what happens to the (hyper)volume of the unit hypercube in $\mathbb{R}^n$ when applying the matrix to all of its vertices - but it applies to any other chunk of the space just the same. Note that the determinant also has a sign - so it's a kind of signed volume change factor. The determinant happens to be equal to the product of the eigenvalues. If we have a triangular matrix, then the determinant can be easily be computed as the product of the diagonal entries. If the determinant of a matrix is zero, it means geometrically that a hypervolume of $\mathbb{R}^n$ will be completely collapsed by the matrix into a lower dimensional object. For example, in $\mathbb{R}^3$, a 3D volume could be collapsed into some flat planar 2D object, or just into a 1D line or even into a 0D point. A matrix with a determinat of zero is called \emph{singular}, one with nonzero determinant is called \emph{regular}. Only regular matrices are invertible. This is geometrically plausible because you cannot undo the total collapse of a dimension - once a dimension has been squashed away, the information about that dimension is irrecoverably lost. 

\medskip
There are various ways to compute the determinant - the most practical perhaps being to transform the matrix into triangular form while keeping track of what the transformations do to the determinant (swapping rows changes the sign, multiplying a row by a factor changes the determinant by the same factor). At the end of that transformation process, we would compute the product of diagonal elements and take into account the modifications that accumulated during the transformation. ...VERIFY...TBC...

% https://en.wikipedia.org/wiki/Determinant
% https://en.wikipedia.org/wiki/Dieudonn%C3%A9_determinant  generalization

\paragraph{Norms}
Just like for vectors, there exist also various norms for matrices ...TBC...

TODO: Frobenius norm, spectral norm, ..., norm compatibility (with vectors)
% explain norm compatibility with vector norms

% https://en.wikipedia.org/wiki/Matrix_norm



%---------------------------------------------------------------------------------------------------
\subsubsection{Special Matrices} % Formerly "Special Properties"
Some matrices with special features are important enough to have been given special names.

% Maybe rename to speical matrices
% Maybe move that below the characteristioc numbers and eigenspaces subsections because some 
% properties may be defined in terms of these numbers (e.g.: contractive: all eigvals < 1 in 
% abs-val)
% By a property of a matrix, I mean a feature thata matrix may or may not have - a boolean feature, so to speak.



\paragraph{Square Matrices}
A matrix is called a square matrix, if it has the same number of rows as it has columns. In general, matrices are rectangular arrays of numbers. In square matrices that rectangle is actually a square. It's an important special case and many of the following definitions make sense only for square matrices.

\paragraph{Regular and Singular Matrices}
A regular matrix is a square matrix whose determinant is nonzero. The notion applies only to square matrices because only for those, the determinant is defined. A singular matrix is one whose determinant is zero. Regular matrices are invertible, singular ones are not. Both implications go both ways, i.e. the features "regular" and "invertible" imply each other just as "singular" and "non-invertible" do. %One could perhaps say that non-square matrices are always singular [VERIFY]

% https://en.wikipedia.org/wiki/Invertible_matrix

\paragraph{Symmetric Matrices}
A square matrix is defined to be symmetric if it is equal to its own transpose: $\mathbf{A} = \mathbf{A}^T$. That means that the rows are equal to the columns. Another way to state this is that for the matrix elements we must have $a_{ij} = a_{ji}$ for all pairs $i,j$. Obviously, symmetry is a feature that only square matrices can possibly have because otherwise we couldn't even pair up every $a_{ij}$ with a corresponding $a_{ji}$.
% Give fomrula for invers of transpose and/or for (AB)^T = B^T A^T iirc
% Symmetry means that $a_{ij} = a_{ji}$ for all $i,j$

\paragraph{Antisymmetric Matrices}
A square matrix is defined to be antisymmetric if $a_{ij} = -a_{ji}$ for all pairs $i,j$. For the diagonal elements, they only way that $a_{ii} = -a_{ii}$ can happen is when $a_{ii} = 0$. So, antisymmetric matrices necessarily have all zeros on their diagonal.

% also called skew-symmetric

\paragraph{Hermitian Matrices}
A square matrix is defined to be Hermitian if it is equal to its own Hermitian transpose: $\mathbf{A} = \mathbf{A}^H$ which is the transpose combined with complex conjugation of the elements. It is usually the right generalization of symmetric matrices when moving from the real to the complex case.

% maybe put these "boolean" features into a subsection
\paragraph{Orthogonal Matrices} A square matrix is called \emph{orthogonal} when its columns, seen as a set of vectors, form an \emph{orthonormal} set. That means, the scalar product of each column with itself must be one and the scalar product of any column with any other column must be zero. Non-obviously, this is equivalent to requiring that the rows must be orthonormal to one another. I have no idea where the inconsistency between usage of ortho\emph{gon}al and ortho\emph{norm}al between matrices and sets of vectors comes from but it seems like we are unfortunately stuck with it. Or maybe I'll be rebellious and call them orthonormal instead - at least, when there isn't any potential for confusion. Being orthonormal is equivalent to say that the inverse of the matrix is just the transpose: $\mathbf{A}^{-1} = \mathbf{A}^T$ iff $\mathbf{A}$ is orthonormal. That is a pretty convenient feature because transposing a matrix is very easy whereas inverting a matrix is quite hard (i.e. computationally expensive) in general.

% https://math.stackexchange.com/questions/316208/difference-between-orthogonal-and-orthonormal-matrices

% https://www.quora.com/Why-don-t-we-call-orthogonal-matrix-just-orthonormal-matrix-if-its-columns-rows-are-orthonormal

%where this additional condition leads to the stronger
% a (square) matrix is orthogonal when all its rows are mutually orthogonal. This implies

% https://en.wikipedia.org/wiki/Orthogonal_matrix

\paragraph{Unitary Matrices}
A unitary matrix is one whose Hermitian transpose is equal to its inverse: $\mathbf{A}^{-1} = \mathbf{A}^H$ means that $\mathbf{A}$ is unitary. Just like being Hermitian is the generalization of being symmetric to the complex case, being unitary is the generalization of being orthogonal to the complex case.



% https://en.wikipedia.org/wiki/Hermitian_matrix
%https://mathworld.wolfram.com/HermitianMatrix.html

\paragraph{Definite Matrices}
A matrix $\mathbf{A}$ is said to be \emph{positive definite} if for every nonzero vector $\mathbf{x}$, the sandwich product of the vector with that matrix is positive: $\forall \mathbf{x \neq 0}: \; \mathbf{x}^T \mathbf{A x} > 0$. It's called \emph{positive semidefinite} if $\mathbf{x}^T \mathbf{A x} \geq 0$, \emph{negative definite} if $\mathbf{x}^T \mathbf{A x} < 0$ and \emph{negative semidefinite} if $\mathbf{x}^T \mathbf{A x} \leq 0$. I believe that have heard the "definite" without any qualifiers being used for matrices that satisfy $\mathbf{x}^T \mathbf{A x} \neq 0$ for all nonzero vectors $\mathbf{x}$ [VERIFY!]. TODO: explain indefinite and what definiteness implies for the eigenvalues

% It was here - at 15:35:
% https://www.youtube.com/watch?v=qwsvXa7nIGg
% in the context of norms.

% https://www.quora.com/What-is-the-definition-of-definite-in-mathematics

% Definite without qualifier:

% boolean fetaures: nilpotent, idempotent, unitary, normal,  self-adjoint,
% contractive (all eigenvalues < 1 in absolute value, vectors get shorter), self-inverse
% (aka involution), is there something like "unipotent" such that some power of the matrix is the
% identity matrix? regular/singular
%
% discrete features: rank, index of nilpotency, signature
% scalar: determinant, condition number
% list of scalars: eigenvalues, singular values
% list of vectors: (left/right) (generalized) eigenvectors, singular vectors
% boolean features of pairs of matrices: similarity (is A similar to B), simultaneously
%   diagonalizable,
% matrix: set of similar matrices,



% https://en.wikipedia.org/wiki/Frobenius_matrix
% https://en.wikipedia.org/wiki/Normal_matrix
% Toeplitz, Circulant, Jordan, Vandermonde

% https://en.wikipedia.org/wiki/Cauchy_matrix
% 




% https://en.wikipedia.org/wiki/Stochastic_matrix
% https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem

%---------------------------------------------------------------------------------------------------
\subsubsection{Eigenspaces}
When forming a product between an $n \times n$ matrix $\mathbf{A}$ and an $n$-vector $\mathbf{v}$, we will get a new $n$-vector, which will, in general, point into a different direction than  $\mathbf{v}$. There may, however, be certain special vectors which do not change direction as a result of such a multiplication by a matrix but rather just (possibly) change their length. Such special vectors are called the \emph{eigenvectors} of the matrix $\mathbf{A}$ and the length-change factor is called the corresponding \emph{eigenvalue}. The equation that expresses this circumstance is given by:
\begin{equation}
 \mathbf{A v} = \lambda \mathbf{v}  
 \quad \Leftrightarrow \quad
 \det ( \mathbf{A} - \lambda \mathbf{I} ) = 0
\end{equation}
which is called the characteristic equation. [the right one, I think]
[VERIFY if implication is two-sided]. TODO: explain what this has to do with the determinant, explain etymology of "eigen"
% The lhs is a matrix-vecztor product whereas the right hand side is just a scalar multplication with the sclaing factor $\lambda$.


\paragraph{Eigenvalues and Characteristic Polynomial}
If we expand the left hand side of the characteristic equation $\det ( \mathbf{A} - \lambda \mathbf{I} ) = 0$ according to the rules for how determinants are computed, it will turn out that we will get a polynomial in $\lambda$ of degree $n$. This polynomial is called the \emph{characteristic polynomial} of the matrix  $\mathbf{A}$. The right hand side says that this polynomial should be equal to zero for $\lambda$ to be an eigenvalue. Another way to say that is that an eigenvalue $\lambda$ is a root of the characteristic polynomial. We know that roots of polynomials may have a multiplicity, i.e. in the product form of the polynomial, the linear factor that corresponds to the root may occur multiple times. The multiplicity of the eigenvalue as the root of the characteristic polynomial is called the \emph{algebraic multiplicity} of the eigenvalue and denoted by $\alg(\lambda)$. Note that even for matrices of real numbers, the eigenvalues may be complex. If the matrix is real, then complex eigenvalues will always come in pairs of conjugates [VERIFY!].

...TBC...

% The eigenvalues are important because they characterize the features of the matrix in a way that is independent from the chosen coordinate system, i.e. they are basis independent. More precisely, they characterize not the matrix itself but rather the underlying linear transformation that our matrix represents. These characterizations do not depend on that particular (arbitrary) representation - in another coordinate system, the matrix will look totally different but these features will still be the same. The eigenvectors, on the other hand, will be different in another coordinate system (aka basis). I like to think of them as relating the basis independent features (given by the eigenvalues) to our particular chosen basis, i.e. they determine, how exactly the basis independent features manifest themselves in our particular chosen basis.

% Algebraic multiplicity, geometric multiplicity
% left and right eigenvectors

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors

%\paragraph{Eigenvalues}

\paragraph{Eigenvectors and Eigenspace}
If we have an eigenvalue $\lambda$, we may also want to know the corresponding eigenvector(s). Multiple eigenvectors that correspond to a single eigenvalue may only exist when the algebraic multiplicity is greater than one. The number of eigenvectors that correspond to a given eigenvalue is called the \emph{geometric multiplicity} of the eigenvalue and denoted by $\geo(\lambda)$ and that geometric multiplicity is always at least one and at most equal to the algebraic multiplicity: $1 \leq \geo(\lambda) \leq \alg(\lambda)$. If $\geo(\lambda) < \alg(\lambda)$, the eigenvalue is called a \emph{defective eigenvalue}. A matrix that has defective eigenvalues is called a \emph{defective matrix} [VERIFY!]. Just like eigenvalues, eigenvectors too can be complex even for real matrices. In the case of real matrices, they will also come in complex conjugate pairs. 

\medskip
To find an eigenvector for an eigenvalue $\lambda$, we need to solve the linear system of equations represented by $\mathbf{A v} = \lambda \mathbf{v}$.  This can be rewritten as $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$ which is in the standard form $\mathbf{A x} = \mathbf{b}$ that we need for solving such equations. In this form, $\mathbf{x}$ is the unknown vector that we are trying to find, $\mathbf{A}$ is a known matrix and $\mathbf{b}$ is a known right hand side vector. We will learn later how to solve such equations. At this point, it should only be pointed out that the solution to such an equation is not necessarily a unique vector and indeed, in our case here it can't be unique because eigenvectors can always be scaled at will. That latter statement means that if $\mathbf{v}$ is an eigenvector, then any scaled version of $\mathbf{v}$ will also be an eigenvector. It is therefore common practice to normalize eigenvectors to unit length.

\medskip
An eigenspace of an $n \times n$ matrix $\mathbf{A}$ associated with a given eigenvalue $\lambda$ is defined to be the space spanned by all the eigenvectors that correspond to $\lambda$ [VERIFY!]. Formally, it is the following set of vectors: $E(\lambda) = \{\mathbf{v} \in \mathbb{C}^n : (\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}\}$. 

%In words, that is the set of all vectors that satisfy the equation $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$.

% The eigenvectors may also be complex even for real matrices

%\paragraph{Eigenspaces}
%

% Say something about the space in which the vectors live. If A is a real matrix, the eigenvectors may actually be complex.
% ...but i think, the system is alway singular - we will at least get a 1D continuum of solutions because eigenvectors can be scaled at will.

%If $\alg(\lambda) > 1$, it may happen that this system has no unique solution but rather a whole space of solutions

...TBC...

% Can it happen that the geometric multiplcity is zero? If so - what does it mean?

% If the algebraic multiplicity of $\lambda$ is $1$, then there can be only one such eigen

% Multiple eigenvectors can only exist when the algebraic multiplicity is gretaer than 1

\paragraph{Generalized Eigenvectors} We established that an eigenvector $\mathbf{v}$ with eigenvalue $\lambda$ is a vector with the property $\mathbf{A v} = \lambda \mathbf{v}$ which can be rewritten as $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$. A \emph{generalized eigenvector} with \emph{rank} $r$ is a vector $\mathbf{v}$ that satisfies $(\mathbf{A} - \lambda \mathbf{I})^r \mathbf{v} = \mathbf{0}$. The rank $r \geq 1$ is a natural number and it is understood to be the smallest possible number for which that equation is satisfied, i.e. we also require  $(\mathbf{A} - \lambda \mathbf{I})^{r-1} \mathbf{v} \neq \mathbf{0}$. For $r = 1$, the generalized equation reduces to the defining equation of ordinary eigenvectors. Generalized eigenvectors become important when the geometric multiplicity of an eigenvalue $\lambda$ is less than its algebraic multiplicity, i.e. when we are dealing with defective eigenvalues.   ...TBC...

% what about the edge case $k = 0$. then the matrix becomes then identity matrix and the equation has only the trivial solution. So, could we say that the zero vector is a generalized eigenvector of rank zero? Is that useful? I mean, it's trivial but maybe it completes a general pattern in a satisfying way? I think we have a chain of subspaces of decreasing dimension when dealing wit generalized eigenvectors and the zero vector can indeed be seen a 0-dimensional subspace?

% https://en.wikipedia.org/wiki/Generalized_eigenvector
% https://en.wikipedia.org/wiki/Modal_matrix
% https://en.wikipedia.org/wiki/Modal_matrix#Generalized_modal_matrix


\paragraph{Simpler Special Cases} In the important special case where all eigenvalues are distinct, much of the considerations above simplify considerably. When all eigenvalues $\lambda_i$ are distinct, it means that all the algebraic multiplicities are one and therefore also all the geometric multiplicities are one because of the general rule $1 \leq \geo(\lambda) \leq \alg(\lambda)$. That, in turn, implies that there is a one-to-one correspondence between eigenvalues and eigenvectors. Each eigenvalue has exactly one corresponding eigenvector. Don't make the all too tempting mistake of assuming that this is always so in the general case. It is only so in this special friendly case which is fortunately quite common [VERIFY!]. This distinctness of eigenvalues has the further implication that the matrix is diagonalizable. We'll see later what that means. Suffice to say here that this feature is usually a good thing. For diagonalizability, it is actually already enough if the algebraic and geometric multiplicities of all eigenvalues match: $\forall i: \geo(\lambda_i) = \alg(\lambda_i)$. 

%Don't make the mistake of thinking that there is a one-to-one correspondence between eigenvectors and eigenvalues in general. 

%It is, in general, not the case that each eigenvalue has one and only one corresponding eigenvector. ...TBC...

\paragraph{Invariant Subspaces}
We have learned that an eigenvector of a matrix $\mathbf{A}$ is a vector $\mathbf{v}$ that doesn't change direction under the transformation that is achieved when $\mathbf{A}$ is acting on  $\mathbf{v}$ via the product $\mathbf{w} = \mathbf{A} \mathbf{v}$. We can pick any subset of the set of all (generalized?) eigenvectors and use this subset as a basis for an invariant subspace, i.e. a subspace that is mapped to itself under the matrix A. That the matrix cannot "lift" a vector out of this subspace is clear when we express the vector as linear combination of the selected eigenvectors. Then, each component of the vector in that basis will just be scaled by applying the matrix. No components into other directions will be created. [VERIFY ALL]

% https://en.wikipedia.org/wiki/Invariant_subspace
% https://www.statlect.com/matrix-algebra/invariant-subspace
% https://www.spektrum.de/lexikon/mathematik/invarianter-unterraum/7094

% matrices satsify their own characteristic equation



\paragraph{Cayley Hamilton Theorem}
We know how to multiply matrices and we also know how to scale them. Therefore we can create polynomials with scalar coefficients and (square) matrix valued inputs (and outputs). The characteristic polynomial of a matrix $\mathbf{A}$ is a polynomial with scalar coefficients. What happens, if we interpret it as polynomial that takes a matrix argument and then plug in the matrix $\mathbf{A}$? That might seem like a random thing to do so we might not expect anything in particular. No so! The Cayley Hamilton theorem tells us than when we do this, the result will always be zero matrix. That means: every matrix sastifies its own characteristic equation, i.e. the equation that sets its characteristic polynomial to zero.

% https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem

\paragraph{Minimal Polynomial}
The minimal polynomial of a matrix $\mathbf{A}$ is defined to be the monic polynomial $p(\mathbf{A})$ of smallest degree such that  $p(\mathbf{A}) = \mathbf{0}$. ...TBC...

% https://en.wikipedia.org/wiki/Minimal_polynomial_(linear_algebra)


%---------------------------------------------------------------------------------------------------
%\subsubsection{Properties of Pairs of Matrices}
\subsubsection{Relations between Matrices}
We have seen "boolean properties" (programmer speak) of single matrices, i.e. features that a given matrix may or may not have such as symmetry, definiteness, etc. There are also some of these boolean properties that pairs of matrices may or may not have. Another way to say that is two matrices may or may not be in a certain relation with one another. Some important of such relations will be defined now.

% Could also be called "Relations between Matrices"
% -reciprocity/inversion
% -commutativity
% -bi-orthogonality (?)
% -what about asymmetric relations - maybe something like <, i.e. an order..maybe based on a norm?

\paragraph{Similarity}
A matrix $\mathbf{A}$ is said to be \emph{similar} to another matrix $\mathbf{B}$, if there exists an invertible matrix $\mathbf{S}$ such that $\mathbf{B} = \mathbf{S^{-1} A S}$. We denote this by $\mathbf{A} \sim \mathbf{B}$ which we read as "A is similar to B" [VERIFY!]. Matrix similarity is an equivalence relation, so we have, among other things, a symmetry of the relation: $\mathbf{A} \sim \mathbf{B} \Leftrightarrow \mathbf{B} \sim \mathbf{A}$. We can actually solve for $\mathbf{A}$ by pre-multiplying both sides by $\mathbf{S}$ and post-multiplying both sides by $\mathbf{S}^{-1}$ to obtain $\mathbf{A} = \mathbf{S B S^{-1}}$. The transformation that maps $\mathbf{A}$ to $\mathbf{B}$ is called a \emph{similarity transformation} or a \emph{conjugation}. Conjugation is a term from group theory which we will encounter in a later chapter. The matrix $\mathbf{S}$ that achieves this transformation into the new basis is called \emph{change of basis matrix} or \emph{transition matrix}. 

\medskip
We can see how $\mathbf{A}$ and $\mathbf{B}$ encode the same transformation by considering, how the product: $\mathbf{A} = \mathbf{S B S^{-1}}$ acts on a vector $\mathbf{v}$. It computes $\mathbf{S B S^{-1} v}$ which we can understand as 3 successive transformations by reading the 3 matrices from right to left. As first step, we apply $\mathbf{S}^{-1}$ which transforms the vector $\mathbf{v}$ into the new coordinate system. Then we apply the matrix $\mathbf{B}$ which expresses our actual geometric transformation in this new coordinate system. Then we transform the result back into our old coordinate system by applying $\mathbf{S}$. The overall effect of these 3 transformations should be the same as applying $\mathbf{A}$ which represents our geometric transformation directly in the original coordinate system. So, the matrix $\mathbf{S}$ is actually the matrix that transforms from the coordinate system in which $\mathbf{B}$ represents our transformation into the coordinate system where $\mathbf{A}$ represents our transformation [VERIFY!].

%https://en.wikipedia.org/wiki/Change_of_basis



\medskip
A similarity transformation does not change the basis independent properties of a matrix. In particular, it doesn't change the characteristic polynomial and therefore the eigenvalues, which are the  roots of said polynomial, are invariant under the transformation. Furthermore, it also doesn't change the determinant and trace because they are the product and sum of the eigenvalues respectively. The multiplicities (geometric and algebraic) of the eigenvalues are also invariant. The eigenvectors are mapped via the change of basis transformation $\mathbf{S}$: If $\mathbf{a}$ is an eigenvector of  $\mathbf{A}$, then $\mathbf{b} = \mathbf{S a}$ is the corresponding eigenvector of $\mathbf{B}$ [VERIFY! It might be $\mathbf{S}^{-1}$]. Note that the implication (similar $\Rightarrow$ same eigenvalues) is only one way. It doesn't hold in the other direction - two matrices with equal eigenvalues may nevertheless be dissimilar. Other features that are invariant under a similarity transform are: rank, index of nilpotence, Jordan normal form (up to permutation of the blocks), Frobenius normal form, minimal polynomial and elementary divisors. Don't worry, if you don't know what some of these terms mean. I don't know all of them either and just wanted to list them all for completeness (the list is taken from Wikipedia). [Q: What other transformations do not change the characteristic polynomial? Are there any? Or are similarity transformations the only kind of transformation with that property?]

% https://en.wikipedia.org/wiki/Matrix_similarity
% https://math.stackexchange.com/questions/255172/eigenvectors-of-similar-matrices

% Explain how \mathbf{S^{-1} A S} works in 3 steps when being applied to a vector. We first
% transform into the new basis via S, then apply A, then transform back into the old basis via
% S^-1. Verify it all with numeric examples


% explain hwo the product S^-1 A S can be interpreted in terms of hwo it acts on a vector: chaneg basis via S, apply the transformation via A, change back to the old basis via S^-1

\medskip
Now, this is a rather implicit definition "there exists a matrix $\mathbf{S}$ such that..." and in practice, we may want to know (1) How can we decide, whether or not such a matrix exists? (2) If it does exist, is it unique? (3) If it is unique, what is it? If it isn't unique, what matrices are possible? Generally, if two matrices are similar, they represent the same geometric transformation but expressed in different coordinate systems. The matrix $\mathbf{S}$ is the matrix that transforms from one coordinate system to the other [TODO: be more specific: which way?] and is called the \emph{change of basis} matrix. So, in a typical practical situation, this matrix $\mathbf{S}$ will probably be given to us so we don't really need to compute it from $\mathbf{A,B}$. But if we really encounter a situation where we need to, this is how it can be done:

[...TBC...ToDo: give algorithm to compute $\mathbf{S}$, given $\mathbf{A,B}$. Maybe use high-level pseudo code and refer to an actual (to be written) implementation in the C++ codebase ]


% How to find P: write the relation as PA = PB  ->  (PA - PB) = 0  ->  P(A-B) = 0
% nah! only true if PA = AP, in general we have
% PB = AP
% https://math.stackexchange.com/questions/14075/how-do-i-tell-if-matrices-are-similar
% says the solution may not be unique - the system to solve may be singular. But the zero matrix is always a solution. A solution algo that computes a minimum norm solution would probably pick the zero matrix? ...yeah - it can't be unique because we see immediately that P could be multiplied by any scalar factor. Maybe pick a solution that has unit norm...I guess this would mean unit-Frobenius norm? 


% https://en.wikipedia.org/wiki/Matrix_similarity
% https://mathworld.wolfram.com/SimilarMatrices.html

% https://en.wikipedia.org/wiki/Matrix_congruence
% a stronger notion of similarity where S^{-1} = S^T

% https://en.wikipedia.org/wiki/Matrix_equivalence
% a weaker(!) notion than similarity. the equation is the same but applies also to non-square
% matrices

%\paragraph{Commuting Matrices}

% Similar Matrices, Change of Basis Matrices, and Diagonalisable Matrices (Linear Algebra)
% https://www.youtube.com/watch?v=bicwfCavVgU

% Change of basis | Chapter 13, Essence of linear algebra
% https://www.youtube.com/watch?v=P2LTAUO1TdA&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=13

\paragraph{Simultaneously Diagonalizable Matrices}
Two matrices $\mathbf{A}$ and $\mathbf{B}$ are said to be simultaneously diagonalizable if there exists a single matrix $\mathbf{P}$ such that $\mathbf{P}^{-1} \mathbf{A} \mathbf{P}$ and  $\mathbf{P}^{-1} \mathbf{B} \mathbf{P}$ are both diagonal matrices. For that to happen, the matrices $\mathbf{A}$ and $\mathbf{B}$ must have the same eigenvectors because in a diagonalization, these eigenvectors appear as the columns of the diagonalizing $\mathbf{P}$ matrix [VERIFY]. Two matrices commute if and only if they are simultaneously diagonalizable. A set $A$ of matrices is simultaneously diagonalizable if all pairs of matrices in the the set are simultaneously diagonalizable.  


\medskip
Note how the features of being \emph{similar} and being \emph{simultaneously diagonalizable} are complementary: If two matrices are similar, then they have the same eigen\emph{values} (this implication goes only one way, by the way). If two matrices are simultaneously diagonalizable, then they have the same eigen\emph{vectors} (Q: Is this implication also only one way?). One might be tempted to conclude that, if two matrices have both features, i.e. are simliar \emph{and} simultaneously diagonalizable, then they must have the same eigenvalues \emph{and} the same eigenvectors and hence they must in fact be the same matrix. But I think, that conclusion would be false because even if two matrices have the same eigenvalues and eigenvectors, the matrices may still be different because the way in which the eigenvalues and eigenvectors are associated may be different. [Figure this out! I think, the diagonalizing matrices must be related by a permutation of columns?]

% But maybe if two matrices have the same eigenvalues and eigenvectors, their diagonalizing matrices are related in a simple way? Or the matrices themselves have some simple relation? I think, the diagonalizing matrices must be related by a permutation of columns - because the order of the columns is what establishes the pairing between eigenvectors and -values.

% I think, a diagonalizing matrix S of a matrix A is *not* unique anyway unless we specify that the values on the diagonal matrix must be in a certain order (say ascending). Different diagonalizing matrices (related by column permutaions, I think) would give rise to different ordering of the eigenvalues along the diagonal.

%No  - I think, the conclusion is false. Even if tow matrices have the eigenvalues and the same eigenvectors, they may still be different matrices because the way in which the eigenvalues and eigenvectors are paired may be different!

%\medskip

% Two matrices with the same eigenvalues can be dissimilar nonetheless:
% https://www.youtube.com/watch?v=MOn7cLZQH-0

% so, that means: similar matrices have equal eigenvalues and simulatneously diagonalizabel matrices have the same eigenvectors?

% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Simultaneous_diagonalization
% https://en.wikipedia.org/wiki/Triangular_matrix#Simultaneous_triangularisability

% https://en.wikipedia.org/wiki/Matrix_mechanics#Matrix_basics
% Seems like matrices are simultaneously diagonalizable, if they have the same eigenvectors. ...yeah - that makes sense in the lght of the equation above




\paragraph{Misc Special Matrices}
% Toeplitz, circulant, unitary (maybe file under orthogonal - it's the complex version), tringular, ...

% https://en.wikipedia.org/wiki/Involution_(mathematics)

% https://en.wikipedia.org/wiki/Normal_matrix
% https://en.wikipedia.org/wiki/Triangular_matrix#Unitriangular_matrix

% https://en.wikipedia.org/wiki/Matrix_congruence

% Maybe sort the features according to arity:
% unary: A is symmetric, unitary,... 
% binary A and B are similar, congruent, ...

% https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra


% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
% https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix




%===================================================================================================
\subsection{Lesser Known Definitions}
%There are some lesser known operations on matrices which we will introduce here.

%---------------------------------------------------------------------------------------------------
\subsubsection{Block Matrices}
Matrices whose entries are themselves also matrices (and not numbers as usual) are called block matrices. For example, such a block matrix could look like:
\begin{equation}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} & \mathbf{C} \\
\mathbf{D} & \mathbf{E} & \mathbf{F} 
\end{pmatrix}
\end{equation}
The constituent matrices may or may not be of equal shape, but the shapes must be related in such a way that the tiling fits together [VERIFY]. For example, we could have $\mathbf{A} = 2 \times 4$,  $\mathbf{B} = 2 \times 2$, $\mathbf{C} = 2 \times 3$, $\mathbf{D} = 3 \times 4$, $\mathbf{E} = 3 \times 2$, $\mathbf{F} = 3 \times 3$. However, when we want to add or multiply two block matrices, not only must the outer matrix shapes be compatible but also the shapes of all the inner element-matrices. That may then actually constrain the shapes of the inner matrices some more. The constraints on the shapes really depend on what types of operations we want to perform with such block matrices. ...TBC...

% https://en.wikipedia.org/wiki/Block_matrix

%---------------------------------------------------------------------------------------------------
\subsubsection{Vectorization of a Matrix}
The vectorization of a matrix $\mathbf{A}$ is denoted by $\vectorize(\mathbf{A})$ and is an operator that turns an $m \times n$ matrix into an $mn$-dimensional column vector by vertically stacking the columns of the matrix on top of each other. A row-wise vectorization can be obtained by $\vectorize(\mathbf{A}^T)$ so we don't define a special operator for that. 

% explain "reshape" functions for matrices in software and what role the memory layout plays (row major vs column major)

%If we identify $k$-dimensional vectors with $k \times 1$ matrices, we may actually consider this as a special case of a reshaper operation

...TBC...

% https://en.wikipedia.org/wiki/Vectorization_(mathematics)


% IIRC there was some youtube video about conics that had an interesting equation involving the vec
% operator

% https://en.wikipedia.org/wiki/Matrix_calculus
% https://www.statlect.com/matrix-algebra/vec-operator

% Is there also an inverse - a matricization of a vector?
% I think, this is a special case of (tensor) reshaping
% https://en.wikipedia.org/wiki/Tensor_reshaping#Mode-m_Flattening_/_Mode-m_Matrixization

%---------------------------------------------------------------------------------------------------
%\subsubsection{Other Products}
%The matrix product as defined above is by far the most important "product-like" operation between matrices. But there are some other, too.

% ToDo: list common features such as associativity and distributivity, if applicable
% maybe move this below the "features" section because it references features such as rank, det
% eigenvalues. etc.

%---------------------------------------------------------------------------------------------------
\subsubsection{Hadamard Product} The Hadamard product of two $m \times n$ matrices is just the element-wise product. It is associative, commutative and distributive over addition. It is denoted by $\mathbf{A} \odot \mathbf{B}$ and satisfies

\medskip
\begin{tabular}{l l l l}
Determinant: & $\det(\mathbf{A} \odot \mathbf{B})$ 
             & $\geq \;\; \det(\mathbf{A})  \det(\mathbf{A})$   
             & when $\mathbf{A,B}$ are positive semidefinite \\
Rank:        & $\rank(\mathbf{A} \odot \mathbf{B}) $
             & $\leq \;\; \rank(\mathbf{A}) \rank(\mathbf{A})$     \\
\end{tabular}
\medskip

This product may also be called the \emph{naive matrix product}. Its applications are not so much in the realm of linear algebra but more in areas like numerical processing of 2D array data such as in image processing where we often want to do element-wise array operations. 

% ToDo: give more applications
% -in numerical processing, we often need element-wise array operations

% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)

%---------------------------------------------------------------------------------------------------
\subsubsection{Kronecker Product} The Kronecker product between an $m \times n$ matrix $\mathbf{A}$ and a $p \times q$ matrix $\mathbf{B}$ is an $mp \times nq$ matrix $\mathbf{C}$ in which each element is a product of one element from $\mathbf{A}$ and one element from $\mathbf{B}$. ...TBC..

% can be seen as inserting a scaled copy of the rightfactor as submatrix into the result where the scaling factor is taken from the left factor. show this by two 2x2 matrices

% https://en.wikipedia.org/wiki/Kronecker_product
% https://de.wikipedia.org/wiki/Kronecker-Produkt

% I think, for square matrices A (m x m), B (n x n) we have:
% -(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}
% -If A has eigenvalues \lambda_i, i=1,..,m and B has eigenvalues \mu_j, j = 1,..,n then C has
%  eigenvalues \lambda_i \mu_j

%\paragraph{Khatri-Rao Product} This is variation of the Kronecker product. ...
%\paragraph{Tracey-Singh Product} This is another variation of the Kronecker product. ...


% https://en.wikipedia.org/wiki/Kronecker_product#Related_matrix_operations
% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#The_mixed-product_property
% https://en.wikipedia.org/wiki/Khatri%E2%80%93Rao_product#Face-splitting_product
% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#The_penetrating_face_product
% https://en.wikipedia.org/wiki/Frobenius_inner_product
% https://en.wikipedia.org/wiki/Pointwise_product
% what about convolution?
% https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_operator

% -mention approximation of matrix as weighted sum over Kronecker products of vectors
% -maybe this is also known as tensor decomposition? figure out!
%  https://en.wikipedia.org/wiki/Tensor_rank_decomposition
% -Try to approximate mxn matrix A as a weighted sum over Kronecker products of vectors in a 
%  least squares sense. ..has to do with separable filter kernels in image processing. Maybe make % %  the ansatz 
%    A ~ sum_{k=1}^n u_k \otimes v_k  or  
%    A ~ sum_{i=1}^m sum_{j=1}^n w_{ij} u_i \otimes v_j
%  where u_i v_j are vectors and w_{ij} are scalar weights

% Maybe try to approximate a 4x4 matrix C as Kronecker product of 2 2x2 matrices A,B
%
%      c11 c12 c13 c14
% C =  c21 c22 c23 c24   A = a11 a12   B = b11 b12
%      c31 c32 c33 c34       a21 a22       b21 b22
%      c41 c42 c43 c44
%
% kr(A,B) = a11*b11 a11*b12  a12*b11 a12*b12
%           a11*b21 a11*b22  a12*b21 a12*b22
%
%           a21*b11 a21*b12  a22*b11 a22*b12
%           a21*b21 a21*b22  a22*b21 a22*b22
%
% We want (c11-a11*b11)^2 + (c12-a11*b12)^2 + ... = min. This can perhaps be done using gradient
%% descent.

% https://www.youtube.com/watch?v=i0cp3iQXSk8
% defines a "tilt product" between two (unit) vectors: tilt(u, v) = v  u^T - u v^T. So it's
% kind of like the commutator of the outer product of two vectors. Yields a matrix which can
% be used to encode rotations via the matrix exponential. For orthogonal u,v, there's the
% closed form formula for a rotation by an angle theta: 
%   exp(tilt(u,v)) = I + sin(theta)*tilt(u,v) + (1-cos(theta))*(tilt(u,v))^2
% The tilt product gives a matrix which projects vectors onto the uv-plane *and* rotates them
% in this plane by the angle between u and v. He calls the formula "Generalized Euler's"
% formula. It's also similar to Rodrigues rotation formula
% It has also some formulas for tze matrix epxonential


%===================================================================================================
\subsection{Important Facts and Formulas}

\paragraph{Spectral Theorem}
% -spectral theorem: https://www.youtube.com/watch?v=4zD8Kd3HgJA
%  https://en.wikipedia.org/wiki/Spectral_theorem

\paragraph{Transposition Formulas}
\begin{equation}
(\mathbf{A_1 A_2 \ldots  A_n})^T = \mathbf{A_n}^T \ldots \mathbf{A_2}^T \mathbf{A_1}^T
\end{equation}


\paragraph{Inversion Formulas}
\begin{equation}
(\mathbf{A_1 A_2 \ldots  A_n})^{-1} = \mathbf{A_n}^{-1} \ldots \mathbf{A_2}^{-1} \mathbf{A_1}^{-1}
\end{equation}

% https://en.wikipedia.org/wiki/Woodbury_matrix_identity
% https://tlienart.github.io/posts/2018/12/13-matrix-inversion-lemmas/index.html
% https://www.statlect.com/matrix-algebra/matrix-inversion-lemmas
% http://www0.cs.ucl.ac.uk/staff/g.ridgway/mil/mil.pdf

% https://stattrek.com/matrix-algebra/matrix-theorems

\paragraph{Rank Nullity Theorem}

% -rank-nullity theorem
% https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem

\paragraph{Cayley Hamilton Theorem}

% https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem

% -matrix inversion lemma

% Matrix exponential - formula with commutators

%===================================================================================================
%\subsection{Computing with Spaces}


%On a beginner level, one usually assumes to deal regular matrices and as soon as one encounters a singular matrix, one throws the towel

%and just says things like "there are no solutions"

% Computing with spaces - operations like perp (orthogonal complement)

\begin{comment}
Explain what happens to the eigenvalues when we do certain things to a matrix (shifts, etc.)
I have a list of that in some text file. Shifting eigenvalues and manipulating them in other ways
can be important to improve convergence of numerical algrithms

Other possibly relevant matrix types to mention:
https://en.wikipedia.org/wiki/Companion_matrix
https://en.wikipedia.org/wiki/Smith_normal_form



Make a section for "Functions of Matrices" - or soemthing like that. Explain the matrix exponential
and hwo to compute it in practice. See:

Der Putzer-Algorithmus, den kaum jemand kennt, zur Bestimmung der Matrixexponentialfunktion
https://www.youtube.com/watch?v=zUcwmGWh2UA
-Maybe that should go into a matrix calculus chapter
-d/dt e^{t A} = A e^{t A}

....also explain computation via diagonalization

https://en.wikipedia.org/wiki/Matrix_differential_equation


\end{comment}