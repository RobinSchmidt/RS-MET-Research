\section{Vectors and Matrices}

\paragraph{Vectors}
Consider a point in the 2D plane or in 3D space. To represent such a point mathematically, we would need a pair or a triple of numbers respectively. An $n$-dimensional vector can generally be thought of as an $n$-tuple of numbers. For intuition building, it's best to think of real numbers although in general, other kinds of numbers may also be allowed in the more general case. Such a vector can not only represent a position in space but also a translation. Imagine the $xy$-plane. The vector $(x,y) = (3,2)$ could encode the action of first going $3$ units into the $x$-direction (usually to the right) and then going $2$ units into the $y$-direction (usually upward). Of course, we could as well go first the $2$ units into the $y$-direction and the $3$ units into the $x$-direction - the location we would end up would be the same in both cases. Vectors are usually denoted as columns. For example, a vector $\mathbf{v}$ with given $x$ and $y$ values is written as:
\begin{equation}
\mathbf{v} 
= \begin{pmatrix} x \\ y \end{pmatrix} 
=   x \cdot \begin{pmatrix} 1 \\ 0 \end{pmatrix} 
  + y \cdot \begin{pmatrix} 0 \\ 1 \end{pmatrix} 
\qquad \text{or}  \qquad
\mathbf{v} = (x,y) = x \cdot (1, 0) + y \cdot (0, 1)
\end{equation}
where the right hand sides can be seen as an interpretation of what the tuple $(x,y)$ actually encodes. We imagine $x$ to be a scaling factor (aka coefficient) of a unit vector $(1,0)$ into the $x$-direction and $y$ to be a coefficient for a unit vector $(0,1)$ into the $y$-direction. The left and right versions are just different notations. The right notation using tuples is more convenient when embedding an equation in a text or when vertical space shall be saved. In non-embedded equations, most books will usually use the left notation where vectors are written as columns with the components stacked vertically. 

%You may also (rarely) see the notation $\mathbf{v} = (x \;\, y)^T$, i.e. like tuple notation but without the comma and a superscript $T$. This $T$ stands for \emph{transposition} which, in this case, means to turn the row into a column.

% Explain notation: when the components are separated by a comma, we see the vectors as tuples. Without commas, we could write them as $(1 0)^T$ where the superscript $T$ in $(\ldots)^T$ stands for \emph{transposition} which, in this case, means to turn the row into a column.

...TBC...

% Give the standard basis (1,0),(0,1) of R^2, explain how any vector can be expressed as a linear combination of them. done

% Explain the uniot basis vectors and the zerop vector

% Explain how the sum is "formal" in an algebraic sense but real in a geometric sense. Or ...well...it is actually also real in an algebraic sense - we can collapse it into a single vector, namely the vector (x,y). Writing it as such a sum can actually be interpreted as decomposition into a purely horizontal and a purely vertical component. These components, when added together in a sense that is soon to be defined, give back the original vector. 

%A vector may not only represent a point itself, but also a translation ...tbc...


\paragraph{Matrices}
Vectors can be used to represent locations and translations. Translations are a specific kind of geometric transformation. A matrix represents a different kind of transformation, namely a \emph{linear transformation}, that we can apply to a vector. In geometric terms, linear transformations are scalings, rotations and shears. In this point of view, the columns of the matrices tell us, where the standard basis vectors will get mapped to (we'll see later what these are).

...TBC...


% A matrix is a 2D array of numbers, i.e. a table with rows and columns. Each cell contains a number pretty much like in a spreadsheet.
% the columns of the matrix say where the basis vectors go

% We need to define what the identity matrix is. Maybe the zero matrix, too

% Translations are not among the linear transformations that we can represent by matrices.


\paragraph{Well, actually...}
Strictly speaking, an $n$-tuple of numbers is not what a vector really \emph{is} by its nature. By its nature, a vector is a geometric entity such as a point in space or an arrow with a direction and length. The $n$-tuple of numbers is a specific \emph{representation} of that geometric entity that depends on the coordinate system that we have chosen. But you may take that statement as a foreshadowing to a more advanced viewpoint. For the purposes of this section, it's totally okay to picture a vector as a tuple of numbers. Likewise, matrices are also only a specific representation of a linear transformation. In a different coordinate system, the same transformation would be represented by a different 2D array of numbers. We will say more about this in the context of so called \emph{change of basis} transformations.

\medskip
Moreover, when we look at things more carefully, we need to distinguish between \emph{points} and \emph{vectors}. A point represents just a location and it does not really make any sense to add a location to another location. What is Berlin plus Paris supposed to mean? It makes no sense - even when expressed as geolocations! (By the way: I'm conveniently assuming a flat map here. Taking the spherical earth into account is a further complication which is of no relevance to my point). A vector, on the other hand, represents an arrow which we interpret as a translation. It does indeed make sense to add translations: we just perform one after the other. We may first go 30 km east and 20 km north and thereafter go 40 km east and 50 km north - that makes perfect sense. However - in practice (like in vector graphics APIs), points are usually identified with their \emph{position vectors} (aka location vector or radius vector), i.e. vectors that point from the origin to the given point. Represented as such, we actually can add points in our graphics code - we just should ask ourselves, if it makes any sense to do so in our particular case.
% ...TBC...

% https://en.wikipedia.org/wiki/Position_(geometry)

%===================================================================================================
\subsection{Vector Operations}

\paragraph{Scalar Multiplication}
Scalar multiplication refers to the act of multiplying a vector $\mathbf{v}$ by a scalar, i.e. a number, let's say $a$. This is just done by multiplying every component of the vector by that number. Geometrically, this scales the length of the vector by a factor of $a$. That's where the name "scalar" comes from. You may interpret it as "scaler". If $a > 1$, the vector just gets longer. If $a < 1$ but still $a > 0$, the vector gets shorter but retains its direction. If $a < 0$, the vector reverses direction in addition to being lengthened or shortened (in the special case of $a=-1$, it gets only reflected). If $a = 0$, the vector actually gets collapsed into the zero vector $\mathbf{0}$.

\paragraph{Vector Addition}
Algebraically, there is not much to say about vector addition - we just add the tuples component-wise and that's it. We can interpret such a vector addition geometrically at placing the vectors tip to tail. ...TBC... [ToDo: insert figure with the parallelogram picturing $\mathbf{a + b}$ and $\mathbf{b + a}$].

% What about when dimensionalities don't match? Maybe we can zero-pad the shorter vector in certain
% cases. Geometrically, this would mean to iamgine the low-dimensional vector spcae as being 
% embedded into a higher dimensional space by setting the extra coordinates in the higher
% dimensional space to zero. For example, a 2D vector (x,y) could be augmented to a 3D vector
% (x,y,z) by just setting $z=0$. Check how graphics libraries like OpenGL handle that.
% 

\paragraph{Vector Subtraction}
% What about subtraction? Algebraically, it's just addition of the negative but geometrically, it
% has a meaningful interpretation, so maybe it should get is won section

% maybe generalize to "Linear Combinations"

\paragraph{Linear Combinations}
A linear combination of a bunch of vectors is just a weighted sum of those vectors. That means each vector gets multiplied by a weight via our scalar multiplication and then these scaled vectors are added up via our vector addition. Vector subtraction is also just a special case of a linear combination of two vectors where one vector gets a weight of $+1$ and the other a weight of $-1$. 



\paragraph{Inner Product aka Scalar Product}
The \emph{inner product}, also known as the \emph{scalar product} between two vectors $\mathbf{v}$ and $\mathbf{w}$ is just a number. It is computed by taking all the component-wise products and summing them up. ...TBC...
% explain geometric interpretation
% -coefficient of projection
% -correlation in the sense of: how similar is the direction of the vectors (when bot are
%  normalized)
% Mention different (more general) definitions: 
% -with complex conjugation,
% -with a (positive definite) matrix sadwiched in between

%\paragraph{Vector Products}
% cross-product, triple-product (but this gives a scalar), wedge-product

\paragraph{Orthogonal Projection}

\paragraph{Vector Decomposition}
% We have already seen how a vector $(x,y)$ can be decomposed into a purely horizontal and a purely vertical component. That decomposition can be generalized.
% we project the vector onto one other vector to obtain it's component
% We often choose the vectors into which we decompose a given vector to be orthogonal but that doesn't need to be the case. ...but I think, the general decomposition cannot be obtained by simple projections. I think, we really need to solve a linear system of equations for this.

% https://ccrma.stanford.edu/~jos/st/Projection_onto_Non_Orthogonal_Vectors.html

% https://math.stackexchange.com/questions/1882096/how-to-decompose-a-vector-into-non-orthogonal-components


\paragraph{Outer Product}
\paragraph{Cross Product}
\paragraph{Wedge Product}
\paragraph{Triple Product}






% Norm, Normalization

% Outer product (is actually a special case tensor product or Kronecker product)

% Inversion
% Divide by its length twice. Normalize and then divide by the former length. Reciprocate the length
% but retain direction. Geometrically, it's inversion in a (hyper)sphere

% In linear algebra, we mostly deal with linear combinations, the scalar product and orthogonal projections. In analytic geometry of 3D space, the cross product is also common and the triple product will occasionally be seen. The rest is less common but included for completeness sake and will be picked up in later sections. The wedge product is important in exterior and geometric algebra. The outer product will be revisited in tensor algebra (in a different guise).

%===================================================================================================

\subsection{Matrix Operations}
\subsubsection{Addition and Subtraction}

%---------------------------------------------------------------------------------------------------
\subsubsection{Multiplication} In order to multiply two matrices, we must demand that the number of columns of the left factor must be equal to the number of rows in the right factor. When we multiply an $m \times p$ matrix $\mathbf{A}$ with a $p \times n$ matrix $\mathbf{B}$ and call the product $\mathbf{C}$ such that $\mathbf{A} \mathbf{B} = \mathbf{C}$, then the element $c_{ij}$ of the product can be computed from the elements $a_{ij}, b_{ij}$ of $\mathbf{A}, \mathbf{B}$ as follows:
\begin{equation}
 \mathbf{C} = \mathbf{A} \mathbf{B} 
 \quad \text{where} \quad
 c_{ij} = \sum_{k=1}^p a_{ik} b_{kj}
\end{equation}
The matrix $\mathbf{C}$ will have a shape of $m \times n$. Note that the element $c_{ij}$ can be interpreted as a scalar product of the $i$-th row of $\mathbf{A}$ with the $j$-th column of  $\mathbf{B}$ [VERIFY!].

% -maybe add also the Kronecker product and list formulas for its connection with the regular
%  product
% -maybe explain block matrices
% -But maybe put these things into a section Lesser Known Stuff
%  ...although, it could actually also fit into the "decomposition" section
% -The columns of a matrix may be interpreted as the images of the standard basis vectors. But maybe
%  this should go to a section matrix-vector operations. It may contain products like A x, x^T A,
%  x^T A x (sandwich product)
% -The matrix product can be seen as being made up from scalar products of the rows of the left 
%  factor with the columns of the right factor

%---------------------------------------------------------------------------------------------------
\subsubsection{Inversion}
% Could be done via Gauss-Jordan Elimination Algorithm decribed in the section about solvin linear 
% systems

% Misc: transposition, hermitian tranpose, etc.



%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix-Vector Products}
We have seen operations involving two vectors as well as operations involving two matrices. From a geometric perspective, linear algebra really comes to life when considering the interplay between vectors and matrices. Vectors encode positions and therefore also geometric shapes which are defined by the positions of their vertices. Matrices encode geometric transformations such as reflections, rotations, scalings, etc. We can interpret such a transformation as an \emph{action} that is applied to a shape, i.e. to a set of vectors. A matrix can \emph{act on} a vector or - in passive language - can \emph{be applied to} the vector. This action is effected by forming the \emph{matrix-vector product}. We actually already have everything we need. We do not need to define any brand new product for this. Instead, we observe that column vectors can be interpreted as special matrices - namely as matrices that just have a single column, i.e. as $n \times 1$ matrices where $n$ is the dimensionality of the vector. We can multiply such an $n$-vector $\mathbf{v}$, seen as $n \times 1$ matrix, from the left with an $n \times n$ matrix  $\mathbf{A}$ using the rules of matrix multiplication to get another $n \times 1$ matrix, i.e. another $n$-vector $\mathbf{w} = \mathbf{A v}$. Forming such a product between a matrix as left factor and a column vector as right factor will be our usual way of expressing geometric transformations. 

%\medskip
\paragraph{Left vs Right Multiplication}
An entirely equivalent formalism for geometric transformations can be formulated using row vectors, i.e. $1 \times n$ matrices and products where the matrix factor appears as the right factor. To switch from one formalism to the other requires to transpose all matrices and write all products in reverse order [VERIFY!]. The advantage of that would be that we can read such products from left to right and that reading direction corresponds to the order in which the transformations are applied. However, we will stick to column vectors and multiplying by matrices from the left because that's more common in mathematics. 

% https://math.stackexchange.com/questions/2738278/what-exactly-is-a-left-multiplication-transformation
% https://medium.com/geekculture/right-and-left-matrix-multiplication-d21947f195d8
% https://math.stackexchange.com/questions/3252146/linear-transformations-and-left-multiplication-matrix

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Left_and_right_eigenvectors

%\medskip
\paragraph{Dimensionality Conversions}
The rules of matrix multiplication would also allow to use an $m \times n$ matrix - that is, the dimensionality $m$ of the output vector could in general be different from the dimensionality $n$ of the input vector. If $m < n$, we would get a dimensionality reduction. That means, some sort of projection must be involved in the transformation and information will be discarded by forming the product. This is quite common in graphics when we eventually need to project a 3D scene onto a 2D screen. The $m > n$ case would mean that our $n$-vector lives in a (sub)space of lower dimensionality that is embedded in some higher dimensional space. In such a case, the matrix-vector product may lift the vector out of its subspace.

%\medskip
\paragraph{Sandwich Products}
Another kind of product that can be formed between vectors and matrices is sometimes called the \emph{sandwich product}. It takes as inputs an $m \times 1$ vector $\mathbf{w}$, an $m \times n$ matrix $\mathbf{A}$ and an $n \times 1$ vector $\mathbf{v}$ and produces the product $\mathbf{w}^T \mathbf{A v}$. The transposition of $\mathbf{w}$ into $\mathbf{w}^T$ turns it into an $1 \times m$ row vector which is required to make the matrix shapes (vectors are special matrices!) compatible for multiplication. Due to associativity of matrix multiplication, it doesn't matter if you evaluate it as $\mathbf{w}^T (\mathbf{A v})$ or as $(\mathbf{w}^T \mathbf{A}) \mathbf{v}$. The result will be the same in both cases and it will be a scalar, i.e. just a number. Sandwich products are important in the definition of quadratic forms which in turn are important in defining some basic geometric shapes (ellipsoids, hyperboloids, paraboloids) and also in optimization and approximation applications. These sandwich products also appear a lot in quantum mechanics and they may serve as a formalism for generalizing the scalar product especially in the context of differential geometry.  There, the regular scalar product, which we normally use to compute the squared length of a vector, is replaced by such a sandwich product involving the so called metric tensor - which is basically just the matrix that you sandwich into the regular scalar product. [VERIFY if we also use the sandwich product to compute angles between vectors in DG or if we use it just for the length]

TODO: explain kernel an image of a matrix

%generalizations of the scalar product (especially in differential geometry) and also in 

% explain the ^T notation

%...TBC...TODO: mention sandwich product $\mathbf{w}^T \mathbf{A v}$.

% Left-product, right -product, sandwich product



%===================================================================================================
\subsection{Vector Features}

\paragraph{Vector Norms}
The norm of a vector captures its length. The most common norm is the Euclidean norm but there are others as well...TBC...
% are a measure of length, $L_p$-norm, explain the general requirements to a norm
% norm equivalence


\paragraph{Orthogonality and Angles}
% Angles between a are defined in terms of the cosine of the norm
% 


\paragraph{Span of a Set of Vectors}
As the span of a set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$, we define the set of all vectors that can be formed by arbitrary linear combinations of those vectors, i.e. the set of all vectors that can be written as:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_n \mathbf{v}_n 
\end{equation}
for some set of scalar coefficients $a_1, \ldots, a_n$.


\paragraph{Linear Independence}
A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$ is said to be linearly independent, if none of its elements can be expressed as a linear combination of other elements. A necessary and sufficient mathematical condition for linear independence is that the equation:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_n \mathbf{v}_n = \mathbf{0}
\end{equation}
can be satisfied only if all the $a_i$ are equal to zero, i.e. only the trivial solution for the $a_i$ exists [VERIFY]. Linear independence is an important condition for a set of vectors to have. In a sense, it means that all vectors are needed and none of them is redundant. 

%If a set of vectors is linearly independent, it means that removing one of the vectors would necessarily reduce the \emph{dimension} of their span by one. 
%\newline TODO: define the term "dimension" formally.

% TODO: explain basis (ordered or not)

\paragraph{Dimension}
The sense in which none of the vectors is redundant in a linearly independent set of vectors is the following: If we would remove any of the vectors, the space spanned by the remaining set vectors would get reduced in dimension (by one). The \emph{dimension} of a space is the smallest number of vectors that is needed to span that space, i.e. to make any point in the space "reachable" via a linear combination of these vectors.

\paragraph{Basis}
A \emph{set} $\{\mathbf{v}_1, \ldots, \mathbf{v}_n \}$ of linearly independent vectors that spans a given space is called a \emph{basis} for that space. A \emph{tuple} $(\mathbf{v}_1, \ldots, \mathbf{v}_n)$ of such vectors is called an \emph{ordered basis}.

TODO: Explain the standard basis


\paragraph{Orthogonalization via Gram-Schmidt Algorithm}


% TODO: explain how we could order a given basis:
% Idea: 
% -Compute for each vector in the basis the center of gravity / center of mass 
% -To do this, use the absoulte values of the entries
% -Order the basis vectors according to ascending center of gravity.
% -The idea is that the ordered basis kind of "roughly resembles" the standard basis. If the algo
%  gets the standard basis in shuffled order as input, it will return it in the correct order.
% -What about a basis like (1,1), (1,-1)? the center of mass is the same for both (at "index" 0.5)
%  Maybe in such a case, we should also consider angles: choose the vector that aligns best with
%  the standard basis vector of that index. If more of them align best, choose the one with
%  positive angle
%
% Try to find a way to canonicalize a basis B = b_1,..,_b_n. That process could involve (in that
% order)
% -Ordering of the vectors according to center of weight
% -Tweaking the initial vector b_1 to align most closely with e_1 = (1,0,0,...) without leaving
%  the span of b_1,...,b_n. That means: if e_1 is actually in the span of B, then the updated b_1
%  will indeed become e_1 itself
% -Gram-Schmidt orthogonalization of the remaining vectors b_k, k = 2,...,n
%  -Maybe in each step, try to align the vector b_k as closely as possible with e_k
% 


%===================================================================================================
\subsection{Matrix Features}


%---------------------------------------------------------------------------------------------------
\subsubsection{Special Properties}
% Maybe move that below the characteristioc numbers and eigenspaces subsections because some 
% properties may be defined in terms of these numbers (e.g.: contractive: all eigvals < 1 in 
% abs-val)
% By a property of a matrix, I mean a feature thata matrix may or may not have - a boolean feature, so to speak.

\paragraph{Squareness}
A matrix is called a square matrix, if it has the same number of rows as it has columns. The general rectangular array of elements forms a square.

\paragraph{Symmetry}
A matrix is symmetric if it is equal to its own transpose. That means that the rows are equal to the columns. Obviously, symmetry is a feature that only square matrices can possibly have.
% Give fomrula for invers of transpose and/or for (AB)^T = B^T A^T iirc

% boolean fetaures: symmetric, orthogonal, nilpotent, idempotent, unitary, normal, etc.
% discrete features: rank, index of nilpotency, signature
% scalar: determinant, condition number
% list of scalars: eigenvalues, singular values
% list of vectors: (left/right) (generalized) eigenvectors, singular vectors
% boolean features of pairs of matrices: similarity (is A similar to B), simultaneously
%   diagonalizable,
% matrix: set of similar matrices,

% maybe put these "boolean" features into a subsection
\paragraph{Orthogonality}
% a (square) matrix is orthogonal when all its rows are mutually orthogonal. This implies

\paragraph{Definiteness}

% -contractive (all eigenvalues < 1 in absolute value)

%---------------------------------------------------------------------------------------------------
\subsubsection{Characteristic Numbers}

\paragraph{Rank}
The column rank of a matrix is the number of linearly independent columns

% -rank-nullity theorem
% -row-rank equals column rank

\paragraph{Trace}



\paragraph{Determinant}
% -regular/singular
% -scaling factor for a hypervolume
% -product of eigenvalues

\paragraph{Norms}
% explain norm compatibility with vector norms

% trace

%---------------------------------------------------------------------------------------------------
\subsubsection{Eigenspaces}
When forming a product between an $n \times n$ matrix $\mathbf{A}$ and an $n$-vector $\mathbf{v}$, we will get a new $n$-vector, which will, in general, point into a different direction than  $\mathbf{v}$. There may, however, be certain special vectors which do not change direction as a result of such a multiplication by a matrix but rather just (possibly) change their length. Such special vectors are called the \emph{eigenvectors} of the matrix $\mathbf{A}$ and the length-change factor is called the corresponding \emph{eigenvalue}. The equation that expresses this circumstance is given by:
\begin{equation}
 \mathbf{A v} = \lambda \mathbf{v}  
 \quad \Leftrightarrow \quad
 \det ( \mathbf{A} - \lambda \mathbf{I} ) = 0
\end{equation}
which is called the characteristic equation. [the right one, I think]
[VERIFY if implication is two-sided]. TODO: explain what this has to do with the determinant, explain etymology of "eigen"


\paragraph{Eigenvalues and Characteristic Polynomial}
If we expand the left hand side of the characteristic equation $\det ( \mathbf{A} - \lambda \mathbf{I} ) = 0$ according to the rules for how determinants are computed, it will turn out that we will get a polynomial in $\lambda$ of degree $n$. This polynomial is called the \emph{characteristic polynomial} of the matrix  $\mathbf{A}$. The right hand side says that this polynomial should be equal to zero for $\lambda$ to be an eigenvalue. Another way to say that is that an eigenvalue $\lambda$ is a root of the characteristic polynomial. We know that roots of polynomials may have a multiplicity, i.e. in the product form of the polynomial, the linear factor that corresponds to the root may occur multiple times. The multiplicity of the eigenvalue as the root of the characteristic polynomial is called the \emph{algebraic multiplicity} of the eigenvalue and denoted by $\alg(\lambda)$. Note that even for matrices of real numbers, the eigenvalues may be complex. If the matrix is real, then complex eigenvalues will always come in pairs of conjugates [VERIFY!].

...TBC...

% Algebraic multiplicity, geometric multiplicity
% left and right eigenvectors

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors

%\paragraph{Eigenvalues}

\paragraph{Eigenvectors and Eigenspace}
If we have an eigenvalue $\lambda$, we may also want to know the corresponding eigenvector(s). Multiple eigenvectors that correspond to a single eigenvalue may only exist when the algebraic multiplicity is greater than one. The number of eigenvectors that correspond to a given eigenvalue is called the \emph{geometric multiplicity} of the eigenvalue and denoted by $\geo(\lambda)$ and that geometric multiplicity is always at least one and at most equal to the algebraic multiplicity: $1 \leq \geo(\lambda) \leq \alg(\lambda)$. If $\geo(\lambda) < \alg(\lambda)$, the eigenvalue is called a \emph{defective eigenvalue}. A matrix that has defective eigenvalues is called a \emph{defective matrix} [VERIFY!]. Just like eigenvalues, eigenvectors too can be complex even for real matrices. In the case of real matrices, they will also come in complex conjugate pairs. 

\medskip
To find an eigenvector for an eigenvalue $\lambda$, we need to solve the linear system of equations represented by $\mathbf{A v} = \lambda \mathbf{v}$.  This can be rewritten as $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$ which is in the standard form $\mathbf{A x} = \mathbf{b}$ that we need for solving such equations. In this form, $\mathbf{x}$ is the unknown vector that we are trying to find, $\mathbf{A}$ is a known matrix and $\mathbf{b}$ is a known right hand side vector. We will learn later how to solve such equations. At this point, it should only be pointed out that the solution to such an equation is not necessarily a unique vector and indeed, in our case here it can't be unique because eigenvectors can always be scaled at will. That latter statement means that if $\mathbf{v}$ is an eigenvector, then any scaled version of $\mathbf{v}$ will also be an eigenvector. It is therefore common practice to normalize eigenvectors to unit length.

\medskip
An eigenspace of an $n \times n$ matrix $\mathbf{A}$ associated with a given eigenvalue $\lambda$ is defined to be the space spanned by all the eigenvectors that correspond to $\lambda$ [VERIFY!]. Formally, it is the following set of vectors: $E(\lambda) = \{\mathbf{v} \in \mathbb{C}^n : (\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}\}$. 

%In words, that is the set of all vectors that satisfy the equation $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$.

% The eigenvectors may also be complex even for real matrices

%\paragraph{Eigenspaces}
%

% Say something about the space in which the vectors live. If A is a real matrix, the eigenvectors may actually be complex.
% ...but i think, the system is alway singular - we will at least get a 1D continuum of solutions because eigenvectors can be scaled at will.

%If $\alg(\lambda) > 1$, it may happen that this system has no unique solution but rather a whole space of solutions

...TBC...

% Can it happen that the geometric multiplcity is zero? If so - what does it mean?

% If the algebraic multiplicity of $\lambda$ is $1$, then there can be only one such eigen

% Multiple eigenvectors can only exist when the algebraic multiplicity is gretaer than 1

\paragraph{Generalized Eigenvectors} We established that an eigenvector $\mathbf{v}$ with eigenvalue $\lambda$ is a vector with the property $\mathbf{A v} = \lambda \mathbf{v}$ which can be rewritten as $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$. A \emph{generalized eigenvector} with \emph{rank} $r$ is a vector $\mathbf{v}$ that satisfies $(\mathbf{A} - \lambda \mathbf{I})^r \mathbf{v} = \mathbf{0}$. The rank $r \geq 1$ is a natural number and it is understood to be the smallest possible number for which that equation is satisfied, i.e. we also require  $(\mathbf{A} - \lambda \mathbf{I})^{r-1} \mathbf{v} \neq \mathbf{0}$. For $r = 1$, the generalized equation reduces to the defining equation of ordinary eigenvectors. Generalized eigenvectors become important when the geometric multiplicity of an eigenvalue $\lambda$ is less than its algebraic multiplicity, i.e. when we are dealing with defective eigenvalues.   ...TBC...

% what about the edge case $k = 0$. then the matrix becomes then identity matrix and the equation has only the trivial solution. So, could we say that the zero vector is a generalized eigenvector of rank zero? Is that useful? I mean, it's trivial but maybe it completes a general pattern in a satisfying way? I think we have a chain of subspaces of decreasing dimension when dealing wit generalized eigenvectors and the zero vector can indeed be seen a 0-dimensional subspace?

% https://en.wikipedia.org/wiki/Generalized_eigenvector
% https://en.wikipedia.org/wiki/Modal_matrix
% https://en.wikipedia.org/wiki/Modal_matrix#Generalized_modal_matrix


\paragraph{Simpler Special Cases} In the important special case where all eigenvalues are distinct, much of the considerations above simplify considerably. When all eigenvalues $\lambda_i$ are distinct, it means that all the algebraic multiplicities are one and therefore also all the geometric multiplicities are one because of the general rule $1 \leq \geo(\lambda) \leq \alg(\lambda)$. That, in turn, implies that there is a one-to-one correspondence between eigenvalues and eigenvectors. Each eigenvalue has exactly one corresponding eigenvector. Don't make the all too tempting mistake of assuming that this is always so in the general case. It is only so in this special friendly case which is fortunately quite common [VERIFY!]. This distinctness of eigenvalues has the further implication that the matrix is diagonalizable. We'll see later what that means. Suffice to say here that this feature is usually a good thing. For diagonalizability, it is actually already enough if the algebraic and geometric multiplicities of all eigenvalues match: $\forall i: \geo(\lambda_i) = \alg(\lambda_i)$. 

%Don't make the mistake of thinking that there is a one-to-one correspondence between eigenvectors and eigenvalues in general. 

%It is, in general, not the case that each eigenvalue has one and only one corresponding eigenvector. ...TBC...

\paragraph{Invariant Subspaces}
We have learned that an eigenvector of a matrix $\mathbf{A}$ is a vector $\mathbf{v}$ that doesn't change direction under the transformation that is achieved when $\mathbf{A}$ is acting on  $\mathbf{v}$ via the product $\mathbf{w} = \mathbf{A} \mathbf{v}$. We can pick any subset of the set of all (generalized?) eigenvectors and use this subset as a basis for an invariant subspace, i.e. a subspace that is mapped to itself under the matrix A. That the matrix cannot "lift" a vector out of this subspace is clear when we express the vector as linear combination of the selected eigenvectors. Then, each component of the vector in that basis will just be scaled by applying the matrix. No components into other directions will be created. [VERIFY ALL]

% https://en.wikipedia.org/wiki/Invariant_subspace
% https://www.statlect.com/matrix-algebra/invariant-subspace
% https://www.spektrum.de/lexikon/mathematik/invarianter-unterraum/7094

% matrices satsify their own characteristic equation

%---------------------------------------------------------------------------------------------------
%\subsubsection{Properties of Pairs of Matrices}
\subsubsection{Relations between Matrices}
We have seen "boolean properties" (programmer speak) of single matrices, i.e. features that a given matrix may or may not have such as symmetry, definiteness, etc. There are also some of these boolean properties that pairs of matrices may or may not have. Another way to say that is two matrices may or may not be in a certain relation with one another. Some important of such relations will be defined now.

% Could also be called "Relations between Matrices"
% -reciprocity/inversion
% -commutativity
% -bi-orthogonality (?)
% -what about asymmetric relations - maybe something like <, i.e. an order..maybe based on a norm?

\paragraph{Similarity}
A matrix $\mathbf{A}$ is said to be \emph{similar} to another matrix $\mathbf{B}$, if there exists an invertible matrix $\mathbf{S}$ such that $\mathbf{B} = \mathbf{S^{-1} A S}$. We denote this by $\mathbf{A} \sim \mathbf{B}$ which we read as "A is similar to B" [VERIFY!]. Matrix similarity is an equivalence relation, so we have, among other things, a symmetry of the relation: $\mathbf{A} \sim \mathbf{B} \Leftrightarrow \mathbf{B} \sim \mathbf{A}$. We can actually solve for $\mathbf{A}$ by pre-multiplying both sides by $\mathbf{S}$ and post-multiplying both sides by $\mathbf{S}^{-1}$ to obtain $\mathbf{A} = \mathbf{S B S^{-1}}$. The transformation that maps $\mathbf{A}$ to $\mathbf{B}$ is called a \emph{similarity transformation} or a \emph{conjugation}. Conjugation is a term from group theory which we will encounter in a later chapter. A similarity transformation does not change the basis independent properties of a matrix. In particular, it doesn't change the characteristic polynomial and therefore the eigenvalues, which are the  roots of said polynomial, are invariant under the transformation. Furthermore, it also doesn't change the determinant and trace because they are the product and sum of the eigenvalues respectively. The multiplicities (geometric and algebraic) of the eigenvalues are also invariant. The eigenvectors are mapped via the change of basis transformation $\mathbf{S}$: If $\mathbf{a}$ is an eigenvector of  $\mathbf{A}$, then $\mathbf{b} = \mathbf{S a}$ is the corresponding eigenvector of $\mathbf{B}$ [VERIFY! It might be $\mathbf{S}^{-1}$]. Other features that are invariant under a similarity transform are: rank, index of nilpotence, Jordan normal form (up to permutation of the blocks), Frobenius normal form, minimal polynomial and elementary divisors. Don't worry, if you don't know what some of these terms mean. I don't know all of them either and just wanted to list them all for completeness (the list is taken from Wikipedia).

% https://en.wikipedia.org/wiki/Matrix_similarity
% https://math.stackexchange.com/questions/255172/eigenvectors-of-similar-matrices

% Explain how \mathbf{S^{-1} A S} works in 3 steps when being applied to a vector. We first
% transform into the new basis via S, then apply A, then transform back into the old basis via
% S^-1. Verify it all with numeric examples


\medskip
Now, this is a rather implicit definition "there exists a matrix $\mathbf{S}$ such that..." and in practice, we may want to know (1) How can we decide, whether or not such a matrix exists? (2) If it does exist, is it unique? (3) If it is unique, what is it? If it isn't unique, what matrices are possible? Generally, if two matrices are similar, they represent the same geometric transformation but expressed in different coordinate systems. The matrix $\mathbf{S}$ is the matrix that transforms from one coordinate system to the other [TODO: be more specific: which way?] and is called the \emph{change of basis} matrix. So, in a typical practical situation, this matrix $\mathbf{S}$ will probably be given to us so we don't really need to compute it from $\mathbf{A,B}$. But if we really encounter a situation where we need to, this is how it can be done:

...TBC...ToDo: give algorithm to compute $\mathbf{S}$, given $\mathbf{A,B}$. Maybe use high-level pseudo code and refer to an actual (to be written) implementation in the C++ codebase 


% How to find P: write the relation as PA = PB  ->  (PA - PB) = 0  ->  P(A-B) = 0
% nah! only true if PA = AP, in general we have
% PB = AP
% https://math.stackexchange.com/questions/14075/how-do-i-tell-if-matrices-are-similar
% says the solution may not be unique - the system to solve may be singular. But the zero matrix is always a solution. A solution algo that computes a minimum norm solution would probably pick the zero matrix? ...yeah - it can't be unique because we see immediately that P could be multiplied by any scalar factor. Maybe pick a solution that has unit norm...I guess this would mean unit-Frobenius norm? 


% https://en.wikipedia.org/wiki/Matrix_similarity
% https://mathworld.wolfram.com/SimilarMatrices.html

% https://en.wikipedia.org/wiki/Matrix_congruence
% a stronger notion of similarity where S^{-1} = S^T

% https://en.wikipedia.org/wiki/Matrix_equivalence
% a weaker(!) notion than similarity. the equation is the same but applies also to non-square
% matrices

\paragraph{Misc Special Matrices}
% Toeplitz, circulant, unitary (maybe file under orthogonal - it's the complex version), tringular, ...


% https://en.wikipedia.org/wiki/Normal_matrix
% https://en.wikipedia.org/wiki/Triangular_matrix#Unitriangular_matrix

% https://en.wikipedia.org/wiki/Matrix_congruence

% Maybe sort the features according to arity:
% unary: A is symmetric, unitary,... 
% binary A and B are similar, congruent, ...

% https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra


% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
% https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix




%===================================================================================================
\subsection{Lesser Known Definitions}
%There are some lesser known operations on matrices which we will introduce here.

%---------------------------------------------------------------------------------------------------
\subsubsection{Block Matrices}
Matrices whose entries are themselves also matrices (and not numbers as usual) are called block matrices. For example, such a block matrix could look like:
\begin{equation}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} & \mathbf{C} \\
\mathbf{D} & \mathbf{E} & \mathbf{F} 
\end{pmatrix}
\end{equation}
The constituent matrices may or may not be of equal shape, but the shapes must be related in such a way that the tiling fits together [VERIFY]. For example, we could have $\mathbf{A} = 2 \times 4$,  $\mathbf{B} = 2 \times 2$, $\mathbf{C} = 2 \times 3$, $\mathbf{D} = 3 \times 4$, $\mathbf{E} = 3 \times 2$, $\mathbf{F} = 3 \times 3$. However, when we want to add or multiply two block matrices, not only must the outer matrix shapes be compatible but also the shapes of all the inner element-matrices. That may then actually constrain the shapes of the inner matrices some more. The constraints on the shapes really depend on what types of operations we want to perform with such block matrices. ...TBC...

% https://en.wikipedia.org/wiki/Block_matrix

%---------------------------------------------------------------------------------------------------
\subsubsection{Vectorization of a Matrix}
The vectorization of a matrix $\mathbf{A}$ is denoted by $\vectorize(\mathbf{A})$ and is an operator that turns an $m \times n$ matrix into an $mn$-dimensional column vector by vertically stacking the columns of the matrix on top of each other. A row-wise vectorization can be obtained by $\vectorize(\mathbf{A}^T)$ so we don't define a special operator for that. 

% explain "reshape" functions for matrices in software and what role the memory layout plays (row major vs column major)

%If we identify $k$-dimensional vectors with $k \times 1$ matrices, we may actually consider this as a special case of a reshaper operation

...TBC...

% https://en.wikipedia.org/wiki/Vectorization_(mathematics)


% IIRC there was some youtube video about conics that had an interesting equation involving the vec
% operator

% https://en.wikipedia.org/wiki/Matrix_calculus
% https://www.statlect.com/matrix-algebra/vec-operator

% Is there also an inverse - a matricization of a vector?
% I think, this is a special case of (tensor) reshaping
% https://en.wikipedia.org/wiki/Tensor_reshaping#Mode-m_Flattening_/_Mode-m_Matrixization

%---------------------------------------------------------------------------------------------------
%\subsubsection{Other Products}
%The matrix product as defined above is by far the most important "product-like" operation between matrices. But there are some other, too.

% ToDo: list common features such as associativity and distributivity, if applicable
% maybe move this below the "features" section because it references features such as rank, det
% eigenvalues. etc.

%---------------------------------------------------------------------------------------------------
\subsubsection{Hadamard Product} The Hadamard product of two $m \times n$ matrices is just the element-wise product. It is associative, commutative and distributive over addition. It is denoted by $\mathbf{A} \odot \mathbf{B}$ and satisfies

\medskip
\begin{tabular}{l l l l}
Determinant: & $\det(\mathbf{A} \odot \mathbf{B})$ 
             & $\geq \;\; \det(\mathbf{A})  \det(\mathbf{A})$   
             & when $\mathbf{A,B}$ are positive semidefinite \\
Rank:        & $\rank(\mathbf{A} \odot \mathbf{B}) $
             & $\leq \;\; \rank(\mathbf{A}) \rank(\mathbf{A})$     \\
\end{tabular}
\medskip

This product may also be called the \emph{naive matrix product}. Its applications are not so much in the realm of linear algebra but more in areas like numerical processing of 2D array data such as in image processing where we often want to do element-wise array operations. 

% ToDo: give more applications
% -in numerical processing, we often need element-wise array operations

% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)

%---------------------------------------------------------------------------------------------------
\subsubsection{Kronecker Product} The Kronecker product between an $m \times n$ matrix $\mathbf{A}$ and a $p \times q$ matrix $\mathbf{B}$ is an $mp \times nq$ matrix $\mathbf{C}$ in which each element is a product of one element from $\mathbf{A}$ and one element from $\mathbf{B}$. ...TBC..

% can be seen as inserting a scaled copy of the rightfactor as submatrix into the result where the scaling factor is taken from the left factor. show this by two 2x2 matrices

% https://en.wikipedia.org/wiki/Kronecker_product
% https://de.wikipedia.org/wiki/Kronecker-Produkt

%\paragraph{Khatri-Rao Product} This is variation of the Kronecker product. ...
%\paragraph{Tracey-Singh Product} This is another variation of the Kronecker product. ...


% https://en.wikipedia.org/wiki/Kronecker_product#Related_matrix_operations
% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#The_mixed-product_property
% https://en.wikipedia.org/wiki/Khatri%E2%80%93Rao_product#Face-splitting_product
% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#The_penetrating_face_product
% https://en.wikipedia.org/wiki/Frobenius_inner_product
% https://en.wikipedia.org/wiki/Pointwise_product
% what about convolution?
% https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_operator

% -mention approximation of matrix as weighted sum over Kronecker products of vectors
% -maybe this is also knwon as tensor decomposition? figure out!
%  https://en.wikipedia.org/wiki/Tensor_rank_decomposition
% -Try to approximate mxn matrix A as a weighted sum over Kronecker products of vectors in a 
%  least squares sense. ..has to do with separable filter kernels in image processing. Maybe make % %  the ansatz 
%    A ~ sum_{k=1}^n u_k \otimes v_k  or  
%    A ~ sum_{i=1}^m sum_{j=1}^n w_{ij} u_i \otimes v_j
%  where u_i v_j are vectors and w_{ij} are scalar weights




%===================================================================================================
\subsection{Important Facts and Formulas}
% rank-nullity theorem

%===================================================================================================
%\subsection{Computing with Spaces}


%On a beginner level, one usually assumes to deal regular matrices and as soon as one encounters a singular matrix, one throws the towel

%and just says things like "there are no solutions"

% Computing with spaces - operations like perp (orthogonal complement)

\begin{comment}
Explain what happens to the eigenvalues when we do certain things to a matrix (shifts, etc.)
I have a list of that in some text file. Shifting eigenvalues and manipulating them in other ways
can be important to improve convergence of numerical algrithms

Other possibly relevant matrix types to mention:
https://en.wikipedia.org/wiki/Companion_matrix
https://en.wikipedia.org/wiki/Smith_normal_form

\end{comment}