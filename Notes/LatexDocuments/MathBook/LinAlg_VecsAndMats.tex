\section{Vectors and Matrices}

\paragraph{Vectors}
Consider a point in the 2D plane or in 3D space. To represent such a point mathematically, we would need a pair or a triple of numbers respectively. An $n$-dimensional vector can generally be thought of as an $n$-tuple of numbers. For intuition building, it's best to think of real numbers although in general, other kinds of numbers may also be allowed in the more general case. Such a vector can not only represent a position in space but also a translation. Imagine the $xy$-plane. The vector $(x,y) = (3,2)$ could encode the action of first going $3$ units into the $x$-direction (usually to the right) and the going $2$ units into the $y$-direction (usually upward). Of course, we could as go first the $2$ units into the $y$-direction and the $3$ units into the $x$-direction - the location we would end up would be the same in both cases. Vectors are usually denoted as columns. For example, a vector $\mathbf{v}$ with given $x$ and $y$ values is written as:
\begin{equation}
\mathbf{v} 
= \begin{pmatrix} x \\ y \end{pmatrix} 
=   x \cdot \begin{pmatrix} 1 \\ 0 \end{pmatrix} 
  + y \cdot \begin{pmatrix} 0 \\ 1 \end{pmatrix} 
\end{equation}
where the right hand side can be seen as an interpretation of what the tuple $(x,y)$ actually encodes. We imagine $x$ to be a scaling factor of a unit vector into the $x$ direction 

...TBC...

% Give the standard basis (1,0),(0,1) of R^2, explain how any vector can be expressed as a linear combination of them. done

% Explain how the sum is "formal"

%A vector may not only represent a point itself, but also a translation ...tbc...


\paragraph{Matrices}
Vectors can be used to represent locations and translations. Translations are a specific kind of geometric transformation. A matrix represents a different kind of transformation, namely a \emph{linear transformation}, that we can apply to a vector. In geometric terms, linear transformations are scalings, rotations and shears. ...TBC...

% the columns of the matrix say where the basis vectors go

%Translations are not among the linear transformations that we can represent by matrices.


\paragraph{Well, actually...}
Strictly speaking, an $n$-tuple of numbers is not what a vector really $is$ by its nature. By its nature, a vector is a geometric entity such as a point in space or an arrow with a direction and length. The $n$-tuple of numbers is a specific representation of that geometric entity that depends on the coordinate system that we have chosen. But take that statement as a foreshadowing to a more advanced viewpoint. For the purposes of this section, it's totally okay to picture a vector as a tuple of numbers. ...TBC...

% Likewise, matrices are also only a specific representation of a linear transformation ...

\medskip
Moreover, when we look at things in a more carefully, we need to distinguish between points and vectors. ...TBC...


%===================================================================================================
%\subsection{Vectors}

\subsection{Vector Operations}

\paragraph{Vector Addition}
Algebraically, there is not much to say about vector addition - we just add the tuples element-wise and that's it. We can interpret such a vector addition geometrically at placing the vectors tip to tail. ...TBC... [ToDo: insert figure with the parallelogram picturing $\mathbf{a + b}$ and $\mathbf{b + a}$].

\paragraph{Scalar Multiplication}

\paragraph{Scalar Product}

\paragraph{Vector Products}
% cross-product, triple-product, wedge-product





%===================================================================================================

\subsection{Matrix Operations}
\subsubsection{Addition and Subtraction}

%---------------------------------------------------------------------------------------------------
\subsubsection{Multiplication}

% -maybe add also the Kronecker product and list formulas for its connection with the regular
%  product
% -maybe exmplain block matrices
% -But maybe put these things into a section Lesser Known Stuff
%  ...although, it could actually also fit into the "decomposition" section


%---------------------------------------------------------------------------------------------------
\subsubsection{Inversion}
% Could be done via Gauss-Jordan Elimination Algorithm decribed in the section about solvin linear 
% systems

% Misc: transposition, hermitian tranpose, etc.





%===================================================================================================
\subsection{Vector Features}

\paragraph{Vector Norms}
The norm of a vector captures its length. The most common norm is the Euclidean norm but there are others as well...TBC...
% are a measure of length, $L_p$-norm, explain the general requirements to a norm
% norm equivalence


\paragraph{Orthogonality and Angles}
% Angles between a are defined in terms of the cosine of the norm
% 


\paragraph{Span of a Set of Vectors}
As the span of a set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$, we define the set of all vectors that can be formed by arbitrary linear combinations of those vectors, i.e. the set of all vectors that can be written as:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_2 \mathbf{v}_n 
\end{equation}
for some set of scalar coefficients $a_1, \ldots, a_n$.


\paragraph{Linear Independence}
A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$ is said to be linearly independent, if none of its elements can be expressed as a linear combination of other elements. A necessary and sufficient mathematical condition for linear independence is that the equation:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_2 \mathbf{v}_n = \mathbf{0}
\end{equation}
can be satisfied only if all the $a_i$ are equal to zero, i.e. only the trivial solution for the $a_i$ exists [VERIFY]. Linear independence is an important condition for a set of vectors to have. In a sense, it means that all vectors are needed and none of the is redundant. If a set of vectors is linearly independent, it means that removing one of the vectors would necessarily reduce the dimension of their span by one.



%===================================================================================================
\subsection{Matrix Features}


%---------------------------------------------------------------------------------------------------
\subsubsection{Special Properties}
% By a property of a matrix, I mean a feature thata matrix may or may not have - a boolean feature, so to speak.

\paragraph{Squareness}
A matrix is called a square matrix, if it has the same number of rows as it has columns. The general rectangular array of elements forms a square.

\paragraph{Symmetry}
A matrix is symmetric if it is equal to its own transpose. That means that the rows are equal to the columns. Obviously, symmetry is a feature that only square matrices can possibly have.
% Give fomrula for invers of transpose and/or for (AB)^T = B^T A^T iirc

% boolean fetaures: symmetric, orthogonal, nilpotent, idempotent, unitary, normal, etc.
% discrete features: rank, index of nilpotency, signature
% scalar: determinant
% list of scalars: eigenvalues, singular values
% list of vectors: (left/right) (generalized) eigenvectors, singular vectors
% boolean features of pairs of matrices: similarity (is A similar to B), 
% matrix: set of similar matrices,

% maybe put these "boolean" features into a subsection
\paragraph{Orthogonality}
% a (square) matrix is orthogonal when all its rows are mutually orthogonal. This implies



%---------------------------------------------------------------------------------------------------
\subsubsection{Characteristic Numbers}

\paragraph{Rank}

\paragraph{Determinant}
% regular/singular

\paragraph{Norms}
% explain norm compatibility with vector norms


%---------------------------------------------------------------------------------------------------


\paragraph{Characteristic Polynomial}

\paragraph{Eigenvalues}

\paragraph{Eigenvectors}

\paragraph{Generalized Eigenvectors}





\paragraph{Similarity}
A matrix $\mathbf{A}$ is said to be \emph{similar} to another matrix $\mathbf{B}$, if there exists an invertible matrix $\mathbf{S}$ such that $\mathbf{B} = \mathbf{S^{-1} A S}$. We denote this by $\mathbf{A} \sim \mathbf{B}$ which we read as "A is similar to B" [VERIFY!]. Matrix similarity is an equivalence relation, so we have, among other things, a symmetry of the relation: $\mathbf{A} \sim \mathbf{B} \Leftrightarrow \mathbf{B} \sim \mathbf{A}$. We can actually solve for $\mathbf{A}$ by pre-multiplying both sides by $\mathbf{S}$ and post-multiplying both sides by $\mathbf{S}^{-1}$ to obtain $\mathbf{A} = \mathbf{S B S^{-1}}$. The transformation that maps $\mathbf{A}$ to $\mathbf{B}$ is called a \emph{similarity transformation} or a \emph{conjugation}. Conjugation is a term from group theory which we will encounter in a later chapter. A similarity transformation does not change the basis independent properties of a matrix. In particular, it doesn't change the characteristic polynomial and therefore the eigenvalues, which are the  roots of said polynomial, are invariant under the transformation. Furthermore, it also doesn't change the determinant and trace because they are the product and sum of the eigenvalues respectively. The multiplicities (geometric and algebraic) of the eigenvalues are also invariant. The eigenvectors are mapped via the change of basis transformation $\mathbf{S}$: If $\mathbf{a}$ is an eigenvector of  $\mathbf{A}$, then $\mathbf{b} = \mathbf{S a}$ is the corresponding eigenvector of $\mathbf{B}$ [VERIFY! It might be $\mathbf{S}^{-1}$]. Other features that are invariant under a similarity transform are: rank, index of nilpotence, Jordan normal form (up to permutation of the blocks), Frobenius normal form, minimal polynomial and elementary divisors. Don't worry, if you don't know what some of these terms mean. I don't know all of them either and just wanted to list them all for completeness (the list is taken from Wikipedia).

% https://en.wikipedia.org/wiki/Matrix_similarity
% https://math.stackexchange.com/questions/255172/eigenvectors-of-similar-matrices

% Explain how \mathbf{S^{-1} A S} works in 3 steps when being applied to a vector. We first
% transform into the new basis via S, then apply A, then transform back into the old basis via
% S^-1. Verify it all with numeric examples

\medskip
Now, this is a rather implicit definition "there exists a matrix $\mathbf{S}$ such that..." and in practice, we may want to know (1) How can we decide, whether or not such a matrix exists? (2) If it does exist, is it unique? (3) If it is unique, what is it? If it isn't unique, what matrices are possible? Generally, if two matrices are similar, they represent the same geometric transformation but expressed in different coordinate systems. The matrix $\mathbf{S}$ is the matrix that transforms from one coordinate system to the other [TODO: be more specific: which way?] and is called the \emph{change of basis} matrix. So, in a typical practical situation, this matrix $\mathbf{S}$ will probably be given to us so we don't really need to compute it from $\mathbf{A,B}$. But if we really encounter a situation where we need to, this is how it can be done:

...TBC...ToDo: give algorithm to compute $\mathbf{S}$, given $\mathbf{A,B}$. Maybe use high-level pseudo code and refer to an actual (to be written) implementation in the C++ codebase 


% How to find P: write the relation as PA = PB  ->  (PA - PB) = 0  ->  P(A-B) = 0
% nah! only true if PA = AP, in general we have
% PB = AP
% https://math.stackexchange.com/questions/14075/how-do-i-tell-if-matrices-are-similar
% says the solution may not be unique - the system to solve may be singular. But the zero matrix is always a solution. A solution algo that computes a minimum norm solution would probably pick the zero matrix? ...yeah - it can't be unique because we see immediately that P could be multiplied by any scalar factor. Maybe pick a solution that has unit norm...I guess this would mean unit-Frobenius norm? 


% https://en.wikipedia.org/wiki/Matrix_similarity
% https://mathworld.wolfram.com/SimilarMatrices.html

% https://en.wikipedia.org/wiki/Matrix_congruence
% a stronger notion of similarity where S^{-1} = S^T

% https://en.wikipedia.org/wiki/Matrix_equivalence
% a weaker(!) notion than similarity. the equation is the same but applies also to non-square
% matrices

\paragraph{Misc Special Matrices}
% Toeplitz, circulant, unitary (maybe file under orthogonal - it's the complex version), tringular, ...


% https://en.wikipedia.org/wiki/Normal_matrix
% https://en.wikipedia.org/wiki/Triangular_matrix#Unitriangular_matrix

% https://en.wikipedia.org/wiki/Matrix_congruence

% Maybe sort the features according to arity:
% unary: A is symmetric, unitary,... 
% binary A and B are similar, congruent, ...

% https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra


% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
% https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix




%===================================================================================================
\subsection{Lesser Known Definitions}
%There are some lesser known operations on matrices which we will introduce here.

%---------------------------------------------------------------------------------------------------
%\subsubsection{Other Products}
%The matrix product as defined above is by far the most important "product-like" operation between matrices. But there are some other, too.

% ToDo: list common features such as associativity and distributivity, if applicable
% maybe move this below the "features" section because it references features such as rank, det
% eigenvalues. etc.

%---------------------------------------------------------------------------------------------------
\subsubsection{Hadamard Product} The Hadamard product of two $m \times n$ matrices is just the element-wise product. It is associative, commutative and distributive over addition. It is denoted by $\mathbf{A} \odot \mathbf{B}$ and satisfies

\medskip
\begin{tabular}{l l l l}
Determinant: & $\det(\mathbf{A} \odot \mathbf{B})$ 
             & $\geq \;\; \det(\mathbf{A})  \det(\mathbf{A})$   
             & when $\mathbf{A,B}$ are positive semidefinite \\
Rank:        & $\rank(\mathbf{A} \odot \mathbf{B}) $
             & $\leq \;\; \rank(\mathbf{A}) \rank(\mathbf{A})$     \\
\end{tabular}
\medskip

% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)

%---------------------------------------------------------------------------------------------------
\subsubsection{Kronecker Product} The Kronecker product between an $m \times n$ matrix $\mathbf{A}$ and a $p \times q$ matrix $\mathbf{B}$ is an $mp \times nq$ matrix $\mathbf{C}$ in which each element is a product of one element from  $\mathbf{A}$ and one element from $\mathbf{B}$. ...TBC..

% https://en.wikipedia.org/wiki/Kronecker_product
% https://de.wikipedia.org/wiki/Kronecker-Produkt

%\paragraph{Khatri-Rao Product} This is variation of the Kronecker product. ...
%\paragraph{Tracey-Singh Product} This is another variation of the Kronecker product. ...


% https://en.wikipedia.org/wiki/Kronecker_product#Related_matrix_operations
% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#The_mixed-product_property

% -mention approximation of matrix as weighted sum over Kronecker products of vectors
% -maybe this is also knwon as tensor decomposition? figure out!
%  https://en.wikipedia.org/wiki/Tensor_rank_decomposition
% -Try to approximate mxn matrix A as a weighted sum over Kronecker products of vectors in a 
%  least squares sense. ..has to do with separable filter kernels in image processing. Maybe make % %  the ansatz 
%    A ~ sum_{k=1}^n u_k \otimes v_k  or  
%    A ~ sum_{i=1}^m sum_{j=1}^n w_{ij} u_i \otimes v_j
%  where u_i v_j are vectors and w_{ij} are scalar weights

%---------------------------------------------------------------------------------------------------
\subsubsection{Block Matrices}
Matrices whose entries are themselves also matrices (and not numbers as usual) are called block matrices. For example, such a block matrix could look like:
\begin{equation}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} & \mathbf{C} \\
\mathbf{D} & \mathbf{E} & \mathbf{F} 
\end{pmatrix}
\end{equation}

 ...TBC...

% https://en.wikipedia.org/wiki/Block_matrix


%===================================================================================================
\subsection{Important Facts and Formulas}
% rank-nullity theorem

%===================================================================================================
%\subsection{Computing with Spaces}


%On a beginner level, one usually assumes to deal regular matrices and as soon as one encounters a singular matrix, one throws the towel

%and just says things like "there are no solutions"

% Computing with spaces - operations like perp (orthogonal complement)

\begin{comment}
Explain what happens to the eigenvalues when we do certain things to a matrix (shifts, etc.)
I have a list of that in some text file. Shifting eigenvalues and manipulating them in other ways
can be important to improve convergence of numerical algrithms

Other possibly relevant matrix types to mention:
https://en.wikipedia.org/wiki/Companion_matrix
https://en.wikipedia.org/wiki/Smith_normal_form

\end{comment}