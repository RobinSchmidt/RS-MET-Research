\section{Vectors and Matrices}

\paragraph{Vectors}
Consider a point in the 2D plane or in 3D space. To represent such a point mathematically, we would need a pair or a triple of numbers respectively. An $n$-dimensional vector can generally be thought of as an $n$-tuple of numbers. For intuition building, it's best to think of real numbers although in general, other kinds of numbers may also be allowed in the more general case. Such a vector can not only represent a position in space but also a translation. Imagine the $xy$-plane. The vector $(x,y) = (3,2)$ could encode the action of first going $3$ units into the $x$-direction (usually to the right) and then going $2$ units into the $y$-direction (usually upward). Of course, we could as well go first the $2$ units into the $y$-direction and the $3$ units into the $x$-direction - the location we would end up would be the same in both cases. Vectors are usually denoted as columns. For example, a vector $\mathbf{v}$ with given $x$ and $y$ values is written as:
\begin{equation}
\mathbf{v} 
= \begin{pmatrix} x \\ y \end{pmatrix} 
=   x \cdot \begin{pmatrix} 1 \\ 0 \end{pmatrix} 
  + y \cdot \begin{pmatrix} 0 \\ 1 \end{pmatrix} 
\qquad \text{or}  \qquad
\mathbf{v} = (x,y) = x \cdot (1, 0) + y \cdot (0, 1)
\end{equation}
where the right hand sides can be seen as an interpretation of what the tuple $(x,y)$ actually encodes. We imagine $x$ to be a scaling factor (aka coefficient) of a unit vector $(1,0)$ into the $x$-direction and $y$ to be a coefficient for a unit vector $(0,1)$ into the $y$-direction. The left and right versions are just different notations. The right notation using tuples is more convenient when embedding an equation in a text or when vertical space shall be saved. In non-embedded equations, most books will usually use the left notation where vectors are written as columns with the components stacked vertically. 

%You may also (rarely) see the notation $\mathbf{v} = (x \;\, y)^T$, i.e. like tuple notation but without the comma and a superscript $T$. This $T$ stands for \emph{transposition} which, in this case, means to turn the row into a column.

% Explain notation: when the components are separated by a comma, we see the vectors as tuples. Without commas, we could write them as $(1 0)^T$ where the superscript $T$ in $(\ldots)^T$ stands for \emph{transposition} which, in this case, means to turn the row into a column.

...TBC...

% Give the standard basis (1,0),(0,1) of R^2, explain how any vector can be expressed as a linear combination of them. done

% Explain the uniot basis vectors and the zerop vector

% Explain how the sum is "formal" in an algebraic sense but real in a geometric sense. Or ...well...it is actually also real in an algebraic sense - we can collapse it into a single vector, namely the vector (x,y). Writing it as such a sum can actually be interpreted as decomposition into a purely horizontal and a purely vertical component. These components, when added together in a sense that is soon to be defined, give back the original vector. 

%A vector may not only represent a point itself, but also a translation ...tbc...


\paragraph{Matrices}
Vectors can be used to represent locations and translations. Translations are a specific kind of geometric transformation. A matrix represents a different kind of transformation, namely a \emph{linear transformation}, that we can apply to a vector. In geometric terms, linear transformations are scalings, rotations and shears. In this point of view, the columns of the matrices tell us, where the standard basis vectors will get mapped to (we'll see later what these are).

...TBC...


% A matrix is a 2D array of numbers, i.e. a table with rows and columns. Each cell contains a number pretty much like in a spreadsheet.
% the columns of the matrix say where the basis vectors go

% We need to define what the identity matrix is. Maybe the zero matrix, too

% Translations are not among the linear transformations that we can represent by matrices.


\paragraph{Well, actually...}
Strictly speaking, an $n$-tuple of numbers is not what a vector really \emph{is} by its nature. By its nature, a vector is a geometric entity such as a point in space or an arrow with a direction and length. The $n$-tuple of numbers is a specific \emph{representation} of that geometric entity that depends on the coordinate system that we have chosen. But you may take that statement as a foreshadowing to a more advanced viewpoint. For the purposes of this section, it's totally okay to picture a vector as a tuple of numbers. Likewise, matrices are also only a specific representation of a linear transformation. In a different coordinate system, the same transformation would be represented by a different 2D array of numbers. We will say more about this in the context of so called \emph{change of basis} transformations.

\medskip
Moreover, when we look at things more carefully, we need to distinguish between \emph{points} and \emph{vectors}. A point represents just a location and it does not really make any sense to add a location to another location. What is Berlin plus Paris supposed to mean? It makes no sense - even when expressed as geolocations! (By the way: I'm conveniently assuming a flat map here. Taking the spherical earth into account is a further complication which is of no relevance to my point). A vector, on the other hand, represents an arrow which we interpret as a translation. It does indeed make sense to add translations: we just perform one after the other. We may first go 30 km east and 20 km north and thereafter go 40 km east and 50 km north - that makes perfect sense. However - in practice (like in vector graphics APIs), points are usually identified with their \emph{position vectors} (aka location vector or radius vector), i.e. vectors that point from the origin to the given point. Represented as such, we actually can add points in our graphics code - we just should ask ourselves, if it makes any sense to do so in our particular case.
% ...TBC...

% https://en.wikipedia.org/wiki/Position_(geometry)

%===================================================================================================
\subsection{Vector Operations}

\paragraph{Scalar Multiplication}
Scalar multiplication refers to the act of multiplying a vector $\mathbf{v}$ by a scalar, i.e. a number, let's say $a$. This is just done by multiplying every component of the vector by that number. Geometrically, this scales the length of the vector by a factor of $a$. That's where the name "scalar" comes from. You may interpret it as "scaler". If $a > 1$, the vector just gets longer. If $a < 1$ but still $a > 0$, the vector gets shorter but retains its direction. If $a < 0$, the vector reverses direction in addition to being lengthened or shortened (in the special case of $a=-1$, it gets only reflected). If $a = 0$, the vector actually gets collapsed into the zero vector $\mathbf{0}$.

\paragraph{Vector Addition}
Algebraically, there is not much to say about vector addition - we just add the tuples component-wise and that's it. We can interpret such a vector addition geometrically at placing the vectors tip to tail. ...TBC... [ToDo: insert figure with the parallelogram picturing $\mathbf{a + b}$ and $\mathbf{b + a}$].

% What about when dimensionalities don't match? Maybe we can zero-pad the shorter vector in certain
% cases. Geometrically, this would mean to iamgine the low-dimensional vector spcae as being 
% embedded into a higher dimensional space by setting the extra coordinates in the higher
% dimensional space to zero. For example, a 2D vector (x,y) could be augmented to a 3D vector
% (x,y,z) by just setting $z=0$. Check how graphics libraries like OpenGL handle that.
% 

\paragraph{Vector Subtraction}
% What about subtraction? Algebraically, it's just addition of the negative but geometrically, it
% has a meaningful interpretation, so maybe it should get is won section

% maybe generalize to "Linear Combinations"

\paragraph{Linear Combinations}
A linear combination of a bunch of vectors is just a weighted sum of those vectors. That means each vector gets multiplied by a weight via our scalar multiplication and then these scaled vectors are added up via our vector addition. Vector subtraction is also just a special case of a linear combination of two vectors where one vector gets a weight of $+1$ and the other a weight of $-1$. 



\paragraph{Inner Product}
The \emph{inner product}, also known as the \emph{scalar product} or \emph{dot product} between two vectors $\mathbf{v}$ and $\mathbf{w}$ does not give another vector. Instead, the result is just a number, i.e.a scalar. It is computed by taking all the component-wise products and summing them all up. That is the algebraic definition and the calculation recipe. The inner product can also be defined geometrically as follows: it is the product of the lengths of the two vectors times the cosine of the angle between them. In 3D with $\mathbf{v} = (v_x, v_y, v_z)$ and $\mathbf{w} = (w_x, w_y, w_z)$ we have:
\begin{equation}
 \mathbf{v} \cdot \mathbf{w} 
 = v_x w_x + v_y w_y + v_z w_z 
 = |\mathbf{v}| |\mathbf{w}| \cos( \angle(\mathbf{v}, \mathbf{w}) )
\end{equation}
where on the right we have used two notations that we have yet to explain: By $|\mathbf{v}|$ we denote the length or so called \emph{norm} of the vector $\mathbf{v}$. We can compute it simply by taking the square root of the dot product of the vector $\mathbf{v}$ with itself. By $\angle(\mathbf{v}, \mathbf{w})$ we mean the angle between the two vectors $\mathbf{v}$ and $\mathbf{w}$. To compute this angle, we can just take the arc-cosine of the dot product of the two normalized vectors $\mathbf{\hat{v}} = \mathbf{v} / |\mathbf{v}|$ and  $\mathbf{\hat{w}} = \mathbf{w} / |\mathbf{w}|$. So we have:
\begin{equation}
|\mathbf{v}| = \sqrt{ \mathbf{v} \cdot \mathbf{v}}, 
\qquad 
\angle(\mathbf{v}, \mathbf{w}) = 
 \arccos \left( \frac{\mathbf{v} \cdot \mathbf{w}} {|\mathbf{v}|  |\mathbf{w}|} \right)
\end{equation}
If this definition appears circular, remember that we can always use the purely algebraic definition to compute the dot product which does not yet reference the norm or angle. We see that geometrically, the dot product gives us two things: lengths and angles. Using a hat above a vector is a common notation for \emph{normalized} vectors, i.e. vectors with a norm of one. Dividing a vector by its own norm does indeed yield a vector with unit norm. It has the same direction as the original vector, of course - because scaling all components by the same amount doesn't change the direction of a vector.



 ...TBC...
% explain geometric interpretations
% -coefficient of projection
% -correlation in the sense of: how similar is the direction of the vectors (when both are
%  normalized)
% Mention different (more general) definitions: 
% -with complex conjugation,
% -with a (positive definite) matrix sadwiched in between
% -explain generalization to comlex vectors. I think the left factor is conjugated. Or is it the
%  right one? Does it matter? Maybe not with complex numbers but maybe with further generalizations
%  to vectors of quaternions or other kinds of non-commutative numbers.
% -Maybe the discussion of the geometric meaning could be deferred to the section about analytic
%  geometry. but we can actually mention it here already - it doesn't hurt to be a bit repetitive
% -the inner product is commutative (at least for real-valued vectors) and distributive over vector
%  addition and subtraction. It does *not* satisfy the rule that if the product is zero then at least
%  one of the factors has to be zero.
% -mention the different notations (dot, v^T w, <v,w>, (v,w))

%\paragraph{Vector Products}
% cross-product, triple-product (but this gives a scalar), wedge-product

% Lengths, angles, projection, correlation | Linear algebra episode 2
% https://www.youtube.com/watch?v=mCi_0ML0lG8

\paragraph{Orthogonal Projection}
% -explain how the dot-product being zero means that the vectors are orthogonal and how we can
%  measure to how much amount two vectors point into the same direction by way of the dot product
%   ...interpreted as correlation or angle

\paragraph{Vector Decomposition}
% We have already seen how a vector $(x,y)$ can be decomposed into a purely horizontal and a purely vertical component. That decomposition can be generalized.
% we project the vector onto one other vector to obtain it's component
% We often choose the vectors into which we decompose a given vector to be orthogonal but that doesn't need to be the case. ...but I think, the general decomposition cannot be obtained by simple projections. I think, we really need to solve a linear system of equations for this.

% https://ccrma.stanford.edu/~jos/st/Projection_onto_Non_Orthogonal_Vectors.html

% https://math.stackexchange.com/questions/1882096/how-to-decompose-a-vector-into-non-orthogonal-components


\paragraph{Outer Product}

\paragraph{Cross Product}
% -give algebraic and geometric definitions (geometric: sine of the angle, perpedicular to 
%  the plane spanned etc.)

\paragraph{Wedge Product}
% does not really belong here because it results in a bivector

\paragraph{Triple Product}






% Norm, Normalization

% Outer product (is actually a special case tensor product or Kronecker product)

% Inversion
% Divide by its length twice. Normalize and then divide by the former length. Reciprocate the length
% but retain direction. Geometrically, it's inversion in a (hyper)sphere

% In linear algebra, we mostly deal with linear combinations, the scalar product and orthogonal projections. In analytic geometry of 3D space, the cross product is also common and the triple product will occasionally be seen. The rest is less common but included for completeness sake and will be picked up in later sections. The wedge product is important in exterior and geometric algebra. The outer product will be revisited in tensor algebra (in a different guise).

%===================================================================================================

\subsection{Matrix Operations}

%---------------------------------------------------------------------------------------------------
\subsubsection{Addition and Subtraction}
Matrices are added and subtracted element-wise. For that to work, the two matrices must have the same shapes. TODO: Define the zero matrix and its notation

%---------------------------------------------------------------------------------------------------
\subsubsection{Multiplication} In order to multiply two matrices, we must demand that the number of columns of the left factor must be equal to the number of rows in the right factor. When we multiply an $m \times p$ matrix $\mathbf{A}$ with a $p \times n$ matrix $\mathbf{B}$ and call the product $\mathbf{C}$ such that $\mathbf{A} \mathbf{B} = \mathbf{C}$, then the element $c_{ij}$ of the product can be computed from the elements $a_{ij}, b_{ij}$ of $\mathbf{A}, \mathbf{B}$ as follows:
\begin{equation}
 \mathbf{C} = \mathbf{A} \mathbf{B} 
 \quad \text{where} \quad
 c_{ij} = \sum_{k=1}^p a_{ik} b_{kj}
\end{equation}
The matrix $\mathbf{C}$ will have a shape of $m \times n$. Note that the element $c_{ij}$ can be interpreted as a scalar product of the $i$-th row of $\mathbf{A}$ with the $j$-th column of  $\mathbf{B}$ [VERIFY!]. The matrix that has zeros everywhere except on the main diagonal is called the identity matrix and denoted as usually denoted as $\mathbf{I}$. One could perhaps argue that $\mathbf{1}$ could also be a good notation, but I have never seen that - on the other hand, the difference is subtle anyway. This matrix is the neutral element of matrix multiplication. That means that  $\mathbf{I} \mathbf{A} = \mathbf{A} = \mathbf{A} \mathbf{I}$ for any (square) matrix $\mathbf{A}$ whatsoever. In this notation, it is usually understood that the size of $\mathbf{I}$ is matched to the size of $\mathbf{A}$. Matrix multiplication is associative: $(\mathbf{A} \mathbf{B}) \mathbf{C} = \mathbf{A} (\mathbf{B} \mathbf{C})$ and distributive over matrix addition and subtraction: $\mathbf{A} (\mathbf{B} \pm \mathbf{C}) = \mathbf{A} \mathbf{B} \pm \mathbf{A} \mathbf{C}$ but it is not generally commutative: $\mathbf{AB} \neq \mathbf{BA}$. 

% -maybe add also the Kronecker product and list formulas for its connection with the regular
%  product
% -maybe explain block matrices
% -But maybe put these things into a section Lesser Known Stuff
%  ...although, it could actually also fit into the "decomposition" section
% -The columns of a matrix may be interpreted as the images of the standard basis vectors. But maybe
%  this should go to a section matrix-vector operations. It may contain products like A x, x^T A,
%  x^T A x (sandwich product)
% -The matrix product can be seen as being made up from scalar products of the rows of the left 
%  factor with the columns of the right factor

%---------------------------------------------------------------------------------------------------
\subsubsection{Transposition}
The operation of turning the rows of a matrix into the columns of a new matrix and therefore also the columns into rows is called \emph{transposition} of a matrix. This is denoted by a superscript $T$ as in $\mathbf{A}^T$. When a matrix is complex, then most of the time we want to combine transposition with complex conjugation of the elements because that's how many formulas for real matrices involving the transpose generalize to the complex case. It's rare to see transposition or complex conjugation all by itself applied to a complex matrix (it happens occasionally, though). The combined operation of transposition and conjugation is sometimes called \emph{Hermitian transpose} and denoted by a superscript $H$ as $\mathbf{A}^H$. But there are other notations for that too, for example using a dagger: $\mathbf{A}^\dagger$ or an asterisk $\mathbf{A}^*$ [VERIFY!]. But watch out - often the asterisk denotes only complex conjugation alone and the dagger is also often used for the (pseudo-)inverse.

% https://en.wikipedia.org/wiki/Adjugate_matrix
% https://www.varsitytutors.com/hotmath/hotmath_help/topics/adjoint-of-a-matrix
% https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Inverse_of_a_matrix
% cofactor matrix

%---------------------------------------------------------------------------------------------------
\subsubsection{Inversion}
If we have given a matrix $\mathbf{A}$ and we can find another matrix $\mathbf{B}$ such that  $\mathbf{B} \mathbf{A} = \mathbf{I}$, then we call $\mathbf{B}$ a left inverse of $\mathbf{A}$. A matrix $\mathbf{C}$ with the property $\mathbf{A} \mathbf{C} = \mathbf{I}$ would be called a right inverse. It turns out that there is no difference between these two notions, i.e. left inverses are always also right inverses and vice versa so we call a matrix that has both of these properties just an inverse matrix of $\mathbf{A}$. It turns furthermore out that if such an inverse exists, then it is unique, so we call such a matrix \emph{the inverse} of $\mathbf{A}$ and denote it by $\mathbf{A}^{-1}$. Finding such an inverse for a given matrix $\mathbf{A}$ can be done by an algorithm called Gauss-Jordan elimination which we will be explained later. [VERIFY!] [TODO: explain pseudo-inverse]
% Could be done via Gauss-Jordan Elimination Algorithm decribed in the section about solvin linear 
% systems

% Misc: transposition, hermitian tranpose, etc.


%---------------------------------------------------------------------------------------------------
\subsubsection{Commutator}
Matrix multiplication is, in general, not commutative, i.e. $\mathbf{A} \mathbf{B} \neq \mathbf{B} \mathbf{A}$ in general. We also say, the matrices  $\mathbf{A}$ and $\mathbf{B}$ do not \emph{commute} in general. The difference of the two possible products $\mathbf{C} = \mathbf{A} \mathbf{B} - \mathbf{B} \mathbf{A}$ is called the commutator of $\mathbf{A}$ and $\mathbf{B}$. It can be thought of as measuring, how non-commutative the matrices are with respect to one another. If two matrices commute, their commutator is the zero matrix. [TODO: What about $\mathbf{A} \mathbf{B} \mathbf{A}^{-1} \mathbf{B}^{-1}$. That's how the commutator is defined in group theory. It's the identity matrix if  $\mathbf{A}$ and $\mathbf{B}$ commute, I think. Figure out and explain the relation between these two conflicting definitions!]

%---------------------------------------------------------------------------------------------------
\subsubsection{Similarity Transformation}
Another operation between two matrices $\mathbf{A}$ and $\mathbf{B}$ is defined as follows: $\mathbf{C} = \mathbf{B}^{-1} \mathbf{A} \mathbf{B}$. This operation is called a \emph{similarity transformation} also known as \emph{change of basis} transformation. ...TBC...


% is called \emph{conjugation} of  $\mathbf{A}$ by $\mathbf{B}$. It is defined as: 

% https://math.stackexchange.com/questions/134796/conjugation-of-matrices-and-conjugation-of-complex-numbers

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix-Vector Products}
We have seen operations involving two vectors as well as operations involving two matrices. From a geometric perspective, linear algebra really comes to life when considering the interplay between vectors and matrices. Vectors encode positions and therefore also geometric shapes which are defined by the positions of their vertices. Matrices encode geometric transformations such as reflections, rotations, scalings, etc. We can interpret such a transformation as an \emph{action} that is applied to a shape, i.e. to a set of vectors. A matrix can \emph{act on} a vector or - in passive language - can \emph{be applied to} the vector. This action is effected by forming the \emph{matrix-vector product}. We actually already have everything we need. We do not need to define any brand new product for this. Instead, we observe that column vectors can be interpreted as special matrices - namely as matrices that just have a single column, i.e. as $n \times 1$ matrices where $n$ is the dimensionality of the vector. We can multiply such an $n$-vector $\mathbf{v}$, seen as $n \times 1$ matrix, from the left with an $n \times n$ matrix  $\mathbf{A}$ using the rules of matrix multiplication to get another $n \times 1$ matrix, i.e. another $n$-vector $\mathbf{w} = \mathbf{A v}$. Forming such a product between a matrix as left factor and a column vector as right factor will be our usual way of expressing geometric transformations. 

%\medskip
\paragraph{Left vs Right Multiplication}
An entirely equivalent formalism for geometric transformations can be formulated using row vectors, i.e. $1 \times n$ matrices and products where the matrix factor appears as the right factor. To switch from one formalism to the other requires to transpose all matrices and write all products in reverse order [VERIFY!]. The advantage of that would be that we can read such products from left to right and that reading direction corresponds to the order in which the transformations are applied. However, we will stick to column vectors and multiplying by matrices from the left because that's more common in mathematics. 

% https://math.stackexchange.com/questions/2738278/what-exactly-is-a-left-multiplication-transformation
% https://medium.com/geekculture/right-and-left-matrix-multiplication-d21947f195d8
% https://math.stackexchange.com/questions/3252146/linear-transformations-and-left-multiplication-matrix

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Left_and_right_eigenvectors

%\medskip
\paragraph{Dimensionality Conversions}
The rules of matrix multiplication would also allow to use an $m \times n$ matrix - that is, the dimensionality $m$ of the output vector could in general be different from the dimensionality $n$ of the input vector. If $m < n$, we would get a dimensionality reduction. That means, some sort of projection must be involved in the transformation and information will be discarded by forming the product. This is quite common in graphics when we eventually need to project a 3D scene onto a 2D screen. The $m > n$ case would mean that our $n$-vector lives in a (sub)space of lower dimensionality that is embedded in some higher dimensional space. In such a case, the matrix-vector product may lift the vector out of its subspace.

%\medskip
\paragraph{Sandwich Products}
Another kind of product that can be formed between vectors and matrices is sometimes called the \emph{sandwich product}. It takes as inputs an $m \times 1$ vector $\mathbf{w}$, an $m \times n$ matrix $\mathbf{A}$ and an $n \times 1$ vector $\mathbf{v}$ and produces the product $\mathbf{w}^T \mathbf{A v}$. The transposition of $\mathbf{w}$ into $\mathbf{w}^T$ turns it into an $1 \times m$ row vector which is required to make the matrix shapes (vectors are special matrices!) compatible for multiplication. Due to associativity of matrix multiplication, it doesn't matter if you evaluate it as $\mathbf{w}^T (\mathbf{A v})$ or as $(\mathbf{w}^T \mathbf{A}) \mathbf{v}$. The result will be the same in both cases and it will be a scalar, i.e. just a number. Sandwich products are important in the definition of quadratic forms which in turn are important in defining some basic geometric shapes (ellipsoids, hyperboloids, paraboloids) and also in optimization and approximation applications. These sandwich products also appear a lot in quantum mechanics and they may serve as a formalism for generalizing the scalar product especially in the context of differential geometry.  There, the regular scalar product, which we normally use to compute the squared length of a vector, is replaced by such a sandwich product involving the so called metric tensor - which is basically just the matrix that you sandwich into the regular scalar product. [VERIFY if we also use the sandwich product to compute angles between vectors in DG or if we use it just for the length]

\paragraph{$\star$ Functions of a Matrix}
Now we want to define what is means to apply a function such as the exponential or trigonometric functions to a matrix. What is $f(\mathbf{X}) = \sin(\mathbf{X})$ supposed to mean when $\mathbf{X}$ is a matrix? But before looking at the sine function, let's start with something simpler: the square root function. It makes sense to say that $\sqrt(\mathbf{X})$ is a matrix which results in $\mathbf{X}$ when being squared. That is: we want to find a matrix $\mathbf{Y}$ such that $\mathbf{Y}^2 = \mathbf{X}$ where it is understood that $\mathbf{Y}^2 = \mathbf{Y Y}$, i.e. squaring a matrix means to just form the matrix product with itself. For this to make sense, we must require that $\mathbf{Y}$ is a square matrix because otherwise we cannot form the product. When faced with this equation $\mathbf{Y}^2 = \mathbf{X}$, the first question that arises is whether or not such a matrix even exists. Assuming that it does, we would like to have some sort of algorithm to compute it. [TODO: figure out if nD Netwon iteration works for this]. Matrix functions like $\exp(\mathbf{X})$ or $\sin(\mathbf{X})$ are typically defined via the Taylor expansion of the respective function. We just take the scalar coefficients from the regular Taylor series expansion and replace the variable $x$ with the matrix  $\mathbf{X}$. All we need to know evaluate such a Taylor series is how to multiply a matrix by a scalar and how to multiply two (square) matrices - and these are thing we indeed do know by now. Of course, we must also ensure that series converges.

%multiplied

% log: find a matrix B such that e^B = A
% explain hwo diagonalization can help so evaluate functions

% How can the sqrt be computed? Will Newton iteration work

% sqrt

\paragraph{$\star$ Functions of a Vector}
We have seen that it is possible to define functions of a matrix via the Taylor series. Can we do something like that with vectors, too? Well, the Taylor series approach requires that we need a way to multiply the variable (repeatedly) by itself to form powers. For that, we need a multiplication to be defined. For vectors, we have seen various products. Can one of them be meaningfully iterated? The scalar product takes two vectors as input - but it produces a scalar as output, so that doesn't seem to work out. The cross product could look like a promising candidate. It takes two vectors and input and produces another vector as output. Unfortunately, it works only in 3D - but even there, it doesn't really help much because the cross product of a vector with itself will just produce the zero vector. I'm not aware of any meaningful way to define a \emph{vector valued} function of a vector similarly as we did for matrices. However, we will later in geometric algebra encounter the so called \emph{multivectors} of which the vectors form a subset. For these multivectors, it is indeed possible to define functions that return another multivector - just like we did for matrices.

\medskip
TODO: explain kernel an image of a matrix

%generalizations of the scalar product (especially in differential geometry) and also in 

% explain the ^T notation

%...TBC...TODO: mention sandwich product $\mathbf{w}^T \mathbf{A v}$.

% Left-product, right -product, sandwich product



%===================================================================================================
\subsection{Vector Features}

\paragraph{Vector Norms}
The norm of a vector captures its length. The most common norm is the Euclidean norm but there are others as well...TBC...
% are a measure of length, $L_p$-norm, explain the general requirements to a norm
% norm equivalence


\paragraph{Orthogonality and Angles}
% Angles between a are defined in terms of the cosine of the norm
% 


\paragraph{Span of a Set of Vectors}
As the \emph{span} of a set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$, we define the set of all vectors that can be formed by arbitrary linear combinations of those vectors, i.e. the set of all vectors that can be written as:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_n \mathbf{v}_n 
\end{equation}
for some set of scalar coefficients $a_1, \ldots, a_n$. The span of a set of vectors is itself a vector space: We can add two vectors from that space to get another vector which is again in the same space. And we can also scale the vectors from that space without leaving the space. We also say that our vectors span the space - where "span" is being used as a verb. The span (noun) of a set of vectors is the space that is spanned (verb) by these vectors. I know that this sounds silly and self referential. Often, the span of a set of vectors is a subspace of some larger embedding space. For example, in $3D$ space, a set of two vectors could span a $2D$ subspace, i.e. a $2D$ plane that is embedded in $3D$ space.


\paragraph{Linear Independence}
A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2 \ldots, \mathbf{v}_n \}$ is said to be linearly independent, if none of its elements can be expressed as a linear combination of other elements. A necessary and sufficient mathematical condition for linear independence is that the equation:
\begin{equation}
 a_1 \mathbf{v}_1 + a_2 \mathbf{v}_1  + \ldots + a_n \mathbf{v}_n = \mathbf{0}
\end{equation}
can be satisfied only if all the $a_i$ are equal to zero, i.e. only the trivial solution for the $a_i$ exists [VERIFY]. Linear independence is an important condition for a set of vectors to have. In a sense, it means that all vectors are needed and none of them is redundant. 

\paragraph{Dimension}
The sense in which none of the vectors is redundant in a linearly independent set of vectors is the following: If we would remove any of the vectors, we would lose a degree of freedom to move around. The space spanned by the remaining set of vectors would get reduced in dimension (by one). The \emph{dimension} of a space is the smallest number of vectors that is needed to span that space, i.e. to make any point in the space "reachable" via a linear combination of these basis vectors. Why do we make a big fuss of this? Clearly, the dimension of $\mathbb{R}^n$ is just $n$ right? So why all the fuss about linearly independent vectors? The reason is that in linear algebra, we also often deal with \emph{subspaces} of a given, higher dimensional embedding space. This embedding space could be $\mathbb{R}^n$ but we could consider a subspace of dimension $m < n$ that is spanned by a set of just $m$ vectors rather than $n$.

% The dimension is the number of degrees of freedom or number of independent directions that we can move in

\paragraph{Basis}
A \emph{set} $\{\mathbf{b}_1, \ldots, \mathbf{b}_n \}$ of linearly independent vectors that spans a given space is called a \emph{basis} for that space. A \emph{tuple} $(\mathbf{b}_1, \ldots, \mathbf{b}_n)$ of such vectors is called an \emph{ordered basis}. The vectors $\mathbf{b}_i$ in such a basis (ordered or not) are called the \emph{basis vectors}. For example, the so called \emph{standard basis} (aka canonical basis or natural basis) for $\mathbb{R}^3$ is given by the 3 vectors $\mathbf{e}_1 = (1,0,0), \mathbf{e}_2 = (0,1,0), \mathbf{e}_3 = (0,0,1)$. It is a common convention to denote the $n$ vectors of the standard basis of $\mathbb{R}^n$ as $\mathbf{e}_i, i = 1,\ldots,n$. Specifically in $\mathbb{R}^3$, one may also encounter the notation $\mathbf{e}_x, \mathbf{e}_y, \mathbf{e}_z$ or $\hat{\mathbf{x}}, \hat{\mathbf{y}}, \hat{\mathbf{z}}$ or  $\mathbf{i}, \mathbf{j}, \mathbf{k}$. We'll stick to the general notation $\mathbf{e}_i$ here.

% https://en.wikipedia.org/wiki/Basis_(linear_algebra)

\paragraph{Orthogonalization via Gram-Schmidt Algorithm}
When we have given an arbitrary ordered basis $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$, the basis vectors $\mathbf{b}_i$ will be in general neither normalized to unit length nor orthogonal to one another. The process of \emph{Gram-Schmidt orthogonalization} may be used to derive another basis $C$ from $B$ that has these two (often desirable) features while spanning the same space as $B$. It's an algorithm that works as follows: ...TBC...

% TODO: explain how we could order a given basis:
% Idea: 
% -Compute for each vector in the basis the center of gravity / center of mass 
% -To do this, use the absoulte values of the entries
% -Order the basis vectors according to ascending center of gravity.
% -The idea is that the ordered basis kind of "roughly resembles" the standard basis. If the algo
%  gets the standard basis in shuffled order as input, it will return it in the correct order.
% -What about a basis like (1,1), (1,-1)? the center of mass is the same for both (at "index" 0.5)
%  Maybe in such a case, we should also consider angles: choose the vector that aligns best with
%  the standard basis vector of that index. If more of them align best, choose the one with
%  positive angle
%
% Try to find a way to canonicalize a basis B = b_1,..,_b_n. That process could involve (in that
% order)
% -Ordering of the vectors according to center of weight
% -Tweaking the initial vector b_1 to align most closely with e_1 = (1,0,0,...) without leaving
%  the span of b_1,...,b_n. That means: if e_1 is actually in the span of B, then the updated b_1
%  will indeed become e_1 itself
% -Gram-Schmidt orthogonalization of the remaining vectors b_k, k = 2,...,n
%  -Maybe in each step, try to align the vector b_k as closely as possible with e_k
% 
% -Maybe find a basis that minimizes the distance to a given basis. Maybe if we write the basis as
%  matrix (basis vectors go into the columns), we could definethe distance as Frobenius norm of the
%  difference between our basis and the given basis.
% -Maybe the given basis should be a suitable subset of the standard basis. Which subset is suitable
%  could be figured out using the "center of gravity" approach. For each b_i, find it's center of
%  gravity (it can be interpreted as a non-integer index in 1...n), round and pick e_n. But some 
%  care must be taken if two cneter of gravities round to the same index like 2.6 and 3.4.



%===================================================================================================
\subsection{Matrix Features}


%---------------------------------------------------------------------------------------------------
\subsubsection{Special Properties}
% Maybe move that below the characteristioc numbers and eigenspaces subsections because some 
% properties may be defined in terms of these numbers (e.g.: contractive: all eigvals < 1 in 
% abs-val)
% By a property of a matrix, I mean a feature thata matrix may or may not have - a boolean feature, so to speak.

\paragraph{Squareness}
A matrix is called a square matrix, if it has the same number of rows as it has columns. The general rectangular array of elements forms a square.

\paragraph{Symmetry}
A matrix is symmetric if it is equal to its own transpose. That means that the rows are equal to the columns. Obviously, symmetry is a feature that only square matrices can possibly have.
% Give fomrula for invers of transpose and/or for (AB)^T = B^T A^T iirc

% maybe put these "boolean" features into a subsection
\paragraph{Orthogonality}
% a (square) matrix is orthogonal when all its rows are mutually orthogonal. This implies

\paragraph{Definiteness}

% boolean fetaures: nilpotent, idempotent, unitary, normal,  self-adjoint,
% contractive (all eigenvalues < 1 in absolute value, vectors get shorter), self-inverse
% (aka involution), is there something like "unipotent" such that some power of the matrix is the
% identity matrix? regular/singular
%
% discrete features: rank, index of nilpotency, signature
% scalar: determinant, condition number
% list of scalars: eigenvalues, singular values
% list of vectors: (left/right) (generalized) eigenvectors, singular vectors
% boolean features of pairs of matrices: similarity (is A similar to B), simultaneously
%   diagonalizable,
% matrix: set of similar matrices,





%---------------------------------------------------------------------------------------------------
\subsubsection{Characteristic Numbers}

\paragraph{Rank}
The column rank of a matrix is the number of linearly independent columns

% -rank-nullity theorem
% -row-rank equals column rank

\paragraph{Trace}



\paragraph{Determinant}
% -regular/singular
% -scaling factor for a hypervolume
% -product of eigenvalues

\paragraph{Norms}
% explain norm compatibility with vector norms

% trace

%---------------------------------------------------------------------------------------------------
\subsubsection{Eigenspaces}
When forming a product between an $n \times n$ matrix $\mathbf{A}$ and an $n$-vector $\mathbf{v}$, we will get a new $n$-vector, which will, in general, point into a different direction than  $\mathbf{v}$. There may, however, be certain special vectors which do not change direction as a result of such a multiplication by a matrix but rather just (possibly) change their length. Such special vectors are called the \emph{eigenvectors} of the matrix $\mathbf{A}$ and the length-change factor is called the corresponding \emph{eigenvalue}. The equation that expresses this circumstance is given by:
\begin{equation}
 \mathbf{A v} = \lambda \mathbf{v}  
 \quad \Leftrightarrow \quad
 \det ( \mathbf{A} - \lambda \mathbf{I} ) = 0
\end{equation}
which is called the characteristic equation. [the right one, I think]
[VERIFY if implication is two-sided]. TODO: explain what this has to do with the determinant, explain etymology of "eigen"
% The lhs is a matrix-vecztor product whereas the right hand side is just a scalar multplication with the sclaing factor $\lambda$.


\paragraph{Eigenvalues and Characteristic Polynomial}
If we expand the left hand side of the characteristic equation $\det ( \mathbf{A} - \lambda \mathbf{I} ) = 0$ according to the rules for how determinants are computed, it will turn out that we will get a polynomial in $\lambda$ of degree $n$. This polynomial is called the \emph{characteristic polynomial} of the matrix  $\mathbf{A}$. The right hand side says that this polynomial should be equal to zero for $\lambda$ to be an eigenvalue. Another way to say that is that an eigenvalue $\lambda$ is a root of the characteristic polynomial. We know that roots of polynomials may have a multiplicity, i.e. in the product form of the polynomial, the linear factor that corresponds to the root may occur multiple times. The multiplicity of the eigenvalue as the root of the characteristic polynomial is called the \emph{algebraic multiplicity} of the eigenvalue and denoted by $\alg(\lambda)$. Note that even for matrices of real numbers, the eigenvalues may be complex. If the matrix is real, then complex eigenvalues will always come in pairs of conjugates [VERIFY!].

...TBC...

% The eigenvalues are important because they characterize the features of the matrix in a way that is independent from the chosen coordinate system, i.e. they are basis independent. More precisely, they characterize not the matrix itself but rather the underlying linear transformation that our matrix represents. These characterizations do not depend on that particular (arbitrary) representation - in another coordinate system, the matrix will look totally different but these features will still be the same. The eigenvectors, on the other hand, will be different in another coordinate system (aka basis). I like to think of them as relating the basis independent features (given by the eigenvalues) to our particular chosen basis, i.e. they determine, how exactly the basis independent features manifest themselves in our particular chosen basis.

% Algebraic multiplicity, geometric multiplicity
% left and right eigenvectors

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors

%\paragraph{Eigenvalues}

\paragraph{Eigenvectors and Eigenspace}
If we have an eigenvalue $\lambda$, we may also want to know the corresponding eigenvector(s). Multiple eigenvectors that correspond to a single eigenvalue may only exist when the algebraic multiplicity is greater than one. The number of eigenvectors that correspond to a given eigenvalue is called the \emph{geometric multiplicity} of the eigenvalue and denoted by $\geo(\lambda)$ and that geometric multiplicity is always at least one and at most equal to the algebraic multiplicity: $1 \leq \geo(\lambda) \leq \alg(\lambda)$. If $\geo(\lambda) < \alg(\lambda)$, the eigenvalue is called a \emph{defective eigenvalue}. A matrix that has defective eigenvalues is called a \emph{defective matrix} [VERIFY!]. Just like eigenvalues, eigenvectors too can be complex even for real matrices. In the case of real matrices, they will also come in complex conjugate pairs. 

\medskip
To find an eigenvector for an eigenvalue $\lambda$, we need to solve the linear system of equations represented by $\mathbf{A v} = \lambda \mathbf{v}$.  This can be rewritten as $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$ which is in the standard form $\mathbf{A x} = \mathbf{b}$ that we need for solving such equations. In this form, $\mathbf{x}$ is the unknown vector that we are trying to find, $\mathbf{A}$ is a known matrix and $\mathbf{b}$ is a known right hand side vector. We will learn later how to solve such equations. At this point, it should only be pointed out that the solution to such an equation is not necessarily a unique vector and indeed, in our case here it can't be unique because eigenvectors can always be scaled at will. That latter statement means that if $\mathbf{v}$ is an eigenvector, then any scaled version of $\mathbf{v}$ will also be an eigenvector. It is therefore common practice to normalize eigenvectors to unit length.

\medskip
An eigenspace of an $n \times n$ matrix $\mathbf{A}$ associated with a given eigenvalue $\lambda$ is defined to be the space spanned by all the eigenvectors that correspond to $\lambda$ [VERIFY!]. Formally, it is the following set of vectors: $E(\lambda) = \{\mathbf{v} \in \mathbb{C}^n : (\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}\}$. 

%In words, that is the set of all vectors that satisfy the equation $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$.

% The eigenvectors may also be complex even for real matrices

%\paragraph{Eigenspaces}
%

% Say something about the space in which the vectors live. If A is a real matrix, the eigenvectors may actually be complex.
% ...but i think, the system is alway singular - we will at least get a 1D continuum of solutions because eigenvectors can be scaled at will.

%If $\alg(\lambda) > 1$, it may happen that this system has no unique solution but rather a whole space of solutions

...TBC...

% Can it happen that the geometric multiplcity is zero? If so - what does it mean?

% If the algebraic multiplicity of $\lambda$ is $1$, then there can be only one such eigen

% Multiple eigenvectors can only exist when the algebraic multiplicity is gretaer than 1

\paragraph{Generalized Eigenvectors} We established that an eigenvector $\mathbf{v}$ with eigenvalue $\lambda$ is a vector with the property $\mathbf{A v} = \lambda \mathbf{v}$ which can be rewritten as $(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}$. A \emph{generalized eigenvector} with \emph{rank} $r$ is a vector $\mathbf{v}$ that satisfies $(\mathbf{A} - \lambda \mathbf{I})^r \mathbf{v} = \mathbf{0}$. The rank $r \geq 1$ is a natural number and it is understood to be the smallest possible number for which that equation is satisfied, i.e. we also require  $(\mathbf{A} - \lambda \mathbf{I})^{r-1} \mathbf{v} \neq \mathbf{0}$. For $r = 1$, the generalized equation reduces to the defining equation of ordinary eigenvectors. Generalized eigenvectors become important when the geometric multiplicity of an eigenvalue $\lambda$ is less than its algebraic multiplicity, i.e. when we are dealing with defective eigenvalues.   ...TBC...

% what about the edge case $k = 0$. then the matrix becomes then identity matrix and the equation has only the trivial solution. So, could we say that the zero vector is a generalized eigenvector of rank zero? Is that useful? I mean, it's trivial but maybe it completes a general pattern in a satisfying way? I think we have a chain of subspaces of decreasing dimension when dealing wit generalized eigenvectors and the zero vector can indeed be seen a 0-dimensional subspace?

% https://en.wikipedia.org/wiki/Generalized_eigenvector
% https://en.wikipedia.org/wiki/Modal_matrix
% https://en.wikipedia.org/wiki/Modal_matrix#Generalized_modal_matrix


\paragraph{Simpler Special Cases} In the important special case where all eigenvalues are distinct, much of the considerations above simplify considerably. When all eigenvalues $\lambda_i$ are distinct, it means that all the algebraic multiplicities are one and therefore also all the geometric multiplicities are one because of the general rule $1 \leq \geo(\lambda) \leq \alg(\lambda)$. That, in turn, implies that there is a one-to-one correspondence between eigenvalues and eigenvectors. Each eigenvalue has exactly one corresponding eigenvector. Don't make the all too tempting mistake of assuming that this is always so in the general case. It is only so in this special friendly case which is fortunately quite common [VERIFY!]. This distinctness of eigenvalues has the further implication that the matrix is diagonalizable. We'll see later what that means. Suffice to say here that this feature is usually a good thing. For diagonalizability, it is actually already enough if the algebraic and geometric multiplicities of all eigenvalues match: $\forall i: \geo(\lambda_i) = \alg(\lambda_i)$. 

%Don't make the mistake of thinking that there is a one-to-one correspondence between eigenvectors and eigenvalues in general. 

%It is, in general, not the case that each eigenvalue has one and only one corresponding eigenvector. ...TBC...

\paragraph{Invariant Subspaces}
We have learned that an eigenvector of a matrix $\mathbf{A}$ is a vector $\mathbf{v}$ that doesn't change direction under the transformation that is achieved when $\mathbf{A}$ is acting on  $\mathbf{v}$ via the product $\mathbf{w} = \mathbf{A} \mathbf{v}$. We can pick any subset of the set of all (generalized?) eigenvectors and use this subset as a basis for an invariant subspace, i.e. a subspace that is mapped to itself under the matrix A. That the matrix cannot "lift" a vector out of this subspace is clear when we express the vector as linear combination of the selected eigenvectors. Then, each component of the vector in that basis will just be scaled by applying the matrix. No components into other directions will be created. [VERIFY ALL]

% https://en.wikipedia.org/wiki/Invariant_subspace
% https://www.statlect.com/matrix-algebra/invariant-subspace
% https://www.spektrum.de/lexikon/mathematik/invarianter-unterraum/7094

% matrices satsify their own characteristic equation

%---------------------------------------------------------------------------------------------------
%\subsubsection{Properties of Pairs of Matrices}
\subsubsection{Relations between Matrices}
We have seen "boolean properties" (programmer speak) of single matrices, i.e. features that a given matrix may or may not have such as symmetry, definiteness, etc. There are also some of these boolean properties that pairs of matrices may or may not have. Another way to say that is two matrices may or may not be in a certain relation with one another. Some important of such relations will be defined now.

% Could also be called "Relations between Matrices"
% -reciprocity/inversion
% -commutativity
% -bi-orthogonality (?)
% -what about asymmetric relations - maybe something like <, i.e. an order..maybe based on a norm?

\paragraph{Similarity}
A matrix $\mathbf{A}$ is said to be \emph{similar} to another matrix $\mathbf{B}$, if there exists an invertible matrix $\mathbf{S}$ such that $\mathbf{B} = \mathbf{S^{-1} A S}$. We denote this by $\mathbf{A} \sim \mathbf{B}$ which we read as "A is similar to B" [VERIFY!]. Matrix similarity is an equivalence relation, so we have, among other things, a symmetry of the relation: $\mathbf{A} \sim \mathbf{B} \Leftrightarrow \mathbf{B} \sim \mathbf{A}$. We can actually solve for $\mathbf{A}$ by pre-multiplying both sides by $\mathbf{S}$ and post-multiplying both sides by $\mathbf{S}^{-1}$ to obtain $\mathbf{A} = \mathbf{S B S^{-1}}$. The transformation that maps $\mathbf{A}$ to $\mathbf{B}$ is called a \emph{similarity transformation} or a \emph{conjugation}. Conjugation is a term from group theory which we will encounter in a later chapter. The matrix $\mathbf{S}$ that achieves this transformation into the new basis is called \emph{change of basis matrix} or \emph{transition matrix}. 

\medskip
We can see how $\mathbf{A}$ and $\mathbf{B}$ encode the same transformation by considering, how the product: $\mathbf{A} = \mathbf{S B S^{-1}}$ acts on a vector $\mathbf{v}$. It computes $\mathbf{S B S^{-1} v}$ which we can understand as 3 successive transformations by reading the 3 matrices from right to left. As first step, we apply $\mathbf{S}^{-1}$ which transforms the vector $\mathbf{v}$ into the new coordinate system. Then we apply the matrix $\mathbf{B}$ which expresses our actual geometric transformation in this new coordinate system. Then we transform the result back into our old coordinate system by applying $\mathbf{S}$. The overall effect of these 3 transformations should be the same as applying $\mathbf{A}$ which represents our geometric transformation directly in the original coordinate system. So, the matrix $\mathbf{S}$ is actually the matrix that transforms from the coordinate system in which $\mathbf{B}$ represents our transformation into the coordinate system where $\mathbf{A}$ represents our transformation [VERIFY!].

%https://en.wikipedia.org/wiki/Change_of_basis



\medskip
A similarity transformation does not change the basis independent properties of a matrix. In particular, it doesn't change the characteristic polynomial and therefore the eigenvalues, which are the  roots of said polynomial, are invariant under the transformation. Furthermore, it also doesn't change the determinant and trace because they are the product and sum of the eigenvalues respectively. The multiplicities (geometric and algebraic) of the eigenvalues are also invariant. The eigenvectors are mapped via the change of basis transformation $\mathbf{S}$: If $\mathbf{a}$ is an eigenvector of  $\mathbf{A}$, then $\mathbf{b} = \mathbf{S a}$ is the corresponding eigenvector of $\mathbf{B}$ [VERIFY! It might be $\mathbf{S}^{-1}$]. Other features that are invariant under a similarity transform are: rank, index of nilpotence, Jordan normal form (up to permutation of the blocks), Frobenius normal form, minimal polynomial and elementary divisors. Don't worry, if you don't know what some of these terms mean. I don't know all of them either and just wanted to list them all for completeness (the list is taken from Wikipedia). [Q: What other transformations do not change the characteristic polynomial? Are there any? Or are similarity transformations the only kind of transformation with that property?]

% https://en.wikipedia.org/wiki/Matrix_similarity
% https://math.stackexchange.com/questions/255172/eigenvectors-of-similar-matrices

% Explain how \mathbf{S^{-1} A S} works in 3 steps when being applied to a vector. We first
% transform into the new basis via S, then apply A, then transform back into the old basis via
% S^-1. Verify it all with numeric examples


% explain hwo the product S^-1 A S can be interpreted in terms of hwo it acts on a vector: chaneg basis via S, apply the transformation via A, change back to the old basis via S^-1

\medskip
Now, this is a rather implicit definition "there exists a matrix $\mathbf{S}$ such that..." and in practice, we may want to know (1) How can we decide, whether or not such a matrix exists? (2) If it does exist, is it unique? (3) If it is unique, what is it? If it isn't unique, what matrices are possible? Generally, if two matrices are similar, they represent the same geometric transformation but expressed in different coordinate systems. The matrix $\mathbf{S}$ is the matrix that transforms from one coordinate system to the other [TODO: be more specific: which way?] and is called the \emph{change of basis} matrix. So, in a typical practical situation, this matrix $\mathbf{S}$ will probably be given to us so we don't really need to compute it from $\mathbf{A,B}$. But if we really encounter a situation where we need to, this is how it can be done:

[...TBC...ToDo: give algorithm to compute $\mathbf{S}$, given $\mathbf{A,B}$. Maybe use high-level pseudo code and refer to an actual (to be written) implementation in the C++ codebase ]



% How to find P: write the relation as PA = PB  ->  (PA - PB) = 0  ->  P(A-B) = 0
% nah! only true if PA = AP, in general we have
% PB = AP
% https://math.stackexchange.com/questions/14075/how-do-i-tell-if-matrices-are-similar
% says the solution may not be unique - the system to solve may be singular. But the zero matrix is always a solution. A solution algo that computes a minimum norm solution would probably pick the zero matrix? ...yeah - it can't be unique because we see immediately that P could be multiplied by any scalar factor. Maybe pick a solution that has unit norm...I guess this would mean unit-Frobenius norm? 


% https://en.wikipedia.org/wiki/Matrix_similarity
% https://mathworld.wolfram.com/SimilarMatrices.html

% https://en.wikipedia.org/wiki/Matrix_congruence
% a stronger notion of similarity where S^{-1} = S^T

% https://en.wikipedia.org/wiki/Matrix_equivalence
% a weaker(!) notion than similarity. the equation is the same but applies also to non-square
% matrices

%\paragraph{Commuting Matrices}

\paragraph{Simultaneously Diagonalizable Matrices}
Two matrices $\mathbf{A}$ and $\mathbf{B}$ are said to be simultaneously diagonalizable if there exists a single matrix $\mathbf{P}$ such that $\mathbf{P}^{-1} \mathbf{A} \mathbf{P}$ and  $\mathbf{P}^{-1} \mathbf{B} \mathbf{P}$ are both diagonal matrices. For that to happen, the matrices $\mathbf{A}$ and $\mathbf{B}$ must have the same eigenvectors because in a diagonalization, these eigenvectors appear as the columns of the diagonalizing $\mathbf{P}$ matrix [VERIFY]. Two matrices commute if and only if they are simultaneously diagonalizable. A set $A$ of matrices is simultaneously diagonalizable if all pairs of matrices in the the set are simultaneously diagonalizable.  


\medskip
Note how the features of being \emph{similar} and being \emph{simultaneously diagonalizable} are complementary: If two matrices are similar, then they have the same eigen\emph{values} (this implication goes only one way, by the way). If two matrices are simultaneously diagonalizable, then they have the same eigen\emph{vectors} (Q: Is this implication also only one way?). One might be tempted to conclude that, if two matrices have both features, i.e. are simliar \emph{and} simultaneously diagonalizable, then they must have the same eigenvalues \emph{and} the same eigenvectors and hence they must in fact be the same matrix [VERIFY]. But I think, that conclusion would be false because even if two matrices have the same eigenvalues and eigenvectors, the matrices may still be different because the way in which the eigenvalues and eigenvectors are paired may be different. [Figure this out! (the formation of eigenspaces is actually more complicated than values and vectors being paired 1-to-1 - but that doesn't matter for the argument)]


%That implies that if two matrices have both features, i.e. are simliar \emph{and} simultaneously diagonalizable, then they must have the same eigenvalues \emph{and} the same eigenvectors and hence they must in fact be the same matrix [VERIFY]. [TODO: there are some conclusions in here that I have drawn myself and need to be verified]

%No  - I think, the conclusion is false. Even if tow matrices have the eigenvalues and the same eigenvectors, they may still be different matrices because the way in which the eigenvalues and eigenvectors are paired may be different!

%\medskip



% No - it's not an iff. Two matrices with the same eigenvalues cann be dissimilar nonetheless. The implication is only one way: if tow matrices are similar, they have the same eigenvalues
% https://www.youtube.com/watch?v=MOn7cLZQH-0

% so, that means: similar matrices have equal eigenvalues and simulatneously diagonalizabel matrices have the same eigenvectors?

% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Simultaneous_diagonalization

%https://en.wikipedia.org/wiki/Triangular_matrix#Simultaneous_triangularisability

% https://en.wikipedia.org/wiki/Matrix_mechanics#Matrix_basics
% Seems like matrices are simultaneously diagonalizable, if they have the same eigenvectors. ...yeah - that makes sense in the lght of the equation above


\paragraph{Misc Special Matrices}
% Toeplitz, circulant, unitary (maybe file under orthogonal - it's the complex version), tringular, ...

% https://en.wikipedia.org/wiki/Involution_(mathematics)

% https://en.wikipedia.org/wiki/Normal_matrix
% https://en.wikipedia.org/wiki/Triangular_matrix#Unitriangular_matrix

% https://en.wikipedia.org/wiki/Matrix_congruence

% Maybe sort the features according to arity:
% unary: A is symmetric, unitary,... 
% binary A and B are similar, congruent, ...

% https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra


% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
% https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix




%===================================================================================================
\subsection{Lesser Known Definitions}
%There are some lesser known operations on matrices which we will introduce here.

%---------------------------------------------------------------------------------------------------
\subsubsection{Block Matrices}
Matrices whose entries are themselves also matrices (and not numbers as usual) are called block matrices. For example, such a block matrix could look like:
\begin{equation}
\begin{pmatrix}
\mathbf{A} & \mathbf{B} & \mathbf{C} \\
\mathbf{D} & \mathbf{E} & \mathbf{F} 
\end{pmatrix}
\end{equation}
The constituent matrices may or may not be of equal shape, but the shapes must be related in such a way that the tiling fits together [VERIFY]. For example, we could have $\mathbf{A} = 2 \times 4$,  $\mathbf{B} = 2 \times 2$, $\mathbf{C} = 2 \times 3$, $\mathbf{D} = 3 \times 4$, $\mathbf{E} = 3 \times 2$, $\mathbf{F} = 3 \times 3$. However, when we want to add or multiply two block matrices, not only must the outer matrix shapes be compatible but also the shapes of all the inner element-matrices. That may then actually constrain the shapes of the inner matrices some more. The constraints on the shapes really depend on what types of operations we want to perform with such block matrices. ...TBC...

% https://en.wikipedia.org/wiki/Block_matrix

%---------------------------------------------------------------------------------------------------
\subsubsection{Vectorization of a Matrix}
The vectorization of a matrix $\mathbf{A}$ is denoted by $\vectorize(\mathbf{A})$ and is an operator that turns an $m \times n$ matrix into an $mn$-dimensional column vector by vertically stacking the columns of the matrix on top of each other. A row-wise vectorization can be obtained by $\vectorize(\mathbf{A}^T)$ so we don't define a special operator for that. 

% explain "reshape" functions for matrices in software and what role the memory layout plays (row major vs column major)

%If we identify $k$-dimensional vectors with $k \times 1$ matrices, we may actually consider this as a special case of a reshaper operation

...TBC...

% https://en.wikipedia.org/wiki/Vectorization_(mathematics)


% IIRC there was some youtube video about conics that had an interesting equation involving the vec
% operator

% https://en.wikipedia.org/wiki/Matrix_calculus
% https://www.statlect.com/matrix-algebra/vec-operator

% Is there also an inverse - a matricization of a vector?
% I think, this is a special case of (tensor) reshaping
% https://en.wikipedia.org/wiki/Tensor_reshaping#Mode-m_Flattening_/_Mode-m_Matrixization

%---------------------------------------------------------------------------------------------------
%\subsubsection{Other Products}
%The matrix product as defined above is by far the most important "product-like" operation between matrices. But there are some other, too.

% ToDo: list common features such as associativity and distributivity, if applicable
% maybe move this below the "features" section because it references features such as rank, det
% eigenvalues. etc.

%---------------------------------------------------------------------------------------------------
\subsubsection{Hadamard Product} The Hadamard product of two $m \times n$ matrices is just the element-wise product. It is associative, commutative and distributive over addition. It is denoted by $\mathbf{A} \odot \mathbf{B}$ and satisfies

\medskip
\begin{tabular}{l l l l}
Determinant: & $\det(\mathbf{A} \odot \mathbf{B})$ 
             & $\geq \;\; \det(\mathbf{A})  \det(\mathbf{A})$   
             & when $\mathbf{A,B}$ are positive semidefinite \\
Rank:        & $\rank(\mathbf{A} \odot \mathbf{B}) $
             & $\leq \;\; \rank(\mathbf{A}) \rank(\mathbf{A})$     \\
\end{tabular}
\medskip

This product may also be called the \emph{naive matrix product}. Its applications are not so much in the realm of linear algebra but more in areas like numerical processing of 2D array data such as in image processing where we often want to do element-wise array operations. 

% ToDo: give more applications
% -in numerical processing, we often need element-wise array operations

% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)

%---------------------------------------------------------------------------------------------------
\subsubsection{Kronecker Product} The Kronecker product between an $m \times n$ matrix $\mathbf{A}$ and a $p \times q$ matrix $\mathbf{B}$ is an $mp \times nq$ matrix $\mathbf{C}$ in which each element is a product of one element from $\mathbf{A}$ and one element from $\mathbf{B}$. ...TBC..

% can be seen as inserting a scaled copy of the rightfactor as submatrix into the result where the scaling factor is taken from the left factor. show this by two 2x2 matrices

% https://en.wikipedia.org/wiki/Kronecker_product
% https://de.wikipedia.org/wiki/Kronecker-Produkt

%\paragraph{Khatri-Rao Product} This is variation of the Kronecker product. ...
%\paragraph{Tracey-Singh Product} This is another variation of the Kronecker product. ...


% https://en.wikipedia.org/wiki/Kronecker_product#Related_matrix_operations
% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#The_mixed-product_property
% https://en.wikipedia.org/wiki/Khatri%E2%80%93Rao_product#Face-splitting_product
% https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#The_penetrating_face_product
% https://en.wikipedia.org/wiki/Frobenius_inner_product
% https://en.wikipedia.org/wiki/Pointwise_product
% what about convolution?
% https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_operator

% -mention approximation of matrix as weighted sum over Kronecker products of vectors
% -maybe this is also knwon as tensor decomposition? figure out!
%  https://en.wikipedia.org/wiki/Tensor_rank_decomposition
% -Try to approximate mxn matrix A as a weighted sum over Kronecker products of vectors in a 
%  least squares sense. ..has to do with separable filter kernels in image processing. Maybe make % %  the ansatz 
%    A ~ sum_{k=1}^n u_k \otimes v_k  or  
%    A ~ sum_{i=1}^m sum_{j=1}^n w_{ij} u_i \otimes v_j
%  where u_i v_j are vectors and w_{ij} are scalar weights



% https://www.youtube.com/watch?v=i0cp3iQXSk8
% defines a "tilt product" between two (unit) vectors: tilt(u, v) = v  u^T - u v^T. So it's
% kind of like the commutator of the outer product of two vectors. Yields a matrix which can
% be used to encode rotations via the matrix exponential. For orthogonal u,v, there's the
% closed form formula for a rotation by an angle theta: 
%   exp(tilt(u,v)) = I + sin(theta)*tilt(u,v) + (1-cos(theta))*(tilt(u,v))^2
% The tilt product gives a matrix which projects vectors ont the uv-plane *and* rotates them
% in this plane by the angle between u and v. He calls the formula "Generalized Euler's"
% formula. It's also similar to Rodrigues rotation formula
% It has also some formulas for tze matrix epxonential


%===================================================================================================
\subsection{Important Facts and Formulas}

\paragraph{Spectral Theorem}
% -spectral theorem: https://www.youtube.com/watch?v=4zD8Kd3HgJA
%  https://en.wikipedia.org/wiki/Spectral_theorem

\paragraph{Transposition Formulas}
\begin{equation}
(\mathbf{A_1 A_2 \ldots  A_n})^T = \mathbf{A_n}^T \ldots \mathbf{A_2}^T \mathbf{A_1}^T
\end{equation}


\paragraph{Inversion Formulas}
\begin{equation}
(\mathbf{A_1 A_2 \ldots  A_n})^{-1} = \mathbf{A_n}^{-1} \ldots \mathbf{A_2}^{-1} \mathbf{A_1}^{-1}
\end{equation}

% https://en.wikipedia.org/wiki/Woodbury_matrix_identity
% https://tlienart.github.io/posts/2018/12/13-matrix-inversion-lemmas/index.html
% https://www.statlect.com/matrix-algebra/matrix-inversion-lemmas
% http://www0.cs.ucl.ac.uk/staff/g.ridgway/mil/mil.pdf

% https://stattrek.com/matrix-algebra/matrix-theorems

\paragraph{Rank Nullity Theorem}

% -rank-nullity theorem
% https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem

\paragraph{Cayley Hamilton Theorem}

% https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem

% -matrix inversion lemma

% Matrix exponential - formula with commutators

%===================================================================================================
%\subsection{Computing with Spaces}


%On a beginner level, one usually assumes to deal regular matrices and as soon as one encounters a singular matrix, one throws the towel

%and just says things like "there are no solutions"

% Computing with spaces - operations like perp (orthogonal complement)

\begin{comment}
Explain what happens to the eigenvalues when we do certain things to a matrix (shifts, etc.)
I have a list of that in some text file. Shifting eigenvalues and manipulating them in other ways
can be important to improve convergence of numerical algrithms

Other possibly relevant matrix types to mention:
https://en.wikipedia.org/wiki/Companion_matrix
https://en.wikipedia.org/wiki/Smith_normal_form

\end{comment}