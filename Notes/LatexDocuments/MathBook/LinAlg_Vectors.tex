\section{Vectors and Matrices}

\paragraph{Vectors}
Consider a point in the 2D plane or in 3D space. To represent such a point mathematically, we would need a pair or a triple of numbers respectively. An $n$-dimensional vector can generally be thought of as an $n$-tuple of numbers. For intuition building, it's best to think of real numbers although in general, other kinds of numbers may also be allowed in the more general case. ...TBC...

% give the standard basis (1,0),(0,1) of R^2, explain

%A vector may not only represent a point itself, but also a translation ...tbc...


\paragraph{Matrices}
Vectors can be used to represent locations and translations. Translations are a specific kind of geometric transformation. A matrix represents a different kind of transformation, namely a \emph{linear transformation}, that we can apply to a vector. In geometric terms, linear transformations are scalings, rotations and shears. ...TBC...

% the columns of the matrix say where the basis vectors go

%Translations are not among the linear transformations that we can represent by matrices.


\paragraph{Well, actually...}
Strictly speaking, an $n$-tuple of numbers is not what a vector really $is$ by its nature. By its nature, a vector is a geometric entity such as a point in space or an arrow with a direction and length. The $n$-tuple of numbers is a specific representation of that geometric entity that depends on the coordinate system that we have chosen. But take that statement as a foreshadowing to a more advanced viewpoint. For the purposes of this section, it's totally okay to picture a vector as a tuple of numbers. ...TBC...

% Likewise, matrices are also only a specific representation of a linear transformation ...

\medskip
Moreover, when we look at things in a more carefully, we need to distinguish between points and vectors. ...TBC...


%===================================================================================================
%\subsection{Vectors}

%===================================================================================================
%\subsection{Matrices}






%===================================================================================================
\subsection{Operations}

%---------------------------------------------------------------------------------------------------
\subsubsection{Vector Operations}

\paragraph{Vector Addition}
Algebraically, there is not much to say about vector addition - we just add the tuples element-wise and that's it. We can interpret such a vector addition geometrically at placing the vectors tip to tail. ...TBC... [ToDo: insert figure with the parallelogram picturing $\mathbf{a + b}$ and $\mathbf{b + a}$].

\paragraph{Scalar Multiplication}

\paragraph{Scalar Product}

\paragraph{Vector Products}
% cross-product, triple-product, wedge-product

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Operations}
\paragraph{Addition and Multiplication}
\paragraph{Inversion}
% could be done via Gauss-Jordan Elimination Algorithm decribed belo

% Misc: transposition, hermitian tranpose, etc.


%===================================================================================================
\subsection{Linear Systems of Equations}
One important area of application of matrices and vectors is the solution of systems of linear equations

% solvability, rank (may also be filed under matrix features - maybe introduce the concept here and mnetion it there again)


%===================================================================================================
\subsection{Special Features}

%---------------------------------------------------------------------------------------------------
\subsubsection{Vector Features}

\paragraph{Vector Norms}
The norm of a vector captures its length. The most common norm is the Euclidean norm but there are others as well...TBC...
% are a measure of length, $L_p$-norm, explain the general requirements to a norm

\paragraph{Orthogonality and Angles}
% Angles between a are defined in terms of the cosine of the norm
% 

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Features}

\paragraph{Determinant}
% regular/singular

\paragraph{Norms}
% explain norm compatibility with vector norms

\paragraph{Orthogonality}
% a (square) matrix is orthogonal when all its rows are mutually orthogonal. This implies

\paragraph{Symmetry}

\paragraph{Similarity}
A matrix $\mathbf{A}$ is said to be \emph{similar} to another matrix $\mathbf{B}$, if there exists an invertible matrix $\mathbf{S}$ such that $\mathbf{B} = \mathbf{S^{-1} A S}$. We denote this by $\mathbf{A} \sim \mathbf{B}$ which we read as "A is similar to B" [VERIFY!]. Matrix similarity is an equivalence relation, so we have, among other things, a symmetry of the relation: $\mathbf{A} \sim \mathbf{B} \Leftrightarrow \mathbf{B} \sim \mathbf{A}$. We can actually solve for $\mathbf{A}$ by pre-multiplying both sides by $\mathbf{S}$ and post-multiplying both sides by $\mathbf{S}^{-1}$ to obtain $\mathbf{A} = \mathbf{S B S^{-1}}$. The transformation that maps $\mathbf{A}$ to $\mathbf{B}$ is called a \emph{similarity transformation} or a \emph{conjugation}. Conjugation is a term from group theory which we will encounter in a later chapter. A similarity transformation does not change the basis independent properties of a matrix. In particular, it doesn't change the characteristic polynomial and therefore its eigenvalues (they are roots of the polynomial). Furthermore, it also doesn't change the determinant and trace because they are the product and sum of the eigenvalues respectively. The multiplicities (geometric and algebraic) of the eigenvalues are also invariant. The eigenvectors are mapped via the change of basis transformation $\mathbf{S}$: If $\mathbf{a}$ is an eigenvector of  $\mathbf{A}$, then $\mathbf{b} = \mathbf{S a}$ is the corresponding eigenvector of $\mathbf{B}$ [VERIFY! It might be $\mathbf{S}^{-1}$]. Other features that are invariant under a similarity transform are: rank, index of nilpotence, Jordan normal form (up to permutation of the blocks), Frobenius normal form, minimal polynomial and elementary divisors. Don't worry, if you don't know what some of these terms mean. I don't know all of them either and just wanted to list them all for completeness (the list is taken from Wikipedia).

% https://en.wikipedia.org/wiki/Matrix_similarity
% https://math.stackexchange.com/questions/255172/eigenvectors-of-similar-matrices

% Explain how \mathbf{S^{-1} A S} works in 3 steps when being applied to a vector. We first
% transform into the new basis via S, then apply A, then transform back into the old basis via
% S^-1. Verify it all with numeric examples

\medskip
Now, this is a rather implicit definition "there exists a matrix $\mathbf{S}$ such that..." and in practice, we may want to know (1) How can we decide, whether or not such a matrix exists? (2) If it does exist, is it unique? (3) If it is unique, what is it? If it isn't unique, what matrices are possible? Generally, if two matrices are similar, they represent the same geometric transformation but expressed in different coordinate systems. The matrix $\mathbf{S}$ is the matrix that transforms from one coordinate system to the other [TODO: be more specific: which way?] and is called the \emph{change of basis} matrix. So, in a typical practical situation, this matrix $\mathbf{S}$ will probably be given to us so we don't really need to compute it from $\mathbf{A,B}$. But if we really encounter a situation where we need to, this is how it can be done:

...TBC...ToDo: give algorithm to compute $\mathbf{S}$, given $\mathbf{A,B}$


% How to find P: write the relation as PA = PB  ->  (PA - PB) = 0  ->  P(A-B) = 0
% nah! only true if PA = AP, in general we have
% PB = AP
% https://math.stackexchange.com/questions/14075/how-do-i-tell-if-matrices-are-similar
% says the solution may not be unique - the system to solve may be singular. But the zero matrix is always a solution. A solution algo that computes a minimum norm solution would probably pick the zero matrix? ...yeah - it can't be unique because we see immediately that P could be multiplied by any scalar factor. Maybe pick a solution that has unit norm...I guess this would mean unit-Frobenius norm? 


% https://en.wikipedia.org/wiki/Matrix_similarity
% https://mathworld.wolfram.com/SimilarMatrices.html

% https://en.wikipedia.org/wiki/Matrix_congruence
% a stronger notion of similarity where S^{-1} = S^T

% https://en.wikipedia.org/wiki/Matrix_equivalence
% a weaker(!) notion than similarity. the equation is the same but applies also to non-square
% matrices

\paragraph{Misc Special Matrices}
% Toeplitz, circulant, unitary (maybe file under orthogonal - it's the complex version), tringular, ...


% https://en.wikipedia.org/wiki/Normal_matrix
% https://en.wikipedia.org/wiki/Triangular_matrix#Unitriangular_matrix

% https://en.wikipedia.org/wiki/Matrix_congruence

% Maybe sort the features according to arity:
% unary: A is symmetric, unitary,... 
% binary A and B are similar, congruent, ...

% https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra

%===================================================================================================
\subsection{Matrix Decompositions}

% https://en.wikipedia.org/wiki/Matrix_decomposition

%---------------------------------------------------------------------------------------------------
\subsubsection{LU Decomposition}
The LU-decomposition decomposes the matrix into a product $\mathbf{A = L U}$ where $\mathbf{L}$ is a lower triangular matrix and $\mathbf{U}$ is an upper triangular matrix. The LU-decomposition is algorithmically basically Gaussian elimination with some modifications. If you have an LU decomposition of a matrix $\mathbf{A}$ available, then it is straightforward to solve an equation $\mathbf{Ax = b}$. In practice, one often computes a decomposition of the form 

..TBC...explain details of algorithms for the decomposition and the solution of linear systems.
% Lower Triangular, Upper Triangular

% https://en.wikipedia.org/wiki/LU_decomposition

%---------------------------------------------------------------------------------------------------
\subsubsection{QR Decomposition}
% Orthonormal, Upper Triangular
% I think, the R stands for right-triangular?

%https://en.wikipedia.org/wiki/QR_decomposition

%---------------------------------------------------------------------------------------------------
\subsubsection{Eigendecomposition aka Diagonalization}
The most convenient decomposition of a matrix in terms of ease of solving equations involving the matrix and in terms of geometric interpretability of what the matrix actually does is the so called diagonalization of the matrix. ...TBC...

% it's also called Eigendecomposition or Spectral Decomposition

\paragraph{Eigenvalues}

\paragraph{Eigenvectors}

% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization

% https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
% https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix


%---------------------------------------------------------------------------------------------------
\subsubsection{Jordan Normal Form}
A diagonalization of a matrix is unfortunately not always possible [TODO: state conditions]. A Jordan normal form is one of the two second best things that we can do ...TBC...




\paragraph{Generalized Eigenvectors}

% interpretation of the Jordan cells as representing a pair of complex conjugate eigenvalues?
% is the failure to be diagonalizable due to complex eigenvalues? Nah - I think, it's due to
% multiplicity

% Here:
% https://en.wikipedia.org/wiki/Matrix_similarity
% it says that over the complex numbers, every matrix is similar to a Jordan Normal Form

% https://en.wikipedia.org/wiki/Jordan_normal_form
% https://en.wikipedia.org/wiki/Generalized_eigenvector

% Jorand-Arnold Canonical Form (Mathe mit 2x2 Matrizen pg 44)

%---------------------------------------------------------------------------------------------------
\subsubsection{SVD - Singular Value Decomposition}
% Orthonormal, Diagonal, Orthonormal

The other second best thing we can do when a matrix is not diagonalizable 


%---------------------------------------------------------------------------------------------------
\subsubsection{Additive Decompositions}

% symmetric/antisymmetric
% Jordan-Chevalley (SN) decomp: A = S + N where S is diagonalizable, N is nilpotent and 
% N and s commute: SN = NS (Mathe mit 2x2 Matrizen, pg 53)
% https://en.wikipedia.org/wiki/Jordan%E2%80%93Chevalley_decomposition

%===================================================================================================
\subsection{Important Facts and Formulas}
% rank-nullity theorem

%===================================================================================================
%\subsection{Computing with Spaces}


%On a beginner level, one usually assumes to deal regular matrices and as soon as one encounters a singular matrix, one throws the towel

%and just says things like "there are no solutions"

% Computing with spaces - operations like perp (orthogonal complement)

\begin{comment}
Explain what happens to the eigenvalues when we do certain things to a matrix (shifts, etc.)
I have a list of that in some text file. Shifting eigenvalues and manipulating them in other ways
can be important to improve convergence of numerical algrithms

Other possibly relevant matrix types to mention:
https://en.wikipedia.org/wiki/Companion_matrix
https://en.wikipedia.org/wiki/Smith_normal_form

\end{comment}