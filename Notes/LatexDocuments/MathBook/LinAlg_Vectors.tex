\section{Vectors and Matrices}

\paragraph{Vectors}
Consider a point in the 2D plane or in 3D space. To represent such a point mathematically, we would need a pair or a triple of numbers respectively. An $n$-dimensional vector can generally be thought of as an $n$-tuple of numbers. For intuition building, it's best to think of real numbers although in general, other kinds of numbers may also be allowed in the more general case. ...TBC...

% give the standard basis (1,0),(0,1) of R^2, explain

%A vector may not only represent a point itself, but also a translation ...tbc...


\paragraph{Matrices}
Vectors can be used to represent locations and translations. Translations are a specific kind of geometric transformation. A matrix represents a different kind of transformation, namely a \emph{linear transformation}, that we can apply to a vector. In geometric terms, linear transformations are scalings, rotations and shears. ...TBC...

% the columns of the matrix say where the basis vectors go

%Translations are not among the linear transformations that we can represent by matrices.


\paragraph{Well, actually...}
Strictly speaking, an $n$-tuple of numbers is not what a vector really $is$ by its nature. By its nature, a vector is a geometric entity such as a point in space or an arrow with a direction and length. The $n$-tuple of numbers is a specific representation of that geometric entity that depends on the coordinate system that we have chosen. But take that statement as a foreshadowing to a more advanced viewpoint. For the purposes of this section, it's totally okay to picture a vector as a tuple of numbers. ...TBC...

% Likewise, matrices are also only a specific representation of a linear transformation ...

\medskip
Moreover, when we look at things in a more carefully, we need to distinguish between points and vectors. ...TBC...


%===================================================================================================
%\subsection{Vectors}

%===================================================================================================
%\subsection{Matrices}






%===================================================================================================
\subsection{Operations}

%---------------------------------------------------------------------------------------------------
\subsubsection{Vector Operations}

\paragraph{Vector Addition}
Algebraically, there is not much to say about vector addition - we just add the tuples element-wise and that's it. We can interpret such a vector addition geometrically at placing the vectors tip to tail. ...TBC... [ToDo: insert figure with the parallelogram picturing $\mathbf{a + b}$ and $\mathbf{b + a}$].

\paragraph{Scalar Multiplication}

\paragraph{Scalar Product}

\paragraph{Vector Products}
% cross-product, triple-product, wedge-product

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Operations}
\paragraph{Addition and Multiplication}
\paragraph{Inversion}
% could be done via Gauss-Jordan Elimination Algorithm decribed belo



%===================================================================================================
\subsection{Linear Systems of Equations}
One important area of application of matrices and vectors is the solution of systems of linear equations

% solvability, rank (may also be filed under matrix features - maybe introduce the concept here and mnetion it there again)


%===================================================================================================
\subsection{Special Features}

%---------------------------------------------------------------------------------------------------
\subsubsection{Vector Features}

\paragraph{Vector Norms}
The norm of a vector captures its length. The most common norm is the Euclidean norm but there are others as well...TBC...
% are a measure of length, $L_p$-norm, explain the general requirements to a norm

\paragraph{Orthogonality and Angles}
% Angles between a are defined in terms of the cosine of the norm
% 

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Features}

\paragraph{Determinant}
% regular/singular

\paragraph{Norms}
% explain norm compatibility with vector norms

\paragraph{Orthogonality}
% a (square) matrix is orthogonal when all its rows are mutually orthogonal. This implies

\paragraph{Symmetry}

\paragraph{Similarity}
A matrix $\mathbf{A}$ is said to be \emph{similar} to another matrix $\mathbf{B}$, if there exists an invertible matrix $\mathbf{P}$ such that $\mathbf{B} = \mathbf{P^{-1} A P}$. We denote this by $\mathbf{A} \sim \mathbf{B}$ which we read as "A is similar to B" [VERIFY!]. Matrix similarity is an equivalence relation, so we have $\mathbf{A} \sim \mathbf{B} \Leftrightarrow \mathbf{B} \sim \mathbf{A}$. We can actually solve for $\mathbf{A}$ by pre-multiplying both sides by $\mathbf{P}$ and post-multiplying both sides by $\mathbf{P}^{-1}$ to obtain $\mathbf{A} = \mathbf{P B P^{-1}}$. The transformation That maps $\mathbf{A}$ to $\mathbf{B}$ is called a \emph{similarity transformation} or a \emph{conjugation}. Conjugation is a term from group theory which we will explain later.

\medskip
Now, this is a rather implicit definition "there exists a matrix $\mathbf{P}$ such that..." and in practice, we may want to know (1) How can we decide, whether or not such a matrix exists? (2) If it does exist, is it unique? (3) If it is unique, what is it? If it isn't unique, what matrices are possible? Generally, if two matrices are similar, they represent the same geometric transformation but expressed in different coordinate systems. The matrix $\mathbf{P}$ is the matrix that transforms from one coordinate system to the other [TODO: be more specific: which way?] and is called the \emph{change of basis} matrix. In typical practical situation, this will probably be given so we don't really need to compute it from $\mathbf{A,B}$. But if we really encounter a situation where we need to, this is how it can be done:

...TBC...ToDo: give algorithm to compute $\mathbf{P}$, given $\mathbf{A,B}$
%If two matrices are similar

% How to find P: write the relation as PA = PB  ->  (PA - PB) = 0  ->  P(A-B) = 0
% nah! only true if PA = AP, in general we have
% PB = AP
% https://math.stackexchange.com/questions/14075/how-do-i-tell-if-matrices-are-similar
% says the solution may not be unique - the system to solve may be singular. But the zero matrix is always a solution. A solution algo that computes a minimum norm solution would probably pick the zero matrix? ...yeah - it can't be unique because we see immediately that P could be multiplied by any scalar factor. Maybe pick a solution that has unit norm...I guess this would mean unit-Frobenius norm? 


% https://en.wikipedia.org/wiki/Matrix_similarity


\paragraph{Misc Special Matrices}
% Toeplitz, circulant, unitary (maybe file under orthogonal - it's the complex version), tringular, ...










%===================================================================================================
\subsection{Matrix Decompositions}


%---------------------------------------------------------------------------------------------------
\subsubsection{Eigendecomposition aka Diagonalization}

% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization
% https://en.wikipedia.org/wiki/Matrix_similarity


%---------------------------------------------------------------------------------------------------
\subsubsection{Jordan Normal Form}
A diagonalization of a matrix is unfortunately not always possible [TODO: state conditions]. A Jordan normal form is one of the two second best things that we can do ...TBC...

% interpretation of the Jordan cells as representing a pair of complex conjugate eigenvalues?

%---------------------------------------------------------------------------------------------------
\subsubsection{Singular Value Decomposition}



%---------------------------------------------------------------------------------------------------
\subsubsection{Additive Decompositions}



%===================================================================================================
\subsection{Important Facts and Formulas}
% rank-nullity theorem

%===================================================================================================
\subsection{Computing with Spaces}


%On a beginner level, one usually assumes to deal regular matrices and as soon as one encounters a singular matrix, one throws the towel

%and just says things like "there are no solutions"

% Computing with spaces - operations like perp (orthogonal complement)