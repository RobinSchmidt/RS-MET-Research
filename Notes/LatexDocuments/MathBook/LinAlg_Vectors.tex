\section{Vectors and Matrices}

\paragraph{Vectors}
Consider a point in the 2D plane or in 3D space. To represent such a point mathematically, we would need a pair or a triple of numbers respectively. An $n$-dimensional vector can generally be thought of as an $n$-tuple of numbers. For intuition building, it's best to think of real numbers although in general, other kinds of numbers may also be allowed in the more general case. ...TBC...

% give the standard basis (1,0),(0,1) of R^2, explain

%A vector may not only represent a point itself, but also a translation ...tbc...


\paragraph{Matrices}
Vectors can be used to represent locations and translations. Translations are a specific kind of geometric transformation. A matrix represents a different kind of transformation, namely a \emph{linear transformation}, that we can apply to a vector. In geometric terms, linear transformations are scalings, rotations and shears. ...TBC...

% the columns of the matrix say where the basis vectors go

%Translations are not among the linear transformations that we can represent by matrices.


\paragraph{Well, actually...}
Strictly speaking, an $n$-tuple of numbers is not what a vector really $is$ by its nature. By its nature, a vector is a geometric entity such as a point in space or an arrow with a direction and length. The $n$-tuple of numbers is a specific representation of that geometric entity that depends on the coordinate system that we have chosen. But take that statement as a foreshadowing to a more advanced viewpoint. For the purposes of this section, it's totally okay to picture a vector as a tuple of numbers. ...TBC...

% Likewise, matrices are also only a specific representation of a linear transformation ...

\medskip
Moreover, when we look at things in a more carefully, we need to distinguish between points and vectors. ...TBC...


%===================================================================================================
%\subsection{Vectors}

%===================================================================================================
%\subsection{Matrices}






%===================================================================================================
\subsection{Operations}

%---------------------------------------------------------------------------------------------------
\subsubsection{Vector Operations}

\paragraph{Vector Addition}
Algebraically, there is not much to say about vector addition - we just add the tuples element-wise and that's it. We can interpret such a vector addition geometrically at placing the vectors tip to tail. ...TBC... [ToDo: insert figure with the parallelogram picturing $\mathbf{a + b}$ and $\mathbf{b + a}$].

\paragraph{Scalar Multiplication}

\paragraph{Scalar Product}

\paragraph{Vector Products}
% cross-product, triple-product, wedge-product

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Operations}
\paragraph{Addition and Multiplication}
\paragraph{Inversion}
% could be done via Gauss-Jordan Elimination Algorithm decribed belo



%===================================================================================================
\subsection{Linear Systems of Equations}
One important area of application of matrices and vectors is the solution of systems of linear equations

% solvability, rank (may also be filed under matrix features - maybe introduce the concept here and mnetion it there again)


%===================================================================================================
\subsection{Special Features}

%---------------------------------------------------------------------------------------------------
\subsubsection{Vector Features}

\paragraph{Vector Norms}
The norm of a vector captures its length. The most common norm is the Euclidean norm but there are others as well...TBC...
% are a measure of length, $L_p$-norm, explain the general requirements to a norm

\paragraph{Orthogonality and Angles}
% Angles between a are defined in terms of the cosine of the norm
% 

%---------------------------------------------------------------------------------------------------
\subsubsection{Matrix Features}

\paragraph{Determinant}
% regular/singular

\paragraph{Norms}
% explain norm compatibility with vector norms

\paragraph{Orthogonality}
% a (square) matrix is orthogonal when all its rows are mutually orthogonal. This implies

\paragraph{Symmetry}

\paragraph{Similarity}
A matrix $\mathbf{A}$ is said to be \emph{similar} to another matrix $\mathbf{B}$, if there exists an invertible matrix $\mathbf{P}$ such that $\mathbf{B} = \mathbf{P^{-1} A P}$. We denote this by $\mathbf{A} \sim \mathbf{B}$ which we read as "A is similar to B" [VERIFY!]. Matrix similarity is an equivalence relation, so we have $\mathbf{A} \sim \mathbf{B} \Leftrightarrow \mathbf{B} \sim \mathbf{A}$. We can actually solve for $\mathbf{A}$ by pre-multiplying both sides by $\mathbf{P}$ and post-multiplying both sides by $\mathbf{P}^{-1}$ to obtain $\mathbf{A} = \mathbf{P B P^{-1}}$. 
%If two matrices are similar


\paragraph{Misc Special Matrices}
% Toeplitz, circulant, unitary (maybe file under orthogonal - it's the complex version), tringular, ...










%===================================================================================================
\subsection{Matrix Decompositions}


%---------------------------------------------------------------------------------------------------
\subsubsection{Eigendecomposition aka Diagonalization}

% https://en.wikipedia.org/wiki/Diagonalizable_matrix#Diagonalization
% https://en.wikipedia.org/wiki/Matrix_similarity


%---------------------------------------------------------------------------------------------------
\subsubsection{Jordan Normal Form}
A diagonalization of a matrix is unfortunately not always possible [TODO: state conditions]. A Jordan normal form is one of the two second best things that we can do ...TBC...

% interpretation of the Jordan cells as representing a pair of complex conjugate eigenvalues?

%---------------------------------------------------------------------------------------------------
\subsubsection{Singular Value Decomposition}



%---------------------------------------------------------------------------------------------------
\subsubsection{Additive Decompositions}



%===================================================================================================
\subsection{Important Facts and Formulas}
% rank-nullity theorem

%===================================================================================================
\subsection{Computing with Spaces}


%On a beginner level, one usually assumes to deal regular matrices and as soon as one encounters a singular matrix, one throws the towel

%and just says things like "there are no solutions"

% Computing with spaces - operations like perp (orthogonal complement)