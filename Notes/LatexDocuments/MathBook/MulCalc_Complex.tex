\section{Complex Calculus}
Complex calculus is the calculus of functions that map a complex number to another complex number. Since input and output are both 2-dimensional, it makes sense to look at complex calculus from the perspective of 2D vector fields. For a function $w = f(z) = u(x,y) + \i v(x,y)$ to be differentiable in the complex sense, the partial derivatives of the component functions $u,v$ with respect to $x,y$ must exist and satisfy the Cauchy-Riemann (CR) differential equations: $u_x = v_y, u_y = -v_x$. This condition has wide ranging implications which allow a lot of simplifications to be made in the computations of path integrals. If the function $f$ is considered as a mapping that maps points from the $z$-plane to the $w$-plane, the CR equations imply that the images of curves in the $z-$plane that intersect at a given angle, will intersect at the same angle in the $w$-plane. Such an angle-preserving mapping is also called a conformal mapping. If a function is complex differentiable at a given point $z_0$, this implies that all higher order derivatives also exist and that the function can be locally expanded in a Taylor series that converges in a circular region whose radius extends to the nearest singular point. If a function can be expanded in a convergent Taylor series around some point $z_0$, it is said to be \emph{analytic} at that point. A singluar point is a point is a point where the function "misbehaves" in some way. In most cases, the misbehavior is due to a division by zero. For example, the (real) function $f(x) = 1 / (1 - x^2)$ has singular points at $x = \pm 1$. Of course, we can also regard $f$ as a complex function by just allowing $x$ to be complex. But in that case, it is more common to use $z$ instead of $x$ for the name of the independent variable. The complex function $f(z) = 1 / (1 + z^2)$ has no singular points along the real axis but it does have singularities $z = \pm \i$. We will find that line integrals similar to those from $2D$ vector calculus will have a value of zero around any closed loop when that loop does not contain any singularities. And when the loop does contain singularities, the integral can be evaluated via a simple sum over some special values, called \emph{residues}, which are associated with these singularities and are comparatively easy to compute. That fact is what helps to simplify the evaluation of some definite integrals. With some ingenuity, it will even help to evaluate certain real integrals which would be hard to evaluate otherwise. Path integrals over paths that are not closed loops are actually path independent and given by simple differences between values at the endpoints of an antiderivative - just like in $1D$ calculus. But we are in $2D$, so a complex antiderivative is actually more like a potential....tbc...


\subsection{Complex Functions}
Complex functions are functions $f$ into which we can plug in a complex number $z$ as input and get out another complex number $w$ as output. We write $w = f(z)$. If we split input and output into their real and imagniray parts: $z = x + \i y, w = u  + \i v$, we can write the complex function in terms of two bivariate component functions: $u = u(x,y), v = v(x,y)$ such that $w = f(z) = f(x + \i y) = u(x,y) + \i v(x,y)$. In this form, a complex function is quite reminiscent of a vector field in $2D$ and in our interpretation of complex path integrals, we will indeed refer back to our intuitions about path integrals in $\mathbb{R}^2$. Consider the simple function $f(z) = z^2$. If we write $z = x + \i y$, this becomes $f(z) = (x + \i y)^2 = (x^2 - y^2) + (2 x y ) \i$ where I used some superfluous parentheses to emphasize, how we may think of $f$ as having a real part of $u(x,y) = x^2 - y^2$ and an imaginary part of $v(x,y) = 2 x y$.

% explain Polya vector fields. explain, how to split a function such as z^2 into its real and imaginary component functions x^2 - y^2, 2 x y 

%\subsubsection{Polynomials}

% The Fundamental Theorem of Algebra
% https://www.youtube.com/watch?v=RBRVL6nP2Dk
% Winding numbers



\subsubsection{Rational Functions}
Among the simplest functions are monomials, i.e. functions of the type $f(z) = a z^n$ for some fixed complex number $a$ and some integer $n$. We already know how to multiply together two complex numbers which means we also know how to multiply a complex number $z$ by itself several times to form $z^n$. So we have everything we need to evaluate the expression $a z^n$ and the result will be another complex number. That means, this expression defines a complex function and so it is one of specimen we are going to investigate here. We also know how to add complex numbers, so we can also already take sums of terms of the form above. Such weighted sums of powers of the input variable are just our good old friends, the polynomials - just now with complex inputs and outputs and possibly also with complex coefficients. Complex division is also already defined, so we can also divide the outputs of two polynomials. That means, rational functions of a complex variable are also already defined wihtout requiring us to think much about it. In summary, for complex functions $w = f(z)$ of the form:
\begin{equation}
f(z) = \frac{\sum_{k=0}^{M} a_k z^k}{\sum_{k=0}^{N} b_k z^k}
\end{equation}
we already know what we would have to do to evaluate them for any given input $z$. For other types of functions, we'll have to put in some more thought...

% https://en.wikipedia.org/wiki/Linear_fractional_transformation
% https://en.wikipedia.org/wiki/Bilinear_transform

% maybe tackle the important function f(z) = 1/z, i.e. complex inversion, Moebius transformations


\subsubsection{Roots, Multifunctions and Algebraic Functions}
Consider the square root function again. In the definition of the real valued $f(x) = \sqrt{x}$, we already saw a foreshadowing of a potential problem. First of all, we noticed that some real numbers - namely the negative numbers - had no square roots at all. Some other real numbers - the positive ones - actually had two square roots. The two square roots of $4$ are $+2$ and $-2$, for example. We kind of dodged these issues by just saying that for negative inputs, the square root is undefined and for positive input, \emph{the} square root is \emph{defined} to be the positive solution. In the context of complex analysis, we will have to take this issue more seriously. 

\medskip
Let's first consider the problem of being unable to meaningfully define the square root for negative inputs. In the realm of complex numbers, that problem goes away completely. The purpose of defining $\i = \sqrt{-1}$ was precisely to deal with that situation. To evaluate $\sqrt{-4}$, we re-express it as $\sqrt{4} \sqrt{-1}$ using our usual laws for square roots to arrive at the solution $\sqrt{-4} = 2 \i$. And in case you wonder (you should!): yes - it is indeed allowed to use these laws. They continue to hold in the complex case. But again, $2 \i$ is not the only possible solution and $-2 \i$ is just as good a solution. Again, we actually have found two solutions. You can verify this by just squaring $2 \i$ and $-2 \i$. In both cases, you'll get $-4$.

\paragraph{Multi Valued Functions}
Secondly, let's think about what to make of the fact that we got not one but two values. When I said "solution", you may have asked yourself "solution to what equation"? It is actually a bit sloppy to say $\i = \sqrt{-1}$ and it would be more precise to say that $\i$ is some number that solves the equation $x^2 = -1$ or, equivalently, to  $1 + x^2 = 0$ and $-\i$ is actually a second solution to that (quadratic) equation. Recall that a general quadratic equation of the form $a_0 + a_1 x + a_2 x^2 = 0$  always has two solutions given by the quadratic formula and that in general, according to the fundamental theorem of algebra, a polynomial $p(z)$ of degree $n$ has exactly $n$ complex roots (i.e. solutions to $p(z) = 0$) when we count them with their respective multiplicities. When we are asked to take an $n$th root of some number $c$, i.e. we want to compute $\sqrt[n]{c}$, we can rephrase this question as: "what numbers $z$ have the property that $z^n = c$?" or, equivalently, "what numbers $z$ solve the equation $z^n - c = 0$". Of course, we recognize this as a special case of trying to find the roots of a polynomial of degree $n$ and so we expect $n$ solutions. For a general polynomial of degree $n$, no elementary closed form formulas exist for $n \geq 5$, but for our special case, we have such a formula. Our $n$ roots are given by:
\begin{equation}
 z^n = c = r e^{\i \phi} \quad \Leftrightarrow \quad
 z = \sqrt[n]{c} = \sqrt[n]{r} e^{\i \frac{\phi + 2 \pi k}{n}} \quad \text{for } k = 0,\ldots,n-1
\end{equation}
where we have expressed $c$ in its polar form as $c = r e^{\i \phi}$ with $r = |c|, \phi = \angle c$. The factor $\sqrt[n]{r}$ is the square root of the magnitude of $c$ and that magnitude is just a nonnegative real number, so we know how to take that root. It's just our old $n$th root that we already know from real analysis. The other factor $e^{\i \frac{\phi + 2 \pi k}{n}}$ is new though, so let's give it some attention. First of all, we note it contains a dummy variable $k$ which is supposed to be an integer in the range $0,\ldots,n-1$. That means, we actually get $n$ different solutions - one for each value of $k$. For $k = 0$, the solution simplifies to $\sqrt[n]{r} e^{\i \frac{\phi}{n}}$ which we recognize as a complex number whose radius is given by the $n$th root of the radius of the input and whose angle is given by the angle of the input divided by $n$. For $k=1$, the radius of the solution is the same but the angle is increased by an increment of $2\pi / n$. For $k=2$, the angle is increased by another such increment and so on up to $k=n-1$. Would we plug in $k=n$, we would have come full circle and back to our first $k=0$ solution, so values of $k \geq n$ do not give us any new solutions. In principle, we could allow $k$ to be any integer, but we would just cyclically get the same solutions over and over. Negative $k$ would also not yield anything new either. Stepping $k$ through $-1,-2,\ldots$, we'd just cycle through our set of solutions backwards. So, it's sensible to restrict $k$ to the range $0,\ldots,n-1$ because that range covers all of our $n$ solutions. These $n$ solutions lie all on a circle of radius $\sqrt[n]{r}$ and are distributed equidistantly around the circle, separated by an angle increment of $2 \pi / n$. The initial angle, i.e. the one of the first solution, is given by $\phi / n$.

\medskip


%
...TBC...

\paragraph{The Roots of Unity}
Let's have a look at the equation $z^n = 1$ for some given positive integer $n$. The complex numbers that solve that equation are called the $n$th roots of unity. Geometrically, they are the vertices of an $n$-sided regular polygon that has one of its corners at $z = 1$. ...tbc...

%\medskip
%Indeed, the equation $z^n = c$ has $n$ solutions





%The problem of not being able to define a square root (or, in fact $n$th root) for ceratin inputs will vanish - in the complex domain, negative numbers do have square roots, albeit "imaginary" ones. 

% principal values, Riemann surfaces, primitive roots
% https://en.wikipedia.org/wiki/Riemann_surface

\subsubsection{Exponential and Trigonometric functions}
Extending the domains of rational and algebraic functions from the real to the complex numbers was a straightforward process requiring only some algebra. The $n$th roots are a bit messy to deal with because we needed to introduce the concept of multivalued functions but they nevertheless did not require much higher level math tools to define. To extend the domains of the exponential and trigonometric functions, we'll need a bigger gun from calculus: power series. The nice thing about power series is that in order to evaluate them, we only need to know how to perform additions and multiplications - and we actually indeed do know, how to add and multiply complex numbers! Using the power series representation of a function is a very common technique in math whenever we desire to define functions of arguments that are not simple numbers. The same idea can also be applied to matrices, multivectors and even operators. But here, we are interested in complex numbers. Let's recall the power series of some functions:
\begin{equation}
 \exp(x) = e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!}, \quad
 \sin(x) =  \sum_{k=0}^{\infty} ...
\end{equation}
% add sin, cos, maybe sinh, cosh
The idea is now to simply allow the input $x$ to be complex valued to define these functions for complex arguments. We would have to take care about the question of convergence but it turns out that these particular power series are very well behaved in that regard. They actually have an infinite radius of convergence....tbc
% Extending the domain of the real valued functions via their power series...



\subsubsection{The Complex Logarithm}
% Log - as infinite-valued multifunction. log(r e^{\i \phi}) = \log(r) + \i phi


\subsection{Complex Differentiation}
Having a little arsenal of complex functions $f: \mathbb{C} \rightarrow \mathbb{C}$ at our disposal, we can now start to do some calculus with them. Naturally, the first thing we may want to do, is to define a notion of a derivative for complex functions. In analogy to real valued functions, we define the complex derivative as:
\begin{equation}
  f'(z)	= \lim_{\Delta z \rightarrow 0 }
  \frac{f(z + \Delta z) - f(z)}{\Delta z}
\end{equation}
where $\Delta z$ is some arbitrary (small) complex number. For this derivative to be well defined, we must make sure that this limit does not depend on the direction of $\Delta z$. Recall our definition of differentiability for vector fields in 2D. For a function $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ with $f(x,y) = (u(x,y), v(x,y))$ to be differentiable at some point $(x_0, y_0)$, we demanded that the partial derivatives $\partial u / \partial x, \partial u / \partial y, \partial v / \partial x, \partial v / \partial y$ need to exist at that point. For a complex function $f: \mathbb{C} \rightarrow \mathbb{C}$ with $f(z) = f(x + \i y) = u(x,y) + \i v(x,y)$, we will also require the existence of these partial derivatives for which we will use the more convenient notation $u_x, u_y, v_x, v_y$. For $f$ to be called complex differentiable, the existence of the partial derivatives is not enough. The independence of the limit from the direction of $\Delta z$ must also be ensured. This additional requirement is captured in...

\subsubsection{The Cauchy-Riemann Equations}
A complex function $f(z) = f(x + \i y) = u(x,y) + \i v(x,y)$ is said to be complex differentiable, iff the four possible partial derivatives $u_x, u_y, v_x, v_y$ exist and satisfy the system of equations:
\begin{equation}
 u_x = v_y, \; u_y = -v_x
\end{equation}
These equations are called the \emph{Cauchy-Riemann equations}. It is a system of two partial differential equations that puts some quite rigid constraints on the partial derivatives. Note that for a general 2D vector field, the Jacobian matrix has four independent entries, i.e. four degrees of freedom. When we collect the partial derivatives of a complex function into a Jacobian matrix, we are effectively demanding that the matrix may only have two degrees of freedom. If we would pick $u_x$ and $u_y$ freely, then $v_x$ and $v_y$ would automatically follow, for example. Our Jacobians must look like:
\begin{equation}
 \mathbf{J} =   \begin{pmatrix} u_x & u_y \\	 v_y & v_y \end{pmatrix}
            =   \begin{pmatrix} u_x & u_y \\	-u_y & u_x \end{pmatrix}
\end{equation}
Geometrically, matrices of this form represent a rotation paired with a uniform scaling and can be expressed as:
\begin{equation}
	 \mathbf{J} = r \begin{pmatrix}  \cos(\phi)   & \sin(\phi)   \\	
                              	 	-\sin(\phi)   & \cos(\phi)   \end{pmatrix}
	\quad \text{where}  \quad
	r = \sqrt{u_x^2 + u_y^2}, \; \phi = \atan2(u_y, u_x)
\end{equation}
Such a geometric transformation might also be called an \emph{amplitwist}. This term was first suggested by Tristan Needham in his excellent book "Visual Complex Analysis" and I like its descriptiveness. Note also that such an amplitwist is precisely the sort of transformation that a multiplication by a complex number achieves. That complex number is of course none other than the value of our derivative. ...tbc...

%From an information theoretic point of view, it makes sense that the Jacobian can only have two degrees of freedom if we assume that we want to encode the derivative in a complex number. This is precisely what we want to do here: The derivative of some complex function $f$ at some value $z_0$ should just be a complex number, i.e. an object with two degrees of freedom rather than the four which a general, unconstrained Jacobian would have.

\paragraph{Example} Let's take $f(z) = z^2$ as an example function. Expressing it as $f(x + \i y) = u(x,y) + \i v(x,y)$, we find that $u(x,y) = x^2 - y^2, v(x,y) = 2 x y$. The partial deriavtives are given by $u_x = 2 x, u_y = - 2 y, v_x = 2 y, v_y = 2 x$. We verify that the Cauchy-Riemann equations are indeed satisfied for this function.

% Try to find a scalar potential u for the vector field (u_x, u_y) = (2x, -2y) and another scalar potential v for the vector field (v_x, v_y) = (2x, 2y). Try also to find a scalar potential for (x^2-y^2, 2 x y)

% What about the idea of trying to find a vector field when a Jacobian is given? Maybe it can be traced back to finding scalar potential for the rows (or columns) of the Jacobian?

% This has also a potentially interesting formula:
% https://en.wikipedia.org/wiki/Differentiable_function#Differentiability_in_complex_analysis
% ah - no - that's actually nto specific to the complex case. It's quite generall true for derivatives - see Wietz, DiffGeo, Eq 9.3

% mention connection to harmonic functions, Laplace equation, harmonic conjugates

%

% https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations

% -If f is complex diffable, the df / dz* is zero, where z* is the complex conjugate of z

\subsubsection{Holomorphic, Meromorphic and Analytic Functions}
It's time to define some more jargon. A complex function is said to be \emph{holomorphic} on some domain iff it is complex differentiable on that domain. If it is holomorphic on that domain except for some isolated poles, then it is said to be \emph{meromorphic} on that domain. A function is said to be \emph{analytic}, if it can be locally expanded into a convergent Taylor series. In the context of complex analysis, holomorphy and analyticity imply each other and are therefore often used interchangably. That means: to show that a complex function is analytic (i.e. Taylor expandable), it is enough to show that it is holomorphic (i.e. that its complex derivative exists). In more general multivariable calculus contexts, the requirement of being Taylor expandable is a much stronger requirement than just mere differentiability but in complex analysis, differentiability already comes with big implications, one of them being Taylor expandibility.

\subsection{Complex Integration}

% I think, when we split the integral into its real and imaginary parts, the real part can be intepreted as a work integral along the path and the imaginary part as a flux integral across the path?

% Consider the Polya vector field of $f$:
% u(x,y) =  \Re(f(z)) =  \Re(f(x + \i y))
% v(x,y) = -\Im(f(z)) = -\Im(f(x + \i y))


% Maybe at first, consider an integral along a straight line between two complex numbers $a,b$ parametrized as $c(t) = (1-t) a + t b$ for $t \in [0,1]$. We can then approximate more complicated paths by a bunch of such straight line segements.

\subsubsection{The Polya Vector Field}

% Irrotational vector fields have a symmetric Jacobian (by the second fundamental theorem for potential fields). Holomorphic functions have antisymmetric Jacobians. Maybe that's the reason we need to negate the imaginary part? Wait - no - it's actually antisymmetric plus a nonzero main diagonal

\paragraph{The Polya Potential}


\subsubsection{Cauchy's Integral Formula}

\subsubsection{Cauchy's Theorem}


\subsection{Laurent Series}

% Hauptteil, Nebenteil (in German) - maps to poles and zeros? Poles are more important, that's why 
% the part defined by  them is the Hauptteil?

\subsection{The Residue Theorem}

\subsection{Analytic Continuation}
As a motivating example, consider the function $f(s)$ defined by the infinite sum:
\begin{equation}
f(s) = \sum_{n=1}^{\infty} = \frac{1}{n^s}
\end{equation}
The usage of $s$ as argument is just a common convention for this particular function which is known as the \emph{Riemann zeta function} and also often denoted as $\zeta (s)$. It is indeed a well defined and even analytic function whenever that infinite sum converges. As we know from our discussion in the series section, this sum does indeed converge for $s > 1$ when we assume $s$ to be real. More generally, if we allow $s$ to be complex, the condition for convergence is that the real part of $s$ must be greater than one. For inputs with real part less than or equal to one, the function is as of yet undefined. However, complex analysis provides us with a technique by which we can uniquely extend the domain of the function to almost the whole complex plane. That technique is called \emph{analytic continuation}. As said, for $\Re(s) > 1$, $f$ is indeed an analytic function. ...

% https://en.wikipedia.org/wiki/Identity_theorem

% We have been able to extend the domains of exp and sin/cos by means of power series. that worked out because the power series of exp and sin/cos were nice enough to have an infinite radius of convergence around the expansion point zero (and, in fact, around any other expansion point as well?). 

% Hmm...but the Riemann Zeta function is not really a power series. At least not in z - but it is one in 1/z. Actually, the simplemost one - with all coeffs being 1. It's like a Laurent series with zero coeffs a_n for n >= 0 and unit coeffs fo n < 0, right? ...wait - no! That would be the case if it would be a sum over 1/s^n but it is actually a sum over 1/n^s

% explain the mapping theroem (Riemann?) see Arens add-on material (I think) - something about polygons in the complex plane

% Analytic Continuation and the Zeta Function
% https://www.youtube.com/watch?v=CjSKmcWRFzE




\subsection{Applications}
Functions of complex variables have numerous applications in fields like digital signal processing, fluid dynamics, quantum mechanics, (real) integration, differential equations, number theory etc. We'll now look at some of them.

\subsubsection{Rational Functions and Filter Theory}

\subsubsection{Elliptic Functions and Filter Design}

\subsubsection{Modular Forms and Number Theory}

\subsubsection{Conformal Mapping and Geometry}
% or maybe "Conformal Mapping and Fluid Dynamics"


%\subsection{Important Classes of Complex Functions}



% Some special cases of rational functions are important enough to have special names. When both numerator and denominator are only first orders polynomials, i.e. the function is of the form $f(z) = (a z  + b) / (c z + d)$, the function is also called a \emph{Moebius transformation}. In some contexts, such functions are also called \emph{linear fractional transform} and the so called \emph{bilinear transform} in digital signal processing is also a special case of such a function. Speaking of signal processing, rational functions where numerator and denominator are quadratic polynomials are alse quite common there. They occur as transfer functions of so called \emph{biquad} filters (for biquadratic) which can be sued as basic building block for all kinds of filters. Those more general filters have also rational functions as transfer functions. 


% https://en.wikipedia.org/wiki/Kramers%E2%80%93Kronig_relations

% https://www.johndcook.com/blog/applied-complex-analysis/


\begin{comment}
	
Make a section about some specific complex functions of interest:
-Moebius transforms, Modular forms
-Elliptic functions. They seem to be related to elliptic curves:	
 https://en.wikipedia.org/wiki/Elliptic_curve#Elliptic_curves_over_the_complex_numbers	
-L-functions / zeta functions	
	
% But is it not really like a scalar potential (via gradient) or vector potential (via curl) - right? ToDo: figure out the exact connection between potentials and complex antiderivatives. What would the scalar potential of the vector field f(x,y) = (x^2-y^2, 2xy) be? It comes from the complex function z^2. How does it relate to the antiderivative z^3/3 of z^2?

% It has been said, that a power series around a point where $f$ is analytic converges in a disc around the expansion center that extends to the nearest singularity. What, if we try to epand the function aorund such a singular point itself? Predictably, it doesn't work - but a generalization that allows also for negative powers, called a Laurent series, may work.

% How do we find the Laurent coeffs? Apparently, we cant just evaluate the function or derivatives at the point because the function will typically be undefined at that point. It's via an integral:

% https://en.wikipedia.org/wiki/Laurent_series
% https://mathworld.wolfram.com/LaurentSeries.html

% Does this mean, this integral for a_n aggrees with the derivative for n >= 0 for regular points? Try to apply the integral formula for the coeffs to some smooth function like sin or exp. the resulting coeffs should 0 for n < 0 and agree with the Taylor coeffs for n >= 0, I guess? Yes - indeed - see formula 5 here:

% https://math.mit.edu/~jorloff/18.04/notes/topic7.pdf

% Motivate Laurent series with the example f(x) = (1+x)/(x^3-x^2) or (1+x)/(x^3+x^2) and try to expand it as power series around x0=0. We will see that we will need to allow for negative powers. Maybe for the example, the coeffs can be found by partial fraction expansion?

% https://en.wikipedia.org/wiki/Singularity_(mathematics)#Complex_analysis
% https://complex-analysis.com/content/classification_of_singularities.html
% https://www.britannica.com/topic/singularity-complex-functions
% https://en.wikipedia.org/wiki/Critical_point_(mathematics)
% https://en.wikipedia.org/wiki/Singular_point_of_a_curve
% https://en.wikipedia.org/wiki/Radius_of_convergence#Radius_of_convergence_in_complex_analysis
% https://mathworld.wolfram.com/Singularity.html


%todo: say something about multivariable complex calculus

% e^(i*w) = i^(2w/pi)
% https://www.youtube.com/watch?v=3geVAJvJM8c  5:10
%
% a^(i*t) goes around the uni circle with speed depending on a
% https://www.youtube.com/watch?v=UOuxo6SA8Uc  25:30

% https://www.youtube.com/playlist?list=PLDcSwjT2BF_UDdkQ3KQjX5SRQ2DLLwv0R
% Essence of complex analysis  by Mathemaniac, uses a vector-field approach	


	
	
https://en.wikipedia.org/wiki/Elliptic_function
https://en.wikipedia.org/wiki/Modular_form
https://en.wikipedia.org/wiki/Automorphic_form	

https://en.wikipedia.org/wiki/Hyperelliptic_curve

That looks pretty nice:
https://math.mit.edu/~jorloff/18.04/notes/

"Cauchy’s theorem is analogous to Green’s theorem for curl free vector fields"
Here, in 3.6:
https://math.mit.edu/~jorloff/18.04/notes/topic3.pdf


-What about double-integrals like:
 I = \int_R u(x,y) \, dA, I = \int_R v(x,y) \, dA, I = \int_R u(x,y) + v(x,y) \, dA
 I = \int_R u(x,y) - v(x,y) \, dA
 where dA = dx dy  or dA = r d\phi dr? Maybe the last is zero if the region R has circular symmetry because v is the same as u but rotated? I think, in the case of 
 f(z)=z^2, u and v are related by a roation of 45°...maybe for z^n, it's a rotation of
 90°/n and an analytic function can be viewed as a weighted sum of such z^n terms?
 
-What about constructing a potential for the Polya vector field? Such a potential 
 should exist, right? We could call it "Polya-Potential". It would encode the full 4D
 information of the function w = f(z) in just 3D and thereby make it visualizable. We
 would have to construct a function p = p(x,y) such that p_x = u, p_y = v (or -v). But
 it wouldn't be unique, right? We could add any constant. Scalar potentials exist for
 vector fields whose rotattion is zero.
 
-Can we say something interesting about double-integrals of such Polya-Potentials?

-Does a vector potential also exist? I think so - the condition is that the divergence
 must be zero. Maybe the vector potential is just the complex antiderivative or at
 least simply related ot it? But how is a vector potential of a 2D vector field even defined?
 It involves the curl. Maybe we could embedd 2D in 3D. Or maybe we could use the exterior 
 derivative in 2D? That would actually give a pseudoscalar which is also a 1D object. Maybe in
 2D, scalar potentials and vector potentials are the same thing?
 
- 
-This snippet may go into the introductory text for the "Multivariable Calculus" chapter:
-Why consider any specific form of nD calculus (like vector or complex calculus) when 
 it can all be embedded in some sort of general "flat" multidimensional calculus R^m -> R^n anyway? For sure, C -> C mappings could all be expressed as R^2 -> R^2. All that going from R^2 to C does is impose restrictions for what is allowed to do. Similar with resricting our attention to specific differential operators such as div and curl when we could just as well allow calculations with *any* combination of partial derivatives. But it is precisely the structure that these restrictions impose, which is of interest. Saying that it's all encompassed by going to a fully unrestricted R^m -> R^n calculus. i.e. general "flat" multivariable calculus is like saying that matrix algebra can be embedded into some sort of algebra between flat 1D arrays. Yes sure, any nD array can be looked at in flattened form and it conatins the same information as the "structured" array - but we are interested in the consequences of imposing that structure. Freeing ourselves to "do anything" prevents us from discovering interesting substructures that arise precisely *from the constraints* that we impose on ourselves. For example, complex calculus can be seen as calculus R^2 -> R^2 with the additional constraint that the functions we cosider must obey the Cauchy-Riemann equations. Surely, just allowing *any* R^2 -> R^2 function contains these as subset. But our complex calculus theorems arise precisiely from the *constraints* that they impose. Similarly, the theorems of 3D vector calculus arise from the constraints that we impose on the functions R^3 -> R^3 that we consider (for example, being potential fields) and/or from the specific combinations of partial derivatives that we consider (grad, div, curl, Laplacian)
	
Resources:
https://en.wikipedia.org/wiki/Riemann_surface	
https://en.wikipedia.org/wiki/Hypercomplex_analysis


Using Complex Analysis to Factorize Power Series
https://www.youtube.com/watch?v=r7sjvrqbGP0
	
\end{comment}