\section{Vector Calculus} 

\subsection{Scalar and Vector Fields}
When we are in the realm of vector calculus, the mathematical idea of a \emph{field} refers to a function $f$ that takes a vector $\mathbf{x}$, usually from $\mathbb{R}^n$, as input and produces an output - as functions do. But we must be careful not confuse this notion with the notion of a field in abstract algebra. There, a field means something else. When the output of the function is a simple scalar, i.e. a (real) number, then we call our field a \emph{scalar field}. So, scalar fields are functions $f: \mathbb{R}^n \rightarrow \mathbb{R}$. If the output of the function is itself a vector and that vector is of the same dimensionality as the input vector, we call the field a \emph{vector field}. So, vector fields are functions $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^n$. Typographically, we denote functions with vector outputs in boldface. In classic vector calculus, the dimensionality $n$ is usually 3, sometimes 2. Therefore, for convenience, we may write scalar fields as $f = f(x,y,z)$ or $f = f(x,y)$ and vector fields as $\mathbf{f}(x,y,z) = (f_1(x,y,z), f_2(x,y,z), f_3(x,y,z))^T$ or $\mathbf{f}(x,y) = (f_1(x,y), f_2(x,y))^T$. One could perhaps use $f_x, f_y, f_z$ instead of $f_1, f_2, f_3$. However, that notation is already commonly used to denote partial derivatives in the realm of partial differential equations (PDEs) which we will encounter later, so we won't use that notation here because we indeed want to use it there later. 

%One also sometimes sees the notation $u(x), v(x), w(x)$ ...really? In differential geometry, one often sees x(u,v), y(u,v), z(u,v) ...but the other way around? ...not sure

%\begin{eqnarray}
% f(x,y,z) &= 
%\end{eqnarray}

%-maybe plot an example vector field


\subsection{Differential Operators}
Operators are mathematical objects that can "act on" or be "applied to" functions to produce new functions. Differential operators are operators which are built from (partial) derivatives. In vector calculus, we will encounter operators that act on scalar- and vector fields and produce other scalar- and vector fields. In this context, one could consider taking the Jacobian and Hessian as operators that act on vector and scalar fields respectively and produce matrix fields as outputs. However, Jacobians and Hessians themselves in their original form are not commonly encountered in the realm of 3D vector calculus, as far as I know. They belong more into the realm of general multivariable calculus and/or maybe into matrix- or tensor calculus. Sometimes, the Jacobian appears in disguise as a gradient of a vector field which is the transpose of the Jacobian and we also encounter the trace of the Hessian in the form of the Laplacian operator.

%[TODO: figure out!]

%-mention how the Jacobian and Hessian fit into this scheme. I think, we may consider them as operators that produce matrix-valued functions, i.e. matrix-fields or tensor fields
%-maybe they occur in disguise as the vector gradient (Jacobian) and Laplacian (trace of Hessian)?

\subsubsection{Gradient}
We already encountered the general $n$-dimensional \emph{gradient} operator that acts on a scalar field $f(\mathbf{x})$ and produces a vector field $\mathbf{grad} f(\mathbf{x})$. The gradient of scalar fields $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ is indeed one of the common differential operators of classic 3D vector calculus. For such a scalar field $f = f(x,y,z)$, the gradient is defined as:
\begin{equation}
 \grad f = \nabla f  \quad = \quad
 \begin{pmatrix}
  \partial f / \partial x \\
  \partial f / \partial y \\
  \partial f / \partial z
 \end{pmatrix}
 =
 \begin{pmatrix}
  \partial_x f \\
  \partial_y f \\
  \partial_z f
\end{pmatrix}
= 
 \begin{pmatrix}
   f_x \\
   f_y \\
   f_z
\end{pmatrix} 
\end{equation}
We introduced two possible notations on the left hand side at once: one using $\grad$ and one using the nabla-operator $\nabla$. We also introduced three possible notations on the right hand side: one using the "partial" symbol $\partial$ in a differential quotient, one using a subscripted $\partial$ symbol and another one using subscripts on $f$. They all mean the same thing and you will find all of them and possibly more in the literature. We will primarily use simplemost one which is the rightmost. In some books, you will also see definitions with the function's arguments written out, i.e. $f$ is replaced by $f(x,y,z)$ everywhere. That would be quite messy so we have already suppressed the arguments here and it is implicitly understood from the context that $f$ is a function of $x,y,z$. Of course, it could also be a 2D function of $x,y$ or an $n$D function of $x_1, \ldots, x_n$. In the former case, the $f_z$ component would be missing and in the latter, you would have a $n$D vector $(\partial f / \partial x_1, \ldots, \partial f / \partial x_n)^T$. The gradient of a scalar field $f$ is always just the vector of all the partial derivatives of $f$. As said before, it gives the direction of steepest ascent in the input space and its length encodes, how steep that steepest ascent is. Don't make the mistake of thinking about it as a direction in the combined input/output space.

\medskip
The gradient is a generalization of the derivative for scalar fields. What about vector fields? We have already seen the Jacobian matrix as an appropriate generalization of the derivative for the $n$ inputs, $m$ outputs case. Now, the gradient vector as defined above is a column vector. The Jacobian matrix has the gradients of the individual component functions as its rows. What do we make of this? Wikipedia offers a definition of a gradient for a vector field in terms of a tensor expression which is equivalent to the transpose of the Jacobian [REF needed]. ...TBC...

%see \hyperlink{www.en.wikipedia.org/wiki/Gradient}{here}.
%\hyperlink{here}{https://en.wikipedia.org/wiki/Gradient}.

 %\hyperlink{https://en.wikipedia.org/wiki/Gradient#Gradient_of_a_vector_field}{here}.
% https://en.wikipedia.org/wiki/Gradient#Gradient_of_a_vector_field

% mismatch between matrix and vector algebra? the dot product of two row vectors would not be defined in matrix algebra - only when the first vecor is transposed, it will work. We informally view vectors as n x 1 matrices, but maybe that's formally wrong?

%graient is row-vector

%-maybe draw a picture



%There is also a notion of a gradient of a vector field. This operator is much less common...TBC...

%-can it also be applied to vector fields? Does it then, in fact, produce the Jacobian?
%-https://en.wikipedia.org/wiki/Gradient#Gradient_of_a_vector_field
%"...this expression equals the transpose of the Jacobian matrix...
% I think, to make sense of this, we should interpret the vector field f = (f_1,f_2,f_3) as a row vector rather than a column vector, then replace each f_i by the column vector of the partial derivatives of that f_i. Each column of the resulting matrix would be the gradient of the scalar function f_i. That's indeed the *transpose* of the Jacoobian

%https://math.stackexchange.com/questions/156880/what-does-it-mean-to-take-the-gradient-of-a-vector-field"

% https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb



\subsubsection{Divergence}
The \emph{divergence} operator acts on a vector field and produces a scalar field. Given a vector field $\mathbf{f}(x,y,z) = (f_1(x,y,z), f_2(x,y,z), f_3(x,y,z))^T$, the divergence of the vector field is defined as:
\begin{equation}
 \dive \mathbf{f} = \nabla \cdot \mathbf{f}  \quad = \quad
 \frac{\partial f_1}{\partial x} + 
 \frac{\partial f_2}{\partial y} + 
 \frac{\partial f_3}{\partial z}
\end{equation}
We have also introduced two notations on the left hand side. The notation using the "nabla-dot" operator $\nabla \cdot$ conveys the idea that the divergence can be formed by taking a formal dot product between the nabla operator $\nabla = (\partial / \partial x, \partial / \partial y, \partial / \partial z)^T$ and the vector field $\mathbf{f} = (f_1, f_2, f_3)^T$ itself. If you expand this dot product formally, you'll get precisely the sum of partial derivatives above. The divergence is a measure of how much the vector field tends to point out of an infinitesimally small region. If you envision the vector field as a velocity field of a fluid, divergence can the thought of as a sort of source strength. It measures how much fluid is "produced" in a region and "flows outward" into its surrounding environment. For this reason, a region where the divergence is positive is called a \emph{source} and a region with negative divergence is called a \emph{sink}. A field in which the divergence is zero everywhere is called \emph{solenoidal} [TODO: explain why]. It is obvious how this operator generalizes to $n$D space: you would just take the sum over $n$ terms where the $k$th term is the partial derivative of the $k$th component function with respect to the $k$th variable.

%ToDo: explain sources and sinks, divergence free fields - solenoidal.
% https://en.wikipedia.org/wiki/Solenoidal_vector_field

\subsubsection{Curl}
The \emph{curl} operator acts on a vector field and produces another vector field. The curl of a vector field $\mathbf{f} = (f_1, f_2, f_3)^T$ is defined as:
\begin{equation}
 \curl \mathbf{f} = \nabla \times \mathbf{f}  \quad = \quad
 \begin{pmatrix}
	\partial f_3 / \partial y \; - \; \partial f_2 / \partial z \\
	\partial f_1 / \partial z \; - \; \partial f_3 / \partial x \\
	\partial f_2 / \partial x \; - \; \partial f_1 / \partial y
\end{pmatrix} 
\end{equation}
Again, the "nabla-cross" notation  $\nabla \times$ can be explained by taking a formal cross product between the nabla operator and the the vector field. The curl measures how much a vector field tends to swirl around locally. It is a vector that points into the direction of the axis around which the field swirls around the most [VERIFY] and its length indicates, how much that is. The curl is also sometimes called the \emph{rotation} and denoted by a $\mathbf{rot}$ operator. Due to the fact that the definition of curl is based on the cross product which exists only in 3D, curl also only exists in 3D. 

ToDo: explain vorticity, give definition in terms of limit of circulation with geometric intuition

% Vector Calculus: Understanding Curl
% https://www.youtube.com/watch?v=A2Sn_xAKMwQ


% Divergence and curl: The language of Maxwell's equations, fluid flow, and more
% https://www.youtube.com/watch?v=rB83DpBJQsE


\medskip
For 2D vector fields $\mathbf{f}(x,y) = (f_1(x,y), f_2(x,y))^T$, we can use a sort of hack: First, we embed the 2D input space in 3D space by just setting $f_3 = 0$. Our functions $f_1, f_2$ are just functions of $x$ and $y$.  Because they do not depend on $z$, all partial derivatives with respect to $z$ will be zero as well. Now we notice that only the last component of our curl vector will be nonzero. That means, if the vector field is (locally) swirling at all, it will do so around an axis parallel to the $z$-axis. This makes a lot of sense - we'd probably think about a 2D curl more as swirling around a point on the plane rather than an axis sticking out of the plane - but if we really force ourselves to think about an axis, then it would have to be perpendicular to the plane. We could now define a sort of pseudo curl as the simple scalar field $\partial f_2 / \partial x - \partial f_1 / \partial y$. Sometimes, such a 2D pseudo curl is useful but it's not really a satisfying solution. A meaningful and satisfying generalization to $n$D including 2D can only be given later in the context of exterior calculus where the cross product is thrown away in favor of the similar but different wedge product. 

\subsubsection{Laplacian}
The \emph{Laplacian} operator acts on a scalar field and produces another scalar field. It is actually the divergence of the gradient and is therefore a second order differential operator in the sense that it involves second order partial derivatives. Given a 3D scalar field $f = f(x,y,z)$, the Laplacian of the field is defined as:
\begin{equation}
 \Delta f = \nabla \cdot \nabla f = \nabla^2 f \quad = \quad
 \frac{\partial^2 f}{\partial x^2} + 
 \frac{\partial^2 f}{\partial y^2} + 
 \frac{\partial^2 f}{\partial z^2}   
\end{equation}
On the left hand side, we again have a couple of alternative notations which all mean the same thing. It is always just the sum of the non-mixed second partial derivatives of $f$ with respect to all the coordinates and generalizes to $n$D in the obvious way. In terms of the Hessian matrix, we recognize the Laplacian as the trace of the Hessian. %When the Hessian is viewed as tensor, taking the trace also corresponds to a contraction. Don't worry about that right now - we'll come back to that later in the context of tensor calculus.

%-It's the trace of the Hessian, I think. When the Hessian is viewed as tensor, that also corresponds to a contraction.
%-I think, it can also be applied to vector fields?
% https://en.wikipedia.org/wiki/Laplace_operator
% https://de.wikipedia.org/wiki/Laplace-Operator

\subsubsection{Identities}
There are a lot of identities involving those differential operators. Having them available can be very helpful in simplifying vector calculus equations. First of all, we note that all these operators are linear:

...TBC...

ToDo: give a comprehensive list of identities involving the differential operators above.

%-all operators are linear
%-gradient-laws: 0 if field is const, product rule

% introduce nabla dot nabla and nabla-squared notation



\subsubsection{Curvilinear Coordinates}
%With these integral theorems, we now have a lot of formulas available to potentially simplify certain integrals. But all of them can also be used together with the coordinate transformation formula for multiple integrals that we have seen earlier. To this end, 

It is useful to have formulas available that tell us, how our differential operators look like in other coordinate systems like cylindrical, spherical, etc.
...TBC...

% give formulas for how gradient, divergence, etc. look in cylindrical, spherical and general coordinate systems
% maybe move into section about differential operators




\subsection{Potential Fields}
We have seen how we are able to produce new fields by applying differential operators to given fields. Fields that are so produced are special and have some convenient properties. If we are able to express a vector field as gradient of a scalar field, then the scalar field contains all the information that the original vector field contains. We can always get our original vector field back by just taking its gradient. We lost no information - yet, a scalar field is a much simpler object than a vector field. Of course, it's not always possible to find such a scalar field - but if it is, we should take advantage of it. If such a scalar field, let's call it $F$, exists for a given vector field $\mathbf{f}$, then we call $F$ a \emph{potential} for $\mathbf{f}$. A potential is a sort of antiderivative of a vector field and sometimes even called by that name. You need to be careful about different sign conventions used in the literature: some authors would define $-F$ to be the potential of $\mathbf{f}$. Typical potentials that occur in physics are the gravitational and the electrostatic potentials. These are scalar fields whose gradients give the gravitational and electric field respectively. Usually when we talk about a potential without any qualifier, we mean a scalar field. However, there are also situations in which a vector field can be obtained by taking the curl of another vector field. In this case, this other vector field is called the \emph{vector potential} of our given vector field. If we want to explicitly emphasize that we are not talking about a vector potential, we may qualify a potential as a \emph{scalar potential}. It may seem strange to consider vector potentials because at face value they don't seem to simplify anything. They just trade a vector field for another vector field. However, they can indeed be convenient [ToDo: give an example, how]. In physics, the magnetic field does indeed possess such a vector potential which is in some sense more fundamental than the magnetic field itself....TBC...

% explain how in electromagnetism, the vector potential is used for the magnetic field

% https://en.wikipedia.org/wiki/Scalar_potential
% both conventions are used - B채rwolff uses the one without the minus - mention that one finds both notations in the literature

%A vector field that can be produced by taking the gradient of a given scalar field is called a potential field

%-scalar and vector potentials ...but we need the curl be defined to define vector potentials
%maybe re-organize:  make this subsection "Scalar and vector Fields" and introduce potential fields after the "Differential Operators" or maybe even after the integrals...nah...maybe not

%-what, if the divergence is zero? Have these fields also some special name?

\subsubsection{Finding Scalar Potentials}
So, if we encounter a vector field $\mathbf{f}$ in some application, how do find its scalar potential? First things first - we should first figure out whether or not such a scalar potential even exists. Fortunately, there's a simple criterion for that: For a given vector field $\mathbf{f}$, a scalar potential $F$ exists, if and only if the curl of the vector field is identically zero. Such vector fields are also called \emph{irrotational}. Due to the fact that curl is only defined in 3D, this definition is only applicable in 3D. In the more general case, the requirement is that the Jacobian matrix of the vector field is symmetric. This fact is so important that it is actually called \emph{second fundamental theorem for potential fields} (we'll see the first one later). %(B채rwolff, pg 560 - aka integrability criterion?)
Let's assume we have checked the criterion and now we actually want to find the potential....TBC...

\paragraph{Example} Consider the 2D vector field $\mathbf{f}(x,y) = (x^2-y^2, -2 x y)$. ...tbc...

% maybe use as example a 3D vector field coming from the potential 1/r^2 where r^2 = x^2+y^2+z^2 and try to reconstruct the potential

%-explain non-uniqueness - we are allowed to add any constant
% what about the Laplacian? does it play nor role here?

% Potential berechnen - Vektorfeld, Beispielrechnung (Physik)
% https://www.youtube.com/watch?v=Wl1yXMicg3c

\subsubsection{Finding Vector Potentials}
If we encounter a vector field $\mathbf{f}$ and we want to know whether or not there exists a vector potential for $\mathbf{f}$, i.e. a vector field $\mathbf{F}$ whose curl $\mathbf{f}$ is: $\curl \mathbf{F} = \nabla \times \mathbf{F} = \mathbf{f}$, a necessary and sufficient condition for such a vector potential $\mathbf{F}$ to exist is that the divergence of $\mathbf{f}$ must be identically zero: $\dive \mathbf{f} = \nabla \cdot \mathbf{f} = 0$. After verifying that this is the case, we can start to work out the vector potential $\mathbf{F}$ of the vector field $\mathbf{f}$ as follows: ...TBC...



%- they are not unique . we can always add the gradient of an arbitrary scalar field - see Feynman Lectures, Vol. 2, Ch. 14

%-mention how this translates to higher dimensions - the divergence based existence criterion carries over as is (right?) - but curl is undefined in nD - we need to do it in the context of exterior calculus

% What about the idea of trying to find a vector field when a Jacobian is given? Maybe it can be traced back to finding scalar potentials for the rows (or columns) of the Jacobian?

\subsection{Harmonic Functions}
A so called \emph{harmonic function} is a scalar field $f$ whose Laplacian is identically zero. That means, it satisfies the so called Laplace equation $\Delta f = 0$ for all possible inputs. The Laplace equation is a partial differential equation that features prominently in physics. Harmonic functions of 2 variables are closely related to the field of complex analysis which we will encounter later ...TBC...

%-harmonic conjugates
%-relation to potential fields? are gradient fields harmonic? i think, it could follow from Lap(f) = div(grad(f)) = 0 ...but wait, no - 
%-https://en.wikipedia.org/wiki/Skew_gradient
% https://en.wikipedia.org/wiki/Harmonic_function
% https://en.wikipedia.org/wiki/Harmonic_conjugate
% https://mathworld.wolfram.com/HarmonicConjugateFunction.html
% https://en.wikipedia.org/wiki/Harmonic_function

% Mapping the Universe with Spherical Harmonics
% https://www.youtube.com/watch?v=Ov8VFq1qLkI
% -Under the application of the Laplace operator, harmonic functions just pick up a negative
%  constant, i.e. if f is harmonic then Lap(f) = -c * f   with c > 0


% The Hidden MAGIC Behind HARMONIC Functions
% https://www.youtube.com/watch?v=yTVCBmml8PE


\subsection{Line Integrals}
Line integrals are integrals in which some field quantity is integrated over some line or curve in our vector space $\mathbb{R}^n$. They are also known as contour integrals, curve integrals or path integrals. The curve is given in parametric form:
...TBC...

\subsubsection{Area of a "Curtain"}
% https://www.khanacademy.org/math/multivariable-calculus/integrating-multivariable-functions/line-integrals-for-scalar-functions-articles/a/line-integrals-in-a-scalar-field

%https://en.wikipedia.org/wiki/Line_integral#Line_integral_of_a_scalar_field

% what about line integrals of scalar fields - see Baerwolff, pg 550. Ah - I think, this is the area under the curtain

\subsubsection{Work in a Force Field}

%explain the meaning of the \oint notation

\subsubsection{Flux through a Curve}
%is specific to 2D, i think


\subsubsection{Circulation}


% redefine curl in terms of circulation - it's the limit when the area shrinks to zero

% Vector Calculus: Understanding Curl
%  https://www.youtube.com/watch?v=A2Sn_xAKMwQ

\subsection{Surface Integrals}
\subsubsection{Area of a Surface}
\subsubsection{Flux through a Surface}

\subsection{Integral Theorems}

\subsubsection{Fundamental Theorem of Line Integrals}
% I think, it's wrong to call int F' - there should be a transposition involved - see B채rwolff, pg 393.
Let $F(\mathbf{x})$ be a scalar-valued function of some vector input $\mathbf{x}$ in $\mathbb{R}^n$ and $\mathbf{f(x)} = \grad F(\mathbf{x})$ be the gradient of $F$. We observe that $\mathbf{f}$ is a vector-valued function of $\mathbf{x}$ and that according to our definition, $F$ is a potential for $\mathbf{f}$. Let $C$ be a contour with start and end points $\mathbf{a,b} \in \mathbb{R}^n$ and let $C$ be given as a parametric curve $\mathbf{c} = \mathbf{c}(t)$ with parameter $t$. Let $d\mathbf{s} = \dot{\mathbf{c}} = d\mathbf{c} / dt$ be an infinitesimal tangent vector along $C$ at the position $\mathbf{c}(t)$ and $\mathbf{c}(t_a) = \mathbf{a}, \mathbf{c}(t_b) = \mathbf{b}$. Then, the following holds:
\begin{equation}
 \int_C \mathbf{f} \cdot d\mathbf{s}  =
 \int_{t_a}^{t_b}  \mathbf{f} (\mathbf{c}(t) ) \cdot \dot{\mathbf{c}}(t) \; dt =
% = \int_{\mathbf{a}}^{\mathbf{b}} \mathbf{f} \cdot d\mathbf{s} 
 F(\mathbf{b}) - F(\mathbf{a}) 
\end{equation}
This is the \emph{fundamental theorem of line integrals} and it is also known as the \emph{first fundamental theorem of potential fields} or as the \emph{gradient theorem}. Let's unpack what this formula means: In the integrand of the left hand side, we are forming a dot product (aka scalar product) between the gradient field $\mathbf{f}$, which is a vector field, and an infinitesimal tangent vector $d\mathbf{s}$ to our parametric contour/curve $C$. When the theorem is stated in this form, it is implicitly understood that we are evaluating $\mathbf{f}$ and $d\mathbf{s}$ at some given value for the parameter $t$. In the middle, we have just expanded in more detail, what the the left hand side actually means. This infinitesimal dot product of two vectors is then integrated, i.e. summed, over the whole length of the curve. Via the dot product, at each point along the curve, we "measure" how much the vector field points into the local (tangential) direction of our curve and that "measurement" gives a little contribution to our integral. If the vector field is locally aligned with the curve, we get the maximum contribution. If the vector field is locally perpendicular to our curve, we get no contribution at all. The right hand side is easy to understand: we just evaluate the scalar field $F$ at the two endpoints of the curve and subtract the two resulting scalar values. What this implies is that the value for the integral depends only on the endpoints $\mathbf{a,b}$ and not at all on the specific curve, i.e. on the way how we move from $\mathbf{a}$ to $\mathbf{b}$. This path-independence is a special convenient feature of potential fields. You cannot expect that from any general vector field. Not all vector fields can be expressed as a gradient of some scalar field. But for those that can, we can apply this theorem and thereby greatly simplify the evaluation of line integrals along paths through this field, provided that we know the potential $F$. An immediate consequence is that the line integral around any closed loop, i.e. a curve whose end point is equal to the start point, must be zero. Just look at the right hand side when $\mathbf{b = a}$. This can be written as:
\begin{equation}
\oint_C \mathbf{f} \cdot d\mathbf{s} = 0
\end{equation}
Not only does this follow from path independence but the converse is also true. The following statements are equivalent: (1) $\mathbf{f}$ is a potential field, (2) line integrals are path independent, (3) integrals around closed loops are zero. In physics, when we have a force field and a particle moves around in this field, then the work done by the field on the particle can be computed by such a line integral. If the force field happens to be a potential field, which is usually the case, then the work done will be zero for any trajectory that is a closed loop. Such a closed trajectory is also often calle an \emph{orbit}. Work done being zero means that no energy was tranferred to the particle while it moved through its orbit, i.e. its energy was conserved. That's why potential fields are also called \emph{conservative} fields - there's some quantity that is conserved when taking line integrals around closed loops. 

...TBC...ToDo: explain potential energy and the back-and-forth between kinetic and potential during orbit traversal in elliptic orbits

%ToDo: explain, how the contour is given as parametric equations and formulate the theorem in terms of the parametric decription as an integral ove the parameter $t$, explain how this generalizes the fundamental theorem of calculus


%-applies to potential fields
%-aka first fundamental theorem for potential fields (see B채rwolff, pg 557)
%-could also be called "Gradient Theorem" - would fit well with "Divergence-" and "Curl Theorem"
% https://en.wikipedia.org/wiki/Gradient_theorem
% https://www.khanacademy.org/math/multivariable-calculus/integrating-multivariable-functions/line-integrals-in-vector-fields-articles/a/fundamental-theorem-of-line-integrals
% https://www.contemporarycalculus.com/dh/Calculus_all/Ch15_4.pdf
% https://math.mit.edu/~jorloff/18.04/notes/greenstheorem.pdf
% https://tutorial.math.lamar.edu/classes/calciii/fundthmlineintegrals.aspx

\subsubsection{Green's Theorem}

% 
% https://en.wikipedia.org/wiki/Shoelace_formula
% Is a special case of Green's Theorem according to 
% https://en.wikipedia.org/wiki/Green%27s_theorem#References

\subsubsection{Gauss's Divergence Theorem}

\subsubsection{Stokes's Curl Theorem}
Stokes's theorem is a 3D generalization of Green's Theorem. It relates a surface integral of a vector field over an arbitrary surface in 3D space to a line integral over the boundary of that surface. Perhaps it's more appropriate to say that Green's theorem is just a special case of Stokes' theorem in which the surface under consideration is just a region in the flat $xy$-plane. ...TBC...

%\subsubsection{Green's Formulas}






\begin{comment}

-make a section for how to compute potentials for a given evctor field
-before that, have a section for how to test, if a potential even exists 

-i'm not sure, if path integrals and surface integrals should be treated here or in the "General Concepts" section ...maybe here - there, we only do integrals of scalar fields


https://en.wikipedia.org/wiki/Matrix_calculus

explain this:
https://en.wikipedia.org/wiki/Helmholtz_decomposition


Mapping the Universe with Spherical Harmonics
https://www.youtube.com/watch?v=Ov8VFq1qLkI


Spherical Harmonics and the Multipole Expansion
https://www.youtube.com/watch?v=g4qa1KCyGfc
-The shperical harmonics are the eigenfunctions of the angular part of Laplace the operator in 
 spherical coordinates
-I think, his analogous to sines and cosines being eigenfunctions of the 2nd derivative operator.
-Maybe such spherical harmonics can be viewed through the lens of nD Fourier analysis?


https://en.wikipedia.org/wiki/Laplace_operator
https://de.wikipedia.org/wiki/Laplace-Operator

https://math.stackexchange.com/questions/1675768/eigenfunction-and-eigenvalues-of-laplacian

https://arxiv.org/pdf/1206.1278

https://en.wikipedia.org/wiki/Spherical_harmonics
https://en.wikipedia.org/wiki/Cylindrical_harmonics


\end{comment}

