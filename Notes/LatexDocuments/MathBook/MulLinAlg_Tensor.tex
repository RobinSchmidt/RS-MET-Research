\section{Tensor Algebra}
Tensors are a generalization of scalars, vectors and matrices and are used to represent physical or geometrical quantities that are invariant with respect to the choice of a coordinate system. This \emph{invariance} or \emph{coordinate independence} seems to be a natural fit for situations where the choice of a coordinate system is arbitrary which is often the case in physics. Once a coordinate system has been chosen, these tensor quantities can be \emph{represented} by multidimensional arrays. A scalar is just a number but can be seen as 0D array in this context. A vector is represented by a 1D array of numbers, usually written as column. A linear transformation of a vector is represented by a 2D array of numbers, i.e. a matrix. Now, we can see how this pattern should continue: we may have quantities that are representable by arrays with more than 2 dimensions - imagine slicing a bunch of matrices on top of each other for a 3D array, giving some sort of 3D block with small cubicles containing the numbers. A 4D array is harder to visualize, but we may imagine it as a list (i.e. a 1D array) of such blocks where the first index just selects, which of the blocks within the list we mean. And then we can imagine a list of such lists for a 5D array - or maybe a matrix of 3D blocks, etc. The dimensionality of the array is called the \emph{rank} of the tensor. Be careful to not confuse this with dimensionality of the underlying vector space. Be also careful to not confuse this notion of "rank" with the rank of a matrix known from linear algebra. This is just the same word for an entirely different concept. In computer science contexts, one often uses the term "tensor" to mean any sort of multidimensional array without imposing any of these "invariance" requirements. For example, in this looser sense of the term, a video can thought of as being stored in a rank-4 tensor: The first index is the frame index, the second and third are row- and column index in the still picture defining the current frame and the fourth index is the color channel. This looser sense of the usage of the word tensor is, however, typically not what mathematicians or physcists mean by that word. In these fields, a tensor is really defined by its particular invariance and transformation properties. So - watch out for the context in which the term is used. Here, we will mostly be concerned with the mathematician's view of what a tensor is.


\subsection{Notation and Terminology}

\subsubsection{Covariance, Contravariance and Invariance}
Our eventual goal is to develop an algebra that is suitable to represent quantities whose real-world meaning is independent from the arbitrariness of our choice of a coordinate system. However, in actual calculations, we have to make such a choice, so we can assign numbers to things. For example, a velocity can be represented by its velocity vector $\mathbf{v}$. The direction of movement in the real world does not depend on our chosen coordinate system but the components of the vector, i.e. the numbers in our 1D array, will. We could represent the \emph{same} vector in a different coordinate system as well. Then we would have different numbers, but the vector itself as an abstract entity would still be the same. Imagine a 2D vector and assume that we would obtain a new coordinate system by rotating both basis vectors of our old coordinate system by 30 degrees counterclockwise. The same vector in this new coordinate system would have components that appear rotated 30 degrees clockwise with respect to the vector's components in the old coordinate system. The clockwise rotation of the vector components compensates for the counterclockwise rotation of our basis vectors. We say that the vector is \emph{contravariant}: in the event of a change of the coordinate basis vectors, the components of the vector change in a way that is contrary to the change in the basis vectors. There can also be vector-like quantities that change in the same way as our basis vectors (TODO: give an example). Such quantities are called \emph{covariant}. Finally, scalars represent quantities whose numerical value never changes at all. Their values are not dependent on our choice of coordinate system. Therefore, scalars are also called \emph{invariant} quantities in the context of tensor algebra (verify!)...are there also other invariant quantities or are scalars the only ones? ..and what about mixed variance in higher rank tensors?

% contravariant vectors: position, velocity, acceleration, force, electric and gravitational field
% covariant vectors (verify): normals to planes, angular momentum moment of inertia, magnetic field...i think, these correspond to covectors? ...or maybe bivectors? Looks like these are what physicists call pseudovectors? Figure out!

..tbc...

% Start with a motivating example: 
% -How do vectors and matrices transform under a change of the coordinate system - I think, for
%  matrices, this is the similarity transformation?
% -Write the transformation rules down in the familiar matrix notation, then from there, write them
%  down in component notation. This may be the starting point for recognizing the general pattern
%  for how general tensors transform

\subsubsection{Rank, Order and Dimensionality}
% What is order? same as rank? explain that the tensor rank has nothing to do with a matrix rank


\subsubsection{Covectors and Duality}
% -Given a vector space V, the dual space is defined to be the space of linear maps from V into the real numbers and denoted as V^*. For a finite-dimensional V, which is the case we have here, V^* is isomorphic to V. 

% Bilinear Maps
% -every bilinear map can be expressed as sum of a symmetric and alternating bilinear map

\subsubsection{Einstein Summation Notation}
% 3 rules:
% -repeated indices are summation indices akk dummy indices
% -we use upper indices for vector components and lower indices for basis vectors
% -for covectors (dual vectors), it's the other way around: lower indices for components, 
%  upper indices for the basis covectors
%
% -For a tensor equation to make sense, the repeated index must occur once as lower and once as
%  upper index (can it occur also twice as upper and lower index?)

% explain also objets that look like tensors but aren't because they don't obey any of the 
% transformation laws


% the vector and covector bases satsify the Kronecker delta relation:
%  b_i(c^j) = c^j(b_i) = \delta_i^j where c^j(b_i) means "apply the j-th basis covector to the
%  i-th basis vector", etc - "apply" in the sense of: evaluate the expression to a scalar
% or maybe use e_i for the basis vectors and d^j for the basis covectors (d for dual). although
% b for basis and c for co-basis is quite nice, too. But maybe we should use c for a general
% covector just as we use v for a general vector. We have c^i(v_i) = v_i(c^i) = \sum_i v[i]*c[i]

\begin{comment}
====================================================================================================
-A tensor can be seen as a function that takes a number of vectors and covectors  as inputs and 
 produces a scalar as output. This mapping is linear in all its inputs, i.e. it is a multilinear
 map.
-It can also be seen as a multidimensional array, or rather, a geometric of physical quantity that
 can be represented by such an array, once a basis has been chosen. The tensor itself is invariant
 ...or is this the right word? i think tensors can be covariant, contravariant or a mix of both. 
 Truly invariant are the scalars that result when fully evaluating a tensor. Regardless of the 
 chosen coordinate system, they will always have the same numerical value
-When seen as a multidimensional array, it must obey certain transformation rules, when the basis
 is changed.
-how can we interpret a matrix vector-product in this context? maybe as partial evaluation? 
 multiplying a matrix from the left by a vector gives another vector. this is a tensor that 
 requires another covector as input to produce a scalar
-what about inverse tensors and tensor division? i think, with repect to the tensor product, this
 would not make much sense. only when a tensor indeed resulted from tensor-multiplying two lower
 rank tensors, we may be able to calculate one of the factors, given the other factor and the 
 product
-Examples for tensor quantities are: scalars, vectors, linear maps, quadratic (bilinear) forms(?)
-Maybe start with re-explaining what a vector is from the perspective of tensor algebra. there are
 at least 3 ways to define a vector: (1) numeric: list of numbers, (2) geometric: arrow in space,
 (3) abstract: element of a vector space. The first one is actually subject to the interpretation
 v = v1*b1 + v2*b2 + ... + vn*bn where the vi are the vector componentes and the bi are the basis
 vectors. If we say nothing about the basis vectors, it is usually implicitly understood that the
 standard basis should be used.

Let's try to express the some known products in terms of tensors and index notation:

scalar product: c = a^i b_i = b_i a^i  (product of vector a and covector b)
matrix-vector product: c_i = a^k_i b_k  (verify)
matrix-matrix product: ...
vector-matrix product:  ...
outer product of vectors: c_{ij} = a_i b_j  ...this is actually the tensor product

Kronecker product of two matrices: c_{ij kl} = a_{ij} b_{kl}
...but wait - no - this is a rank 4 tensor - the kronecker product is again a matrix - it has the
same elements but arranged ad 2D array rather than 4D



Questions:
-What about the various exterior products that can be computed from 1-vectors, 2-vectors, 
 1-covectors, 2-covectors, etc? How can they be expressed in index notation?
-What about the geometric products of various quantities in geometric algebra?

see:
https://www.youtube.com/watch?v=TvxmkZmBa-k&list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG&index=2


https://www.youtube.com/watch?v=YxXyN2ifK8A  Visualization of tensors - part 1

-really intuitive explanation of rank 2 tensors in terms of a stress tensor

-Maybe use the convention to sort the indices alphabetically. Like T^{j}_{ik} would mean that i is
 the 1st index, j the 2nd and k the 3rd. With that convention, we don't need to bother with
 poisitioning the indices, i.e. we don't need to use space between i and k which might be difficult
 to typeset and ugly.
-Maybe it doesn't really matter in which order we assume the indices to be? Maybe that's an 
 "implementation detail" that is irrelevant to the underlying math?
 
Confused by Tensors? You WON'T be after this! | Episode 1, Tensors in Physics: 
https://www.youtube.com/watch?v=3I-_ePWm9NA
-Very nice introduction. Starts with vectors, the explains dual vectors as linear maps from
 vectors to scalars, then explains tensors as multilinear maps from some number dual vectors 
 and some number of vectors to scalars.
-Part of the upcoming playlist: 
 https://www.youtube.com/playlist?list=PLVvnOdsCze6GqXjsibdA3HfLXsaw9hFFh
-in video 3, it is clarified that the tensor can also act on a subset of its vector/covector 
 arguments - the result is then not a scalar but another tensor. I guess, this amounts to
 "partial evaluation" - like when for a bivariate function f(x,y) we fix x (or y) and thereby
 obtain a univariate function.
-I think, the left-to-right placement of the (upper and lower) indices may indicate in which
 order the tensor expects it (vector and covector) arguments - but in the end (i.e. when fully 
 evaluated), the order doesn't really matter because of the asscoiciativity of scalar 
 multiplication? So, some authors do not really care about the left-to-right placement, i.e.
 don't insert spaces such that in a tensor T^i_j it is not clear whether i is the first and j
 the second index or the other way around.
 
Applications in data science:
https://en.wikipedia.org/wiki/Tensor_decomposition
https://en.wikipedia.org/wiki/Higher-order_singular_value_decomposition
https://en.wikipedia.org/wiki/Tucker_decomposition
https://en.wikipedia.org/wiki/Tensor_rank_decomposition
 
https://www.kolda.net/publication/TensorReview.pdf 

Tensoren 101: Mathematische Grundlagen  (Playlist)
https://www.youtube.com/playlist?list=PLT7ziIckLyd60jvS64O78wLhjauT8qJ3y
-Gram matrix: given a bilinear map g(v,w), define the matrix g_ij = g(e_i, e_j) where e_i, e_j are our basis vectors
-A bilinear map is alternating, iff its Gram matrix is antisymmetric
-A bilinear map whose Gram matrix has det zero is called degenerate

MathIntuitionBook (by Vadim Zavalishin:)
-pg 107:
 -superscript indices 
  -transform contravariantly
  -their number gives the rank of the resulting tensor, when the tensor is viewed as multilinear
   function (0: scalar output, 1: vector output)
 -subscript indices:
  -transform covariantly
  -their number gives the number of vector-valued inputs to the multilinear function that the
   tensor represents


I never understood why this math object is found everywhere...until now!
https://www.youtube.com/watch?v=k2FP-T6S1x0
-Has some good physical examples of tensors of various ranks

Tensor algebra  by All Angles
https://www.youtube.com/watch?v=KNQtfxbGnz4&list=PLffJUy1BnWj3uRN-lL7idX8mQeifZaqwn&index=4
https://www.youtube.com/watch?v=KNQtfxbGnz4&list=PLffJUy1BnWj3uRN-lL7idX8mQeifZaqwn&index=5


Spinors & Twistors Animated
https://www.youtube.com/watch?v=220U3eIZFLM&list=PLWEiAJhCw-zQ28C9wGdA15-f6LrEG3pNi&index=3



Different Strain Tensors: Cauchy-Green vs Green-Lagrange vs Euler-Almansi
https://www.youtube.com/watch?v=u6sVUzIHuIg



\end{comment}