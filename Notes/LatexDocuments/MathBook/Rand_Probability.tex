\section{Probability Theory}
Probability theory is the mathematical theory random events. It answers questions like "how likely is event $X$ to occur?". The answer is given in terms of a \emph{probability} which is a real number between zero and one. We can interpret this probability in the following way: We imagine an experimental situation with a random outcome such as the toss of a coin. Now we imagine to repeat the experiment $N$ times and ask about the number of times that we observed a particular outcome - say heads. Let's call that number $H$. We expect that when $N$ gets large, that the ratio $H/N$ will converge to the probability of seeing heads - in this case $\frac{1}{2}$. This interpretation of the idea of a probability is called the \emph{frequentist} interpretation. It bears this name because it interprets the idea of probability as a frequency of occurrence in the sense of how often a particular event occurs when we do a large number of experiments. There is also an alternative interpretation of the idea of a probability known as the Bayesian interpretation which interprets probability more as a subjective belief that may get updated when data is observed. The question of interpreting the meaning of the idea of a probability is a rather philosophical one and the mathematics of probability theory doesn't say anything about such an interpretation. It just gives the rules of computation.  

%...TBC...go into more detail about frequentist vs Baysian

% https://www.redjournal.org/article/S0360-3016(21)03256-9/fulltext
% https://amplitude.com/blog/frequentist-vs-bayesian-statistics-methods

% The biggest beef in statistics explained
% https://www.youtube.com/watch?v=8wVq5aGzSqY
% -Law of large numbers (Frequentist interpretation)
% -Bayes law (Bayesyan interpretaion)

%===================================================================================================
\subsection{Probability Spaces}
The setting in which probability theory takes place is formalized in terms of so called \emph{probability spaces}. A probability space is a triple $(\Omega, \mathcal{F}, P)$ made from the following 3 ingredients: 

\begin{itemize}

\item 
A set $\Omega$, called the \emph{sample space}, that represents the set of all possible \emph{outcomes} of some sort of random experiment.

\item
A set $\mathcal{F}$, called the \emph{event space}, that consists of all the events that could occur in such an experiment. Each such event is represented by a subset of $\Omega$.

\item 
A function $P: \mathcal{F} \rightarrow [0,1]$, called the \emph{probability function}, that assigns to each event $A \in \mathcal{F}$ a probability of occurrence. The probability is a real number in the closed interval $[0,1]$.

\end{itemize}

The intention of the definitions is best understood by an example. Consider the random experiment of rolling a die. The set of possible outcomes is $\Omega = \{1,2,3,4,5,6\}$. An event $A \in \mathcal{F}$ could be "the die lands on 3" represented by the singleton set $A = \{3\}$ which is a subset of $\Omega$. It could also be "the die lands on a number less than 5" represented by $B = \{1,2,3,4\}$ or "the die lands on an even number" represented by $C = \{2,4,6\}$. Assuming a fair die, the probability function $P$ would assign the following probabilities to $A,B,C$: $P(A) = \frac{1}{6}$, $P(B) = \frac{4}{6} = \frac{2}{3}$ and  $P(C) = \frac{3}{6} = \frac{1}{2}$.

\medskip
In general, there are couple of conditions that $\Omega, \mathcal{F}, P$ need to satisfy. The sample space $\Omega$ must partition the set outcomes of our experiment into mutually exclusive outcomes that, taken together, cover all possible outcomes. The term \emph{outcome} is a technical term in this context which shall not be confused with the idea of an \emph{event}. Outcomes are elements of $\Omega$. Events are subsets of $\Omega$. An event containing exactly one outcome, i.e. an event that is a singleton set, is called an \emph{elementary event} or \emph{atomic event}. The event space $\mathcal{F}$ cannot just be any random collection of subsets of $\Omega$. Instead, it must be a $\sigma$-algebra, see page \pageref{Def:SigmaAlgebra}. The probability function $P$ must be measure on $\Omega$ (in the sense of measure theory) with the additional requirements\footnote{As a reminder, a more general measure is allowed to produce values in $\mathbb{R}^+_0 \cup \infty$ and there's no requirement that measure of the whole set must be $1$} that it should produce outputs within the interval $[0,1]$ and that $P(\Omega) = 1$. The function $P$ is then called a \emph{probability measure}.

% P(A^C) = 1 - P(A)  ..or maybe use $\overline{A}$ for the complement - that's how we defined it in
% the chapter about naive set theory

%These technical details are known as the Kolmogorov axioms. VERIFY ... TBC...

% outcome = atomic event = elementary event ..or wel...no!
%  is also sometimes called \emph{atomic event} or \emph{elementary event}.
% An event containing exactly one outcome is called an elementary event.

% Probability Measures | properties, [Laplace, Dirac, Borel]-probability, discrete case
% https://www.youtube.com/watch?v=02TqmIQQAmc




%---------------------------------------------------------------------------------------------------
\subsubsection{Examples}

\paragraph{A Finite $\Omega$} ...TBC...

\paragraph{A Countably Infinite $\Omega$} ...TBC...

\paragraph{A Continuous $\Omega$} ...TBC...



% Give examples
% (1) toss a coin: omega = {0,1}, P(0) = P(1) = 1/2
% (2) roll a die: omega = {1,2,3,4,5,6}, P(n) = 1/6 for all n in 1,..,6
% (3) toss a coin until heads appears: Omega = N+, P(n) = 1/2^n
%     -> counatbly infinite sample space
% (4) pick random number in [0,1], Omega = [0,1], P(A) = \mu(A) where A is a subset of Omega
%     -> continuous sample space

%(1)  The "random experiment" in this context could be, for example, something tossing a coin or rolling a die. In the former case, we would have $\Omega = \{heads, tails\}$ and in the latter case $\Omega = \{1,2,3,4,5,6\}$. 

%(2)  For example: "the die lands on a 3" or "the die lands on an even number" or "the die lands on a number less than 5", etc. This so called \emph{event space} is a particular\footnote{To be precise, the set of subsets must be a $\sigma$-algebra (as defined in measure theory), but that's not particularly important here} set of subsets of $\Omega$. It's the set of all events that we want to assign a probability to. 

%(3) The function $P$ should satisfy the following conditions: An event that is certain should get a probability of one. Formally, that means $P(\Omega) = 1$. An event that is impossible should get a probability of zero. That translates to $P(\emptyset) = 0$.

%...TBC...explain interpretation of the probability in the frequentist and Bayesian sense. Explain how probability zero means (almost) impossible and one means (almost) certain ...the "almost" part relevant only for infinite sample spaces, 

%I think. I think $P$ should also satisfy (countable) additivity: for disjoint sets $A,B$, we should have $P(A \cup B) = P(A) + P(B)$. ...verify! 

% I think, the function $P$ must be a certain kind of measure
% explain elementary events - maybe they make sense only in the context of finite Omega?



% https://en.wikipedia.org/wiki/Probability_axioms

% https://en.wikipedia.org/wiki/Probability_measure

% https://en.wikipedia.org/wiki/Probability_space

% https://en.wikipedia.org/wiki/Outcome_(probability)
% Explain that term

% https://www.its.caltech.edu/~mshum/stats/lect1.pdf
% Good stuff in there!

% https://ethz.ch/content/dam/ethz/special-interest/mavt/dynamic-systems-n-control/idsc-dam/Lectures/Stochastic-Systems/Probability.pdf

% https://www.statlect.com/glossary/probability-space

% define the term "observation"


% https://en.wikipedia.org/wiki/Event_(probability_theory)
% https://en.wikipedia.org/wiki/Elementary_event

%---------------------------------------------------------------------------------------------------
%\subsubsection{Conditional Probabilities}
\subsubsection{Computing with Probabilities}
% find better name - maybe something about "complex probailities", combining propbabilites


\paragraph{Joint Probability}
A \emph{joint probability} of two events $A$ and $B$ is the probability that both events occur. Occurrence of both events $A$ and $B$ means that the event defined by the \emph{intersection set} $A \cap B$ occurs. Think again of a die and take as event $A$ the event that an even number occurs, i.e. $A = \{2,4,6\}$ and as event $B$ that the number is less than $5$, i.e. $B = \{1,2,3,4\}$. The event $A \cap B$ means that the number is even \emph{and} is less then 5. That means we are asking for the probability of $A \cap B = \{2,4\}$. The joint probability of $A$ and $B$ can be written down as $P(A \cap B)$ or in the shorter notation $P(A,B)$. ...TBC...

% P(A \cap B) is also denoted as $P(A,B)$

% https://en.wikipedia.org/wiki/Joint_probability_distribution
% https://statisticsbyjim.com/probability/joint-probability/
% https://byjus.com/maths/joint-probability/
% https://www.sciencedirect.com/topics/engineering/joint-probability


% Basics of joint probability
% https://www.youtube.com/watch?v=CQS4xxz-2s4

\paragraph{Conditional Probability}
A conditional probability is a probability of some event $A$ to occur when we already know that some other event $B$ definitely does occur. For example, if we already know that the die rolled an even number, the probability that it rolled the number $4$ is now $\frac{1}{3}$ and not $\frac{1}{6}$ anymore as it would be if we wouldn't have the information about the evenness. Conditional probabilities are denoted as $P(A|B)$ which we may read as "the probability of $A$ under the condition of $B$" or shorter "the probability of $A$ given $B$". The formula to compute such a conditional probability is given by:
\begin{equation}
\label{Eq:ConditionalProbability}
 P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A, B)}{P(B)}
\end{equation}
...TBC...give intuition why that formula makes sense. Writing it as $P(A,B) = P(B) P(A|B)$ makes it clearer

% P(A|B) = P(A \cap B) / P(B)

% P(A) = 1/6, P(B) = 1/2, P(A|B) = 1/3

% https://en.wikipedia.org/wiki/Conditional_probability


% Conditional probability, joint probability, marginal probability,  Bayes theorem



\paragraph{Bayes Theorem}
%From $P(A \cap B) = P(A) P(B|A)$,  $P(B \cap A) = P(B) P(A|B)$ and $P(A \cap B) = P(B \cap A)$, it immediately follows that:
From $P(A, B) = P(A) P(B|A)$,  $P(B, A) = P(B) P(A|B)$ and $P(A, B) = P(B, A)$, it immediately follows that:
\begin{equation}
\label{Eq:BayesTheorem}
 P(A|B) = \frac{P(B|A) P(A)}{P(B)}
\end{equation}
You just need to plug in the right hand sides of equalities 1 and 2 into equality 3 and then solve for $ P(A|B)$. This result is known as Bayes theorem and is one of the important results in probability theory. In order to memorize it, I'd recommend to just memorize the 3 obvious equalities above from which it immediately follows. They are actually only two equalities - the second is exactly the same as first just with reversed roles of $A$ and $B$. Bayes theorem has a lot of uses....TBC...explain some use cases, give examples

%https://en.wikipedia.org/wiki/Bayes%27_theorem


% https://en.wikipedia.org/wiki/Law_of_total_probability


% https://www.youtube.com/watch?v=CQS4xxz-2s4

%===================================================================================================
\subsection{Probability Distributions}


%---------------------------------------------------------------------------------------------------
\subsubsection{Discrete Distributions}

\paragraph{Uniform Distribution}

\paragraph{Triangular Distribution}

\paragraph{Convolution} TODO: explain how adding two independent random variables corresponds to the convolution of their distributions, the triangular distribution arising from summing tow uniform distributions is an example, mention the Irwin-Hall distribution

%\paragraph{Irwin-Hall Distribution}


\paragraph{Binomial Distribution}

% Binomial distributions | Probabilities of probabilities, part 1
% https://www.youtube.com/watch?v=8idr1WZ1A7Q

\paragraph{Poisson Distribution}
% https://en.wikipedia.org/wiki/Poisson_distribution


\paragraph{Benford's Law}

% https://en.wikipedia.org/wiki/Benford%27s_law


%---------------------------------------------------------------------------------------------------
\subsubsection{Continuous Distributions}
% akak provbaility density function pdf

\paragraph{Irwin-Hall Distributions} TODO: uniform, triangular, piecewsie parabolic, etc., B-splines

\paragraph{Gaussian Distribution}


% Wie ein bisschen Mathe bei der Mondlandung half (Das Kalman-Filter)
% https://www.youtube.com/watch?v=EBjca6tPuO0
% -Multiplication of two Gaussians gives again a Gaussian - mu and sigma of the resulting 
%  distribution are given at 23:12 ...but check, if the result is really normalized
%  "bis auf dass die Konstante nicht stimmt" ...hmm...try to figure out the constant
% -at 24:46 is yet another formula


% https://math.stackexchange.com/questions/1112866/product-of-two-gaussian-pdfs-is-a-gaussian-pdf-but-product-of-two-gaussian-vari

% https://math.stackexchange.com/questions/101062/is-the-product-of-two-gaussian-random-variables-also-a-gaussian

% https://math.stackexchange.com/questions/114420/calculate-the-product-of-two-gaussian-pdfs

% Products and Convolutions of Gaussian Probability Density Functions
% http://www.lucamartino.altervista.org/2003-003.pdf

\paragraph{Exponential Distribution}

\paragraph{Cauchy Distribution}

\paragraph{Student t-Distribution}


\paragraph{The Central Limit Theorem}
One of the important theorems in probability theory is the \emph{central limit theorem}. It states that when we add more and more independent random variables, the distribution of the sum will, under mild conditions, always approach a Gaussian distribution. That's a justification for why in statistical models, the random variables that occur within it, are often assumed to follow a Gaussian distribution. Assuming a Gaussian distribution also happens to be convenient in curve fitting because it means that a maximum likelihood fit simplifies to a least-squares fit [VERIFY!] ...TBC...

% it's also convenient because it implies that a least-squares fit to some data is a maximum likelihood fit

%---------------------------------------------------------------------------------------------------
\subsubsection{Parameters of Distributions}

\paragraph{Mean}

\paragraph{Mode}
% bimodal and multimodal distributions

\paragraph{Median}

\paragraph{Quantiles}
% aka percentiles

\paragraph{Variance}

\paragraph{Covariance}

\paragraph{Correlation}

\paragraph{Moments}
% skew, kurtosis, central moments, ...



\paragraph{Entropy}

\paragraph{Cross Entropy}


\paragraph{Kullback-Leibler Divergence}




% central limit theorem

\begin{comment}

Probability is just...really weird
https://www.youtube.com/watch?v=zczGnnM05TQ




- Probability distributions
  Discrete
  Continuous

- explain some paradoxa - make the point that probability is often counterintuitive, see:
  8 minutes of Counterintuitive Math
  https://www.youtube.com/watch?v=rYijIEl3euU
  it has not only probability related counterintuitive examples, though.


This Video Will Make You More Rational
https://www.youtube.com/watch?v=rH2wqZ58eFw
-About a probability paradoxon: Choose 1 envelope from 2. One has an mount of money x in it, the 
 other has 2*x. After selecting, you have the chance to switch the selection. Switching means that 
 you either double or halve your price with a 50/50 chance for either scenario. The expected amount 
 of money after switching is x * (2 + 1/2) / 2 = 1.25*x. ...so it seems that switching is the right 
 choice - but that seems odd. Intuitively, it shouldn't matter if we switch or not.
 
 
See also the "Monty Hall problem" (it's a different problem)
 

\end{comment}

