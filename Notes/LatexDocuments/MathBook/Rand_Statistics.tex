\section{Statistics}





%===================================================================================================
\subsection{Statistical Parameters}
% Maybe rename to Statistical Moments/Parameters/Quantities

%---------------------------------------------------------------------------------------------------
\subsubsection{Mean}


\paragraph{Arithmetic Mean}

\paragraph{Geometric Mean}

\paragraph{Harmonic Mean}

\paragraph{Generalized Mean}

% why is the power mean so important?
% https://www.youtube.com/watch?v=d5prRQ-qFQ4
% -If p < q, then mean_p(...) < mean_q(...)

\paragraph{Circular Mean}
% extrinsic/intrinsic mean

% https://en.wikipedia.org/wiki/Circular_mean
% https://en.wikipedia.org/wiki/Directional_statistics


% https://www.youtube.com/watch?v=cYVmcaRAbJg
% Mean angle is not a usual average. Means on circle - Intro to directional statistics (3B1B SoME1)


% https://en.wikipedia.org/wiki/Fr%C3%A9chet_mean

% Why Few Math Students Actually Understand the Meaning of Means
% https://www.youtube.com/watch?v=V1_4nNm8a6w

%---------------------------------------------------------------------------------------------------
\subsubsection{Variance}

% Why do we divide by $n-1$ rather than $n$? This has confused me for a long time. Yeah, there were explanations that said something about "degrees of freedom" something but...

% Why n-1? Least Squares and Bessel’s Correction | Degrees of Freedom Ch. 2
% https://www.youtube.com/watch?v=8e9aDMXRRlc
%

\paragraph{Covariance}

\paragraph{Correlation}



% How can you use higher dimensional vectors to find your similarity to someone else?
% https://www.youtube.com/watch?v=P3j70Qw-bPY
% -Goes thorugh some similarity measures similar to correlation


%---------------------------------------------------------------------------------------------------
\subsubsection{Higher Moments}

\paragraph{Skew}

\paragraph{Kurtosis}





%---------------------------------------------------------------------------------------------------
\subsubsection{Entropy}






%===================================================================================================
\subsection{Parameter Estimation}



% Bias/variance, maximum likelihood

% Explain why estimating the variance uses a division by N-1 rather than N when using an estimated 
% mean. Maybe use an example with just two data points x1 = 2, x2 = 6. The estimated mean is 
% (2+6)/2 = 4 and the estimated variance is ((2-4)^2 + (6-4)^2) / (2-1) = (2^2 + 2^2) / 1 = 8 with
% the formula using N-1 and it's 4 with the formula using N.

% https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance
% https://en.wikipedia.org/wiki/Bessel%27s_correction
% https://en.wikipedia.org/wiki/Bessel%27s_correction#Source_of_bias

%https://www.youtube.com/watch?v=v-DQqUcfzQM  Why does the sample variance have an n-1?
%-Makes an intuitive argument why we should use 1/(n-1) rather than 1/n when estimating the sample
% variance by using the sample mean (rather than the population mean) internally. It's because the
% sample mean is "pulled" towards the actual data samples in comparison to the true mean. That let's 
% us underestimate the true variance when using 1/n.



% The idea that won the 2025 "Nobel Prize in Statistics”
% https://www.youtube.com/watch?v=8Ae_QzwwR_U
% -About smoothing splines and the Kimeldorf-Wahba Representer Theorem
%  p(x) = \sum_i b_i K(x, x_i)

% See also:
% https://en.wikipedia.org/wiki/Smoothing_spline
% https://en.wikipedia.org/wiki/Representer_theorem
% https://en.wikipedia.org/wiki/Kernel_method
% https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf
% https://teazrq.github.io/SMLR/reproducing-kernel-hilbert-space.html



% https://en.wikipedia.org/wiki/Stein%27s_example

% The Stein Paradox - Numberphile
% https://www.youtube.com/watch?v=FUQwijSDzg8


% The most important theory in statistics | Maximum Likelihood
% https://www.youtube.com/watch?v=YevSE6bRhTo


%===================================================================================================
\subsection{Hypothesis Testing}



%===================================================================================================
\subsection{Data Visualization}

%---------------------------------------------------------------------------------------------------
\subsubsection{Scatter Plots}

%---------------------------------------------------------------------------------------------------
\subsubsection{Histograms}

% explain problems with binning - see Arens




\begin{comment}

- Maybe rename file to Rand_Statistics.tex

-descriptive vs inductive statistics
 descriptive: describes data that is available
 inductive: draws conclusions from sampled data about a general population

The 7 Levels of Statistics
https://www.youtube.com/watch?v=eg6N0i8NVxs

Degrees of Freedom, Actually Explained - The Geometry of Statistics | Ch. 1 (#SoME4)
https://www.youtube.com/watch?v=VDlnuO96p58

Population Statistics and Random Sampling
https://www.youtube.com/watch?v=OlkL1YatyHI

What is a p-value? | #SoME4
https://www.youtube.com/watch?v=qEE0rzytHls

Why two Formulas for Standard Deviation? When and Why use n vs. n−1.
https://www.youtube.com/watch?v=Z1HejAcoXAE

Statistics' $1,000,000 Idea
https://www.youtube.com/watch?v=VlkByRCztzc
-About "false discovery rate" and p values (I think - didn't watch in full yet)


\end{comment}