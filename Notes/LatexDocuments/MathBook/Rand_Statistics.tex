\section{Statistics}





%===================================================================================================
\subsection{Statistical Parameters}
% Maybe rename to Statistical Moments/Parameters/Quantities

%---------------------------------------------------------------------------------------------------
\subsubsection{Mean}


\paragraph{Arithmetic Mean}

\paragraph{Geometric Mean}

\paragraph{Harmonic Mean}

\paragraph{Generalized Mean}

% why is the power mean so important?
% https://www.youtube.com/watch?v=d5prRQ-qFQ4
% -If p < q, then mean_p(...) < mean_q(...)

\paragraph{Circular Mean}
% extrinsic/intrinsic mean

% https://en.wikipedia.org/wiki/Circular_mean
% https://en.wikipedia.org/wiki/Directional_statistics


% https://www.youtube.com/watch?v=cYVmcaRAbJg
% Mean angle is not a usual average. Means on circle - Intro to directional statistics (3B1B SoME1)


% https://en.wikipedia.org/wiki/Fr%C3%A9chet_mean

%---------------------------------------------------------------------------------------------------
\subsubsection{Variance}


\paragraph{Covariance}

\paragraph{Correlation}


%---------------------------------------------------------------------------------------------------
\subsubsection{Higher Moments}

\paragraph{Skew}

\paragraph{Kurtosis}







%===================================================================================================
\subsection{Parameter Estimation}



% Bias/variance, maximum likelihood

% Explain why estimating the variance uses a division by N-1 rather than N when using an estimated 
% mean. Maybe use an example with just two data points x1 = 2, x2 = 6. The estimated mean is 
% (2+6)/2 = 4 and the estimated variance is ((2-4)^2 + (6-4)^2) / (2-1) = (2^2 + 2^2) / 1 = 8 with
% the formula using N-1 and it's 4 with the formula using N.

% https://en.wikipedia.org/wiki/Variance#Unbiased_sample_variance
% https://en.wikipedia.org/wiki/Bessel%27s_correction
% https://en.wikipedia.org/wiki/Bessel%27s_correction#Source_of_bias

%https://www.youtube.com/watch?v=v-DQqUcfzQM  Why does the sample variance have an n-1?
%-Makes an intuitive argument why we should use 1/(n-1) rather than 1/n when estimating the sample
% variance by using the sample mean (rather than the population mean) internally. It's because the
% sample mean is "pulled" towards the actual data samples in comparison to the true mean. That let's 
% us underestimate the true variance when using 1/n.



% The idea that won the 2025 "Nobel Prize in Statistics”
% https://www.youtube.com/watch?v=8Ae_QzwwR_U
% -About smoothing splines and the Kimeldorf-Wahba Representer Theorem
%  p(x) = \sum_i b_i K(x, x_i)

% See also:
% https://en.wikipedia.org/wiki/Smoothing_spline
% https://en.wikipedia.org/wiki/Representer_theorem
% https://en.wikipedia.org/wiki/Kernel_method
% https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf
% https://teazrq.github.io/SMLR/reproducing-kernel-hilbert-space.html


%===================================================================================================
\subsection{Hypothesis Testing}



%===================================================================================================
\subsection{Data Visualization}

%---------------------------------------------------------------------------------------------------
\subsubsection{Scatter Plots}

%---------------------------------------------------------------------------------------------------
\subsubsection{Histograms}

% explain problems with binning - see Arens




\begin{comment}

- Maybe rename file to Rand_Statistics.tex

-descriptive vs inductive statistics
 descriptive: describes data that is available
 inductive: draws conclusions from sampled data about a general population






\end{comment}