Assume that we have numerical data for a vector field v(x,y), u(x,y) and we know that this data 
represents a potential field. How can we find a corresponding potential numerically? Let's assume we 
have 2 2D data arrays u(i,j), v(i,j) representing the functions u(x,y), v(x,y) where the x,y are 
equidistant with stepsize dx,dy respectively. The situation could look like this:

                u(i,j)                          v(i,j)                             p(i,j)
     i                               i                                  i
  j:     0   1   2   3   4   5    j:     0   1   2   3   4   5       j:     0   1   2   3   4   5
     0  u00 u01 u02 u03 u04 u05      0  v00 v01 v02 v03 v04 v05         0  p00 p01 p02 p03 p04 p05
     1  u10 u11 u12 u13 u14 u15      1  v10 v11 v12 v13 v14 v15         1  p10 p11 p12 p13 p14 p15
     2  u20 u21 u22 u23 u24 u25      2  v20 v21 v22 v23 v24 v25         2  p20 p21 p22 p23 p24 p25
     3  u30 u31 u32 u33 u34 u35      3  v30 v31 v32 v33 v34 v35         3  p30 p31 p32 p33 p34 p35

where the u,v-matrices are known and the p-matrix is to be computed. p should become the potential
of u,v. Note that the visual depiction is misleading in what is x and what is y: x or i goes down 
vertically/row-wise while y or j goes horizontally or column-wise. But that's just the visual 
interpretation of the arrays and doesn't matter conceptually.

Idea:

Let's assume the u,v have arisen from p via numerical differentiation via central differencing. That
means, we can make the ansatz:


 
Maybe such an ansatz can lead to a (sparse) linear system for the p(i,j). Maybe, if we cleverly 
vectorize the matrices, it could even tridiagonal or maybe pentadiagonal? This formula applies only 
to inner, points. If the index ranges are i = 0,...,I-1 and j = 0,...,J-1, then the formula above 
applies only to i = 1,..,I-2, j = 1,...,J-2. For the edges and corners, we may either use special 
formulas or just leave them out for the moment. We want to write the set of equations in the form:

  A * vec(P) = vec(u,v)

where A is a coefficient matrix and  vec(P), vec(u,v) are so appropriate vectorizations of the 
matrices. Let's rewrite the ansatz as:

  p(i+1, j  ) - p(i-1, j  ) = 2*dx * u(i,j)
  p(i,   j+1) - p(i,   j-1) = 2*dy * v(i,j)

Let's assume, vectorizing a matrix just means re-interpreting it as vector without re-ordering it in 
memory. That means, we just concatenate all the rows, one after another, into one big vector. 


For 
our example with the 4x6 matrices, we would get 







----------------------------------------------------------------------------------------------------
If we make an ansatz based on a forward difference:

  u(i,j) = (p(i+1,j) - p(i,j)) / dx,   v(i,j) = (p(i,j+1) - p(i,j)) / dy

we could actually solve both equations for p(i,j). Setting them equal would give one equation for
the two values p(i+1,j) and p(i,j+1). I don't know, if that leads anywhere.

----------------------------------------------------------------------------------------------------
Other Idea (may be obsolete because it doesn't work?):
-Integrate u with respect to x using trapezoidal integration row-wise. Call the result U.
-Integrate v with respect to y using trapezoidal integration column-wise. Call the result V.
-They should match up to functions only in the other variable, i.e. the real potential is a function
 p(x,y) = U(x,y) + f(y) = V(x,y) + g(x). I think, to find f(y) we can use V - U or likewise to find 
 g(x) we can use U - V. U contain components of p that depend on x, V contains only components that 
 depend on y. In U-V and V-U, the terms that depend on both x and y will cancel and it remains only
 the missing part that depends on the other variable. ..I'm totally not sure about that - it's just 
 an idea. Ah - no: U - V = g(x) - f(y) and V - U = f(y) - g(x)
 
Let's take an example. The 2D potential is given by:

  p(x,y) = 3x^3 + 2y^4 + 5xy
  
The vector field u(x,y), v(x,y) is obtained by taking partial derivatives:

  u(x,y) = p_x(x,y) = 9x^2 + 5y
  v(x,y) = p_y(x,y) = 8y^3 + 5x

To get back p, we integrate u with respect to x and call it U and integrate v with respect to y and 
call it V:

  U = U(x,y) = 3x^3 + 5xy = g(x) + h(x,y)
  V = V(x,y) = 2y^4 + 5xy = f(y) + h(x,y)
  
Take the sum, call it W:
    
  W := U+V = 3x^3 + 2y^4 + 10xy  = g(x) + f(y) + 2 h(x,y)

This a linear system of 3 equations for g,f,h which we can solve for either of the 3. U,V,U+V=W are 
the known right hand sides, g,f,h are the variables. Let's try to solve for the common part 
h = h(x,y):

  U =  h + g
  V =  h + f
  W = 2h + g + f
  
...ah..no...the last line is linearly dependent on the first two (its their sum), so the system is 
singular
  




Now take the differences:

  U-V = (3x^3 + 5xy) - (2y^4 + 5xy) = 3x^3 - 2y^4         = g(x) - f(y)
  V-U = (2y^4 + 5xy) - (3x^3 + 5xy) = 2y^4 - 3x^3         = f(y) - g(x)
  U+V = (3x^3 + 5xy) + (2y^4 + 5xy) = 3x^3 + 2y^4 + 10xy  = g(x) + f(y) + 2 h(x,y)

where h(x,y) is the cross-part, the common terms that appear in both integrals which is the 5xy term 
here in this example.

...hmm... don't know, if that leads anywhere


...maybe another way:

   U = U(x,y) = 3x^3 + 5xy + f(y)
   V = V(x,y) = 2y^4 + 5xy + g(x)
  
The numerical integration will assume f(y) = g(x) = 0. But that's wrong. We need to figure out what 
the actual f(y) and/or g(x) was.

  1: U - V = (3x^3 + 5xy + f(y)) - (2y^4 + 5xy + g(x)) = (3x^3 + f(y)) - (2y^4 + g(x))
  2: V - U = (2y^4 + 5xy + g(x)) - (3x^3 + 5xy + f(y)) = (2y^4 + g(x)) - (3x^3 + f(y))
  
Solve 1  