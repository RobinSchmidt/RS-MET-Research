Assume that we have numerical data for a vector field U(x,y), V(x,y) and we know that this data 
represents a potential field. How can we find a corresponding potential numerically? Let's assume we 
have 2 2D data arrays U(i,j), U(i,j) representing the functions U(x,y), V(x,y) where the x,y are 
equidistant with stepsize dx,dy respectively. In a 4x5 toy scenario, the situation could look like 
this:

                U(i,j)                      V(i,j)                         P(i,j)
     i                           i                              i
  j:     0   1   2   3   4    j:     0   1   2   3   4       j:     0   1   2   3   4 
     0  U00 U01 U02 U03 U04      0  V00 V01 V02 V03 V04         0  P00 P01 P02 P03 P04
     1  U10 U11 U12 U13 U14      1  V10 V11 V12 V13 V14         1  P10 P11 P12 P13 P14
     2  U20 U21 U22 U23 U24      2  V20 V21 V22 V23 V24         2  P20 P21 P22 P23 P24
     3  U30 U31 U32 U33 U34      3  V30 V31 V32 V33 V34         3  P30 P31 P32 P33 P34

where the U,V-matrices are known and the P-matrix is to be computed. P should become the potential
of U,V. Note that the visual depiction is misleading in what is x and what is y: x or i goes down 
vertically or row-wise while y or j goes horizontally or column-wise. But that's just the visual 
interpretation of the arrays and doesn't matter conceptually.

Let's assume the U,V data matrices have arisen from P via numerical (partial) differentiation via 
central differencing. That means, we can make the Ansatz:

  U(i,j) = (P(i+1, j  ) - P(i-1, j  )) / (2*dx)    and    
  V(i,j) = (P(i,   j+1) - P(i,   j-1)) / (2*dy)
 
This formula applies only to inner points. That is, if the index ranges are i = 0,...,I-1 and 
j = 0,...,J-1, then the formula above applies only to i = 1,..,I-2, j = 1,...,J-2. For reference, we
call that ansatz "I,CD" for: "inner point, central difference". For the boundary points at the 
edges (first and last row and column of the matrix), we may use special formulas based on forward or 
backward differences:

                                              Applies to       Ansatz name
										
  U(i,j) = (P(i+1, j  ) - P(i,   j  )) / dx   i = 0,   all j   L,FD (left column,  forward diff.)
  U(i,j) = (P(i,   j  ) - P(i-1, j  )) / dx   i = I-1, all j   R,FD (right column, backward diff.)
  V(i,j) = (P(i,   j+1) - P(i,   j  )) / dy   j = 0,   all i   T,FD (top row,      foward diff.)
  V(i,j) = (P(i,   j  ) - P(i,   j-1)) / dy   j = J-1, all i   B,FD (bottom row,   backward diff.)
 
We want to pose the problem as a linear system of equations in the form:

  M * vec(P) = vec(U,V)

where M is a coefficient matrix and vec(P), vec(U,V) are appropriate vectorizations of the 
matrices. Let's rewrite the ansatz for the inner points as:

  a*P(i+1, j  ) + b*P(i-1, j  ) = U(i,j)
  c*P(i,   j+1) + d*P(i,   j-1) = V(i,j)

where we define:

  a = 1/(2*dx), b = -1/(2*dx), c = 1/(2*dy), d = -1/(2*dy)
  A = 1/dx,     B = -1/dx,     C = 1/dy,     D = -1/dy
  
Let's assume, vectorizing a matrix just means re-interpreting it as vector without re-ordering it in 
memory. That means, we just concatenate all the rows, one after another, into one big vector (I 
assume a row-major memory layout here). For our example with the 4x5 matrices, we have 
N = I*J * 4*5 = 20 unknowns, namely the entries of P. We will get an overdetermined linear system of 
2*N+1 = 41 equations that looks in matrix form as follows (RHS means right hand side):

           Matrix                         * Vector =  RHS        What the line means     Ansatz

  B _ _ _ _ A _ _ _ _ _ _ _ _ _ _ _ _ _ _    P00      U00        U00 = (P10-P00)/dx      L,FD
  _ B _ _ _ _ A _ _ _ _ _ _ _ _ _ _ _ _ _    P01      U01        U01 = (P11-P01)/dx      L,FD
  _ _ B _ _ _ _ A _ _ _ _ _ _ _ _ _ _ _ _    P02      U02        U02 = (P12-P02)/dx      L,FD
  _ _ _ B _ _ _ _ A _ _ _ _ _ _ _ _ _ _ _    P03      U03        U03 = (P13-P03)/dx      L,FD
  _ _ _ _ B _ _ _ _ A _ _ _ _ _ _ _ _ _ _    P04      U04        U04 = (P14-P04)/dx      L,FD
  b _ _ _ _ _ _ _ _ _ a _ _ _ _ _ _ _ _ _    P10      U10        U10 = (P20-P00)/(2dx)     I,CD
  _ b _ _ _ _ _ _ _ _ _ a _ _ _ _ _ _ _ _    P11      U11        U11 = (P21-P01)/(2dx)     I,CD
  _ _ b _ _ _ _ _ _ _ _ _ a _ _ _ _ _ _ _    P12      U12        U12 = (P22-P02)/(2dx)     I,CD
  _ _ _ b _ _ _ _ _ _ _ _ _ a _ _ _ _ _ _    P13      U13        U13 = (P23-P03)/(2dx)     I,CD
  _ _ _ _ b _ _ _ _ _ _ _ _ _ a _ _ _ _ _    P14      U14        U14 = (P24-P04)/(2dx)     I,CD
  _ _ _ _ _ b _ _ _ _ _ _ _ _ _ a _ _ _ _    P20      U20        U20 = (P30-P10)/(2dx)     I,CD
  _ _ _ _ _ _ b _ _ _ _ _ _ _ _ _ a _ _ _    P21      U21        U21 = (P31-P11)/(2dx)     I,CD
  _ _ _ _ _ _ _ b _ _ _ _ _ _ _ _ _ a _ _    P22      U22        U22 = (P32-P12)/(2dx)     I,CD
  _ _ _ _ _ _ _ _ b _ _ _ _ _ _ _ _ _ a _    P23      U23        U23 = (P33-P13)/(2dx)     I,CD
  _ _ _ _ _ _ _ _ _ b _ _ _ _ _ _ _ _ _ a    P24      U24        U24 = (P34-P14)/(2dx)     I,CD
  _ _ _ _ _ _ _ _ _ _ B _ _ _ _ A _ _ _ _    P30      U30        U30 = (P30-P20)/dx          R,BD
  _ _ _ _ _ _ _ _ _ _ _ B _ _ _ _ A _ _ _    P31      U31        U31 = (P31-P21)/dx          R,BD
  _ _ _ _ _ _ _ _ _ _ _ _ B _ _ _ _ A _ _    P32      U32        U32 = (P32-P22)/dx          R,BD
  _ _ _ _ _ _ _ _ _ _ _ _ _ B _ _ _ _ A _    P33      U33        U33 = (P33-P23)/dx          R,BD
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ B _ _ _ _ A    P34  =   U34        U34 = (P34-P24)/dx          R,BD
  D C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _             V00        V00 = (P01-P00)/dy      T,FD
  d _ c _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _             V01        V01 = (P02-P00)/(2dy)     I,CD
  _ d _ c _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _             V02        V02 = (P03-P01)/(2dy)     I,CD
  _ _ d _ c _ _ _ _ _ _ _ _ _ _ _ _ _ _ _             V03        V03 = (P04-P02)/(2dy)     I,CD
  _ _ _ D C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _             V04        V04 = (P04-P03)/dy          B,BD
  _ _ _ _ _ D C _ _ _ _ _ _ _ _ _ _ _ _ _             V10        V10 = (P11-P10)/dy      T,FD
  _ _ _ _ _ d _ c _ _ _ _ _ _ _ _ _ _ _ _             V11        V11 = (P12-P10)/(2dy)     I,CD
  _ _ _ _ _ _ d _ c _ _ _ _ _ _ _ _ _ _ _             V12        V12 = (P13-P11)/(2dy)     I,CD
  _ _ _ _ _ _ _ d _ c _ _ _ _ _ _ _ _ _ _             V13        V13 = (P14-P12)/(2dy)     I,CD
  _ _ _ _ _ _ _ _ D C _ _ _ _ _ _ _ _ _ _             V14        V14 = (P14-P13)/dy          B,DB   
  _ _ _ _ _ _ _ _ _ _ D C _ _ _ _ _ _ _ _             V20        V20 = (P21-P20)/dy      T,FD
  _ _ _ _ _ _ _ _ _ _ d _ c _ _ _ _ _ _ _             V21        V21 = (P22-P20)/(2dy)     I,CD
  _ _ _ _ _ _ _ _ _ _ _ d _ c _ _ _ _ _ _             V22        V22 = (P23-P21)/(2dy)     I,CD
  _ _ _ _ _ _ _ _ _ _ _ _ d _ c _ _ _ _ _             V23        V23 = (P24-P22)/(2dy)     I,CD
  _ _ _ _ _ _ _ _ _ _ _ _ _ D C _ _ _ _ _             V24        V24 = (P24-P23)/dy          B,BD
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ D C _ _ _             V30        V30 = (P30-P31)/dy      T,FD
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ d _ c _ _             V31        V31 = (P32-P30)/(2dy)     I,CD
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ d _ c _             V32        V32 = (P33-P31)/(2dy)     I,CD
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ d _ c             V33        V33 = (P34-P32)/(2dy)     I,CD
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ D C             V34        V34 = (P34-P33)/dx          B,BD
  _ _ _ _ _ _ _ 1 _ _ _ _ _ _ _ _ _ _ _ _              K          K  =  P12                Konstant 
  
 
We can interpret this matrix equation as a linear system of equations of the form:

  M * p = vertcat(u,v,K) = w
     
where p,u,v are the vectorizations of P,U,V and w = vertcat(u,v,K) means vertical concatenation of u 
and v and the constant K (I think, vertcat is the Matlab name for this operation, IIRC). If 
N = I*J = 4*5 = 20, we will get 2*N+1 = 41 equations for our 20 unknowns. That's an overdetermined 
system which we may solve as a least squares problem by premultiplying both sides with M^T.

  M^T * M * p = M^T * w

which will reduce the number of equations to 20.  
  
The very last line/equation is needed to make the system nonsingular. Without it, the matrix M^T * M 
is singular because there are infinitely many solutions that work as a potential for the same vector 
field. They differ by a constant shift. The last line fixes P12 at K which can be a user-provided 
constant. The index of the last 1 in the line selects where we want to fix the potential. Here it is 
at index corresponding to P12 as example. If we want to fix the potential at index i,j in the data 
matrix, the corresponding index in the coefficient matrix is i*J + j, here 1*5 + 2 = 7. The triple 
i,j,K can be given by the user and default to 0, 0, 0.0. 

For U = dP/dx = P_x, we have 10 (= 2*5) inner points (two rows) because the top and bottom matrix 
rows count as boundary points. For V = dP/dy = P_y, we have 12 (= 3*4) inner points (3 columns) 
because the left and right matrix columns count as boundary points. The distances between the two
nonzero elements in each row are given by: dist(b,a) = 2*J = 2*5 = 10, dist(B,A) = J = 5, 
dist(d,c) = 2, dist(D,C) = 1. 



Notes:
-The matrix is sparse but unstructured. Maybe with some clever re-ordering of the vectors, we could
 bring it into a more structured sparse form, like - say - pentadiagonal? But that looks like a 
 nightmare to do. We actually need to restructure not M itself but rather M^T * M. The better idea
 would probably be to just use a sparse matrix library or write a (simple) class for sparse matrices 
 myself. Solving sparse systems is typically done with iterative methods like Jacobi or Gauss-Seidel
 iterations. I have some prototype code for that somewhere already (either in the main repo or in 
 the research repo). Using dense matrices may be OK for proof of concept and some unit tests on 
 small toy problems but eventually, we want to apply it to bigger data matrices, like 100x100. For 
 an m-by-n data matrix, the vectors will be of size m*n and the coefficient matrix of the system of 
 size 2*m^2*n^2 (I think), so we really need a sparse implementation to handle real world data 
 matrices.
-We could make a different ansatz based on a different formula to compute numerical derivatives. A 
 higher order finite difference approximation would give rise to more nonzero coefficients in the 
 matrix. The result might be more accurate at the cost of more computations due to a less sparse 
 matrix. We could perhaps also use formulas for numerical integration (i.e. trapezoidal et al) for 
 the ansatz (not sure, if that makes sense, though).
-I think, if we numerically differentiate our so obtained potential using the same differencing 
 formula as was used in the ansatz, we will obtain some sort of best curl-free approximation of our
 original vector field. If it has been (numerically) curl-free in the first place, we should get it 
 back as it was. By "numerically curl-free", I mean a curl-free condition in which derivatives have 
 been replaced by numeric approximations via finite differences. Maybe that can be useful to 
 decompose vector fields according to the Helmoltz decomposition theorem. We could subtract the
 curl-free approximation from the original field to get the curl only. The remainder is then div 
 plus constant flow, I think. See 
 https://en.wikipedia.org/wiki/Helmholtz_decomposition
-When we use the best curl-free approximation algorithm with a negated V, we might obtain a best
 (numerically) analytic approximation to an arbitrary given complex function that is represented
 by U,V. By "numerically analytic", I mean a function that satisfies the numerical pendants of the
 Cauchy-Riemann equations where the derivatives are replaced by finite differences.
-I think, the same approach can be applied to 3D problems, too. Constructing the matrices might be
 even more tedious, though.
 
 
ToDo: 

Instead of appending the extra line for P12 = K, try just replacing the two rows corresponding 
to P12, namely:

  _ _ b _ _ _ _ _ _ _ _ _ a _ _ _ _ _ _ _    row with index  7 = i*J + j
  _ _ _ _ _ _ d _ c _ _ _ _ _ _ _ _ _ _ _    row with index 27 = i*J + j + N  where N = I*J = 20
  
  
by the single row:

  _ _ _ _ _ _ _ 1 _ _ _ _ _ _ _ _ _ _ _ _  
  
and also replace the two corresponding right hand sides U12, V12 with the single right hand side K. 
Instead of imposing an additional equation (i.e an additional constraint) to make the matrix 
nonsingular, we would replace two of the original equations by our constraint equation. For 
production code, that might be nicer because we would deal with slightly smaller matrices (2 lines 
less: 2*N-1 instead of 2*N+1 lines). However, assembling the matrix programmatically may get messier 
because the regular structure gets broken. If doing this, let's keep the orginal implementation as 
prototype for reference anyway.
 

Resources:

https://math.stackexchange.com/questions/1340719/numerically-find-a-potential-field-from-gradient
The answer suggests to make an ansatz based on the formula for trapezoidal integration but seems 
similar to my idea of the ansatz using a central difference approximation








----------------------------------------------------------------------------------------------------
Other Idea (may be obsolete because it doesn't work?):

-Integrate u with respect to x using trapezoidal integration row-wise. Call the result U.
-Integrate v with respect to y using trapezoidal integration column-wise. Call the result V.
-They should match up to functions only in the other variable, i.e. the real potential is a function
 p(x,y) = U(x,y) + f(y) = V(x,y) + g(x). I think, to find f(y) we can use V - U or likewise to find 
 g(x) we can use U - V. U contain components of p that depend on x, V contains only components that 
 depend on y. In U-V and V-U, the terms that depend on both x and y will cancel and it remains only
 the missing part that depends on the other variable. ..I'm totally not sure about that - it's just 
 an idea. Ah - no: U - V = g(x) - f(y) and V - U = f(y) - g(x)
 
Let's take an example. The 2D potential is given by:

  p(x,y) = 3x^3 + 2y^4 + 5xy
  
The vector field u(x,y), v(x,y) is obtained by taking partial derivatives:

  u(x,y) = p_x(x,y) = 9x^2 + 5y
  v(x,y) = p_y(x,y) = 8y^3 + 5x

To get back p, we integrate u with respect to x and call it U and integrate v with respect to y and 
call it V:

  U = U(x,y) = 3x^3 + 5xy = g(x) + h(x,y)
  V = V(x,y) = 2y^4 + 5xy = f(y) + h(x,y)
  
Take the sum, call it W:
    
  W := U+V = 3x^3 + 2y^4 + 10xy  = g(x) + f(y) + 2 h(x,y)

This a linear system of 3 equations for g,f,h which we can solve for either of the 3. U,V,U+V=W are 
the known right hand sides, g,f,h are the variables. Let's try to solve for the common part 
h = h(x,y):

  U =  h + g
  V =  h + f
  W = 2h + g + f
  
...ah..no...the last line is linearly dependent on the first two (its their sum), so the system is 
singular
  
Now take the differences:

  U-V = (3x^3 + 5xy) - (2y^4 + 5xy) = 3x^3 - 2y^4         = g(x) - f(y)
  V-U = (2y^4 + 5xy) - (3x^3 + 5xy) = 2y^4 - 3x^3         = f(y) - g(x)
  U+V = (3x^3 + 5xy) + (2y^4 + 5xy) = 3x^3 + 2y^4 + 10xy  = g(x) + f(y) + 2 h(x,y)

where h(x,y) is the cross-part, the common terms that appear in both integrals which is the 5xy term 
here in this example.

...hmm... don't know, if that leads anywhere


...maybe another way:

   U = U(x,y) = 3x^3 + 5xy + f(y)
   V = V(x,y) = 2y^4 + 5xy + g(x)
  
The numerical integration will assume f(y) = g(x) = 0. But that's wrong. We need to figure out what 
the actual f(y) and/or g(x) was.

  1: U - V = (3x^3 + 5xy + f(y)) - (2y^4 + 5xy + g(x)) = (3x^3 + f(y)) - (2y^4 + g(x))
  2: V - U = (2y^4 + 5xy + g(x)) - (3x^3 + 5xy + f(y)) = (2y^4 + g(x)) - (3x^3 + f(y))
  
Solve 1  

...hmm...however we spin it, we don't seem to get enough equations. Maybe we can obtain more 
equations based on directional derivatives into the diagonal directions? Maybe something like

  U =

