Given a vector field:

  f(x_1,...,x_n) = (f_1(x_1,..,x_n), ..., f_n(x_1,..,x_n))
  
we want to find a scalar field F(x_1,..,x_n) whose gradient is the given vector field f. To find it,
we may proceed as follows (I think):

(1) Integrate f_1 wrt x_1, call the result F ("wrt" = "with respect to")
(2) Integrate f_2 wrt x_2, add those terms to F that don't depend on x_1
(3) Integrate f_3 wrt x_3, add those terms to F that don't depend on x_1, x_2
(4) Integrate f_4 wrt x_4, add those terms to F that don't depend on x_1, x_2, x_3
(5) Integrate f_5 wrt x_5, add those terms to F that don't depend on x_1, x_2, x_3, x_4
...
(n) Integrate f_n wrt x_n, add those terms to F that don't depend on x_1, ..., x_{n-1}

Before excuting this algo, it may make sense to verify that a potential exists (by checking the 
symmetry of the Jacobian, I think). After the potential has been found, it may make sense to verify 
that the so found F is indeed a potential by taking all the partial derivatives of F (i.e. dF/dx_1,
..., df/dx_n) and comparing them with the given f_1,...,f_n. They should match. At each step of the 
algotithm, the terms that *do* depend on previous integration variables should match those of the
previously computed expressions. For example, in step (3), those terms that do depend on x_1 and/or 
x_2, should match corresponding terms that have already been computed in steps (1), (2). They are 
already accumulated into our final F, so we should not add them again. In all the integrations, we 
use an integration constant of 0. At every step, the integration constant may actually be a function 
of the remaining variables. That's precisely the stuff, we add in the subsequent steps.
