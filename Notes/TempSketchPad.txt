This file contains temporary draft notes. It's just a totally messy sketchpad!

----------------------------------------------------------------------------------------------------
Engrowler/Growlifier

- My underlying working assumption is that "growliness" in a signal comes from minor differences 
  between the different pitch cycles in a quasi-periodic signal. This could mean that different 
  cycles have a slightly different lengths, amplitudes, shapes, etc..
  
- If that hypothesis is true, then we could impose "growl" on an existing monophonic, quasi-periodic 
  signal by detecting the pitch cycles (for example by using a zero-crossing based detector) and 
  whenever we encounter a cycle boundary, we could randomly change some parameter of some DSP 
  algorithm. In the simplest case, we could just change the output amplitude like so:
  
  if(cycleEndDetected)
    amp = 1.0 + growlAmount * getRandomNumber();
  return amp * input;
  
- Other parameters that could be changed are filter settings like the center freq for a peaking EQ 
  or the delay time in a delay line. Maybe we could have two biquads in peak eq mode and each gets 
  its own random freq offset on each trigger. That may produce pseudo-random format changes. In a 
  signal generator, we could also change the signal frequency.
  
- Maybe instead of using a PRNG, the random number could be derived from the input signal itself 
  using a sort of "hash" function. That would make the output reproducible. To make the effect 
  invariant with rewspect to the total gain of the input, we could use a signal measure that is 
  itself independent of the gain of the signal. Examples could be the crest factor or the 
  cross-correlation of two successive cycles. As we are detecting the cycle boundaries anyway, the 
  setting elnds itself to use some measurement that is defined over one or multiple past cycles. The
  hash function could be something like  f(x) = 1 - 2*fmod(c*x, 1.0)  or  f(x) = sin(c*x)  where c 
  is some large (perhaps user defined) constant and x is our signal measure (like the correlation).
  I defined the functions is such a way that the output is in the range -1..+1 which seems to be 
  most convenient here. Perhaps the range 0..1 could also be appropriate. Not sure yet.
  
- If we do compute a cycle cross correlation along the way to use it as input for the hash function,
  we could additionally use that measurement to scale the growl amount such that only (quasi) 
  periodic signals are engrowled and nonperiodic signals are left alone.

- Maybe the switch does not need to occurr on every cycle but every 2 cycles or 3 cycles or whatever
  other number. We could provide a user adjustable "Divider" parameter for that. Maybe we could even
  find a way to make that parameter continuous. To switch every 1.5 cycles, we would after detecting 
  a cycle end, delay the switch by half a cycle, next time delay it by a full cycle etc. It may be a
  bit tricky but maybe we can make it work. Maybe different signal parameters (amplitude, formant 1, 
  formant 2, delay, freq, ...) could use different dividers. That would make the effect more 
  complex and more interesting.
  
----------------------------------------------------------------------------------------------------
Anti-Aliasing Techniques:

Catalog of anti-aliasing techniques. 

- Oversampling. Pro: Universally applicable. Easy to implement. Con: Computationally expensive

- Additive Synthesis: Pro: High level of control. Con: Very expensive computationally. Limited
  area of application (synthesis of sinusoidal spectra). Using Chebychev polynomials, the idea can
  be partially extended to waveshaping.
 
- BLIT/BLEP/BLAMP (Bandlimited Impulse Train, Step, Ramp): Pro: Reasonably efficient. Con: 
  Applicable only to synthesis of waveforms with discontinuities (in the wave itself or one of its 
  derivatives and to the jumps in hard-sync)

- ADAA (Anti Derivative Anti-Aliasing): Applicable to waveshapers

- DSF (Discrete Summation Formulas):

- Lowpass filtering before downsampling. Applicable to waveform playback at higher speed.

- Mip-Mapping: Store various different precomputed versions of the signal which are progressively 
  more heavily lowpass filtered. These so called mip-maps can either be decimated according to their
  individual bandwidths or not. This is basically a precomputed/offline variant of the "lowpassing 
  before downsampling" technique  ...TBC...
  
- Magnitude Matching: Applicable to either anti-alias or decramp filter magnitude responses. Which
  of the two it is depends on what you consider as the baseline reference. Impulse invariant 
  designs have indeed aliased frequency responses such that calling it an anti-aliasing technique
  is somewhat appropriate (although the term is being bent a bit to include frequency response
  aliasing), bilinear transform based once show frequency warping.
  
- Parameter range limitation or reduction of parameters that control spectral brightness by the 
  fundamental frequency in various syntesis methods like FM, PD, etc. Pro: Easy to implement. Very
  efficient. Con: It's not proper anti-aliasing but actually just cheating. However, decreasing the
  brightness for higher notes can actually be natural and musical.

- Highpass filtering: Can get rid of aliasing components below the fundamental which tend to be 
  especially annoying. It's also more in the "cheating" territory using psychoacoustic masking 
  effects for the higher alias frequencies.

- Quantizing oscillator frequencies to those which have an integer number of samples in one cycle.
  Perhaps combined with internal resampling. Does not really get rid of the aliasing components but
  strategically places them on top of the already existing harmonic, making their impact far less
  objectionable. They only alter the amplitudes but do not introduce new frequency components.
  Pro: efficient, easy to implement, Con: very restrictive in what pitches are allowed.
  
- The "pitch quantization" method described above can be combined with (deterministic or 
  probabilistic) dithering to get back to the full frequency range - at the expense of introducing
  noise. This "pitch dithering" approach would pick two integer cycle lengths L1, L2 that straddle 
  the desired cycle length L and make sure that the _average_ cycle length equals the desired cycle  
  length by (probabilistically or determistically) alternating between cycles of lengths L1 and L2.

- Non-uniform (re)sampling. See     https://www.researchgate.net/publication/237136907_IMPLEMENTATION_OF_NON-UNIFORM_SAMPLING_FOR_%27ALIAS-FREE_PROCESSING%27_IN_DIGITAL_CONTROL
  
- Advanced interpolation techniques like (windowed) sinc interpolation. Theoretically, this ampounts
  to perfect resampling, i.e. going back to the continuous time domain and then back to a new 
  discrete time domain at a different sampling rate.
  
- Often combinations are used. For example, mip-mapping together with mild oversampling (e.g. 2x) or
  ADAA with moderate oversampling. Especially oversampling is often thrown in as secondary technique
  on top of another one.


ToDo: 

- Format each technique as follows: Name, Description, Applications, Pros, Cons

- Maybe start KVR thread "Catalog of anti-aliasing techniques for digital audio". Maybe others can
  contribute more ideas.

---------------------------------------------------------------------------------------------------- 



---------------------------------------------------------------------------------------------------- 

ResoVerb or TuneVerb or ModalVerb:

- A reverb based on allpasses based on my parallel-comb-bank-allpass filter

- The resonances of the allpass can be adjusted to be musical notes such that we treat the "metallic
  resonances" as a desirable feature, not as a flaw.
  
- Maybe for each note we could tune 2 or 3 modal filters to slightly above and below in order to get
  sinusoidal undulations of amplitude. The frequency of these undulations, i.e. the frequency 
  difference between center freq and outer freqs may be different for each mode such that higher 
  modes modulate faster and qe get a complex overall modulation pattern.
  
- Maybe a similar idea can be applied to the comb filters in an FDN. Tune them to musical notes. 
  Maybe it can also be applied to old school serial allpasses / parallel combs structures, i.e. 
  Schroeder/Moorer reverbs.
 
---------------------------------------------------------------------------------------------------- 
 
About pitch dither probabilities:

See comments in testPitchDithering() in Experiments.cpp

- The table of probabilities could look like as follows. In the column headers, we have some
  desired example cycle lengths. In the row headers, we have the integer cycle lengths that
  are produced. The table entries are the probabilities with which the cycles of length given
  by row header is produced when the desired length is as given by the column header. For 
  example, according to the middle column of the table, when the desired cycle length is 
  100.0, we would produce cycles of length 100 with probability p = 0.5 and cycles of length 
  99 and 101 each with p = 0.25. If the desired cycle length would be 100.5 (one column 
  further right), we would produce cycles of length 100 and 101 each with p = 0.5:
 
        in:  98.0   98.5   99.0   99.5   100.0   100.5   101.0   101.5   102.0
     out             
      98      0.5    0.5    0.25
      99      0.25   0.5    0.5    0.5     0.25   
     100                    0.25   0.5     0.5     0.5     0.25
     101                                   0.25    0.5     0.5     0.5     0.25
     102                                                   0.25    0.5     0.5

  The empty space is implicitly assumed to be zero and the table continues with the pattern
  Here, we have used the [1,1]/2 and [1,2,1]/4 rows of the binomial distribution (i.e. a 
  scaled Pascal's triangle). When the desired length L is an exact integer like 100.0, we use 
  L-1 with p = 1/4, L with p = 1/2 and L+1 with p = 1/4. When L is an exact half integer like 
  100.5, we use floor(L) with p = 1/2 and floor(L) + 1 with p = 1/2 and floor(L) - 1 is not 
  used at all, i.e. with p = 0. The same goes for floor(L) - 2, + 3 or any number and also for
  floor(L) + 2, +3, .... When L is in between and integer and a half integer, we linearly 
  interpolate the probabilities from the nearest full and half integers. 


A geometric interpretation of the table above could be as follows:


             L-1      L      L+1
  -----------------------------------------
  |   98  |   99  |  100  |  101  |  102  |        Band of integer, each int occupies same length
  -----------------------------------------  
              -----------------
              |       |       |                    Slider of length 2
              -----------------
                      |
                     L.f = 100.5
					
The center of the slider gives the position of the desired cycle length which we denote by L.f such
that L is the floor of the desired length and f is its fractional part. In the depicted situation 
above, L.f would be 100.5. The center of the slider is exactly at the middle of the entry 100 in the
band. The probabilities to produce cycles of length L-1, L and L+1 are given by the proportional 
overlap of the slider with the cell of the respective number. Since the length of the slider is 2,
the proportionality factor is 0.5. The slider overlaps fully (i.e. to 100% or 1) with the cell of 
100, so the probability of 100 is given by 0.5*1 = 0.5. The slider also overlaps to 50% with the 
cells of 99 and 101, so these cycle lengths will both be produced with probability 0.5*0.5 = 0.25. 
We could also say that 25% of the slider overlap with 99, 50% of it overlap with 100 and another 25%
of it overlaps with 101.
					
					
By using P(99)=0.25, p(100)= 0.5, p(101)= 0.25 We have

Let "mae(x)"  stand for mean absolute error, i.e. the expectation value of the absolute value of the
error.

  mae(L=100.5) = 0.5*|-0.5| + 0.5*|+0.5| = 0.5^2 + 0.5^2 = 0.25 + 0.25 = 0.5
  mae(L=100.0) = 0.25*|-1| + 0.5*|0| + 0.25*|+1|         = 0.25 + 0.25 = 0.5

So, the geometric approach above would produce the same expected absolute error, regardless of 
whether we are at an integer or an half integer (ToDo: Explain whwy it should also work for 
L.x = 100.3 for example).

To implement this, I think, we would have to produce a random number r in 0..1 and have two 
thresholds t1, t2 and do:

  if(r < t1) 
    produce length L-1 cycle
  else if(r < t2)
    produce length L cycle
  else
    produce length L+1 cycle

Maybe <= is more appropriate than < in one or the other or both conditionals.

I'm not quite sure if equalizing the "mae" is the right approach, though. Maybe the perceived noise
loudness is proportional to the variance of the noise, i.e. the expectation value of the square of 
the error. Let "var(x)" be the variance of x. Let's calculate the variances when we would use the
probabilities from above:

  var(L=100.5) = 0.5*(-0.5)^2 + 0.5*(+0.5)^2 = 0.5*0.25 + 0.5*0.25 = 0.125 + 0.125 = 0.25
  var(L=100.0) = 0.25*(-1)^2 + 0.5*(0)^2 + 0.25*(+1)^2 = 0.25 + 0 + 0.25           = 0.5

So with the probabilities above, we would have a higher noise variance at the integers than at the 
half integers. ToDo: Try probabilities p(99) = p(101) = 0.125, p(100) = 0.75


...Then normalizing the variance 


Maybe use the 3 equations 

  p1 + p2 + p3 = 1
  p1*e1 + p2*e2 + p3*e3 = 0    
  p1*e1^2 + p2*e2^2 + p3*e3^2 = const (= 0.5 I think)
  
where p1 is the probability for L-1 and e1 is the error that we make when using this length and so 
on.


Let's start with a given desired period  P  and 3 arbitrary integer lengths  L1,L2,L3  with the only 
constraint that it is possible to form a weighted sum of  p1*L1 + p2*L2 + p3*L3  that sums up to P 
when p1..p3 are all >= 0 and p1 + p2 + p3 = 1. That constraint requires in general that at least one
of L1..L3 values is <= P and at least one of them is >= P (Verify! There may be edge cases, I 
think). Define the 3 error values that correspond to the 3 L1..L3 lengths as:

  e1 = L1 - P,  e2 = L2 - P,  e3 = L3 - P
  
Define an error measure function f(x) that can be applied to the raw error values e1..e3. The 
function f(x) could be, for example, the absolute value f(x) = |x| or the square f(x) = x^2. Using 
this function, compute the 3 error measures:

  m1 = f(e1),  m2 = f(e2),  m3 = f(e3)

So far, everything can be explicitly and readily computed from the known P,L1,L2,L3 values. Now we 
establish a linear system of 3 equations for our 3 weights or probabilities p1..p3 as follows:

  p1    + p2    + p3    = 1        As usual for probabilities
  p1*e1 + p2*e2 + p3*e3 = 0        The error should average out to zero
  p1*m1 + p2*m2 + p3*m3 = M        The mean error measure should be a prescribed constant M
 
This is a linear system of 3 equations for 3 unknowns p1,p2,p3 which we can readily solve. Solving 
equation 1 for p3 gives:

  p3 = 1 - p1 - p2
  
Plugging this expression into equation 2 and then solving it for p2 gives:

  0 = p1*e1 + p2*e2 + (1 - p1 - p2)*e3
    = p1*e1 + p2*e2 + e3 - p1*e3 - p2*e3
	= p1*(e1-e3) + p2*(e2-e3) + e3
	
  p2 = (-e3 - p1*(e1-e3)) / (e2-e3)
     = ( e3 + p1*(e1-e3)) / (e3-e2)
  
Finally, plugging the expressions for p3,p2 into equation 3 and solving for p1 gives:  

  M = p1*m1 + ...
  
...TBC...well - let's do that in SageMath:

var("P M L1 L2 L3 p1 p2 p3 m1 m2 m3")
e1 = L1 - P
e2 = L2 - P
e3 = L3 - P
eq1 = 1 == p1    + p2    + p3
eq2 = 0 == p1*e1 + p2*e2 + p3*e3
eq3 = M == p1*m1 + p2*m2 + p3*m3
solve([eq1,eq2,eq3],[p1,p2,p3])

produces:

  p1 =  (L3*(M - m2) - L2*(M - m3) + P*(m2 - m3)) / (L3*(m1 - m2) - L2*(m1 - m3) + L1*(m2 - m3))
  p2 = -(L3*(M - m1) - L1*(M - m3) + P*(m1 - m3)) / (L3*(m1 - m2) - L2*(m1 - m3) + L1*(m2 - m3))
  p3 =  (L2*(M - m1) - L1*(M - m2) + P*(m1 - m2)) / (L3*(m1 - m2) - L2*(m1 - m3) + L1*(m2 - m3))
  
Hmm...that's not so good. Let's reformulate the equations in terms of e1,e2,e3 rather than L1,L2,L3:

reset()
var("P M e1 e2 e3 p1 p2 p3 m1 m2 m3")
eq1 = 1 == p1    + p2    + p3
eq2 = 0 == p1*e1 + p2*e2 + p3*e3
eq3 = M == p1*m1 + p2*m2 + p3*m3
solve([eq1,eq2,eq3],[p1,p2,p3])

produces:

  p1 = -((M - m3)*e2 - (M - m2)*e3) / (e3*(m1 - m2) - e2*(m1 - m3) + e1*(m2 - m3))
  p2 =  ((M - m3)*e1 - (M - m1)*e3) / (e3*(m1 - m2) - e2*(m1 - m3) + e1*(m2 - m3))
  p3 = -((M - m2)*e1 - (M - m1)*e2) / (e3*(m1 - m2) - e2*(m1 - m3) + e1*(m2 - m3))

which is a bit shorter. To get rid of the minus signs, we swap the terms in the numerators of the
1st and 3d equation:

  p1 =  ((M-m2)*e3 - (M-m3)*e2) / (e3*(m1-m2) - e2*(m1-m3) + e1*(m2-m3))
  p2 =  ((M-m3)*e1 - (M-m1)*e3) / (e3*(m1-m2) - e2*(m1-m3) + e1*(m2-m3))
  p3 =  ((M-m1)*e2 - (M-m2)*e1) / (e3*(m1-m2) - e2*(m1-m3) + e1*(m2-m3))

Let's define:

  M1 = M - m1,  M2 = M - m2,  M3 = M - m3
  S  = 1 / (e3*(m1-m2) - e2*(m1-m3) + e1*(m2-m3))
  
Then we can write it as:

  p1 = (M2*e3 - M3*e2) * S
  p2 = (M3*e1 - M1*e3) * S
  p3 = (M1*e2 - M2*e1) * S
  
To assign a value for M, we can compute M for the special case that P is a half integer. This 
special case is the worst case in the sense that it's farthest away from an integer. In this special
case, we want to use the probabilities p1 = 0, p2 = 0.5, p3 = 0.5. That means that when the desired
period is P = 100.5, we would use cycles of length L1 = 99 with zero probability and cycles of 
lengths L1 = 100 and L2 = 101 each with probability 0.5. For the general case, we want to compute 
our probabilities p1..p3 in such a way that the error measure always equals the worst case error
measure that results from the half-integer special case.

In the approach described above with the slider which shall henceforth be called the "geometric 
approach" (or maybe the "overlap approach"), we would replace the 3rd equation (i.e. the M = ... 
equation) with the very simple equation p2 = 1/2. . When we slide the slider of length over the 
"bar" with lengths and use the percentage of overlap with the middle length L2, it will always be 
1/2 because the length of the slider is 2 length units.
 
 
----------------------------------------------------------------------------------------------------
De-Dithering or De-Quantization by Polynomial Regression

 
 
---------------------------------------------------------------------------------------------------- 
 
 
 
https://ultra.audio/   Nice GUI!


https://github.com/pure-data/externals-howto

This Will Change The Way You Think About Arranging Music
https://www.youtube.com/watch?v=nfLMVhheS2g  


How Lagrange Solved the "Vandermonde Nightmare
https://www.youtube.com/watch?v=JaQLpJEaHvU
-Good explanation of Lagrange interpolation. Has interesting "barycentric optimization" at the end.
 Maybe I should implement that. 